{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Optimization, chapter 1, Introduction-\n",
    "## Topics to cover\n",
    "### What is mathematical optimization\n",
    "### Least-squares and linear programming\n",
    "### Convex optimization\n",
    "### How to read the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a mathematical optimization problem ?\n",
    "* Can be written in a canonical form\n",
    "  \\begin{align}\n",
    "    \\text{minimize} \\quad & f_0(x) \\\\\n",
    "    \\text{such that} \\quad & f_i(x) \\leq b_i, i=1,\\dots,m\n",
    "  \\end{align}\n",
    "* Vocabulary\n",
    "  * $x = (x_0, . . . , x-{n-1})$ is the optimization variable of the problem\n",
    "  * $f_0 : \\mathbb{R}^n \\Leftrightarrow \\mathbb{R}$ is the objective function\n",
    "  * $f_i : \\mathbb{R}^n \\Leftrightarrow \\mathbb{R}$, $i = 1, \\dots, m$ are the (inequality) constraint functions\n",
    "  * The constants $b_1, \\dots, b_{m}$ are the limits, or bounds, for the constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class of optimization problems\n",
    "* The type of optimization problem depends on the nature of $f_0, f_i \\dots$\n",
    "  * We have a linear program if $f_i$ are linear: i.e $f_i(\\alpha x + \\beta y)=\\alpha f_i(x)+\\beta f_i(y)$\n",
    "  * If a problem is not linear, it is called a nonlinear program, examples are quadratic, geometric, cp, socp\n",
    "* The book is about convex optimization problems\n",
    "  * $f_i$ should be convex, ie:\n",
    "    \\begin{align}\n",
    "      & f_i(\\alpha x + \\beta y) \\leq \\alpha f_i(x)+\\beta f_i(y)\\\\\n",
    "      & \\forall x,y \\in \\mathbb{R}^n \\text{ and } \\forall \\alpha, \\beta \\in \\mathbb{R}^+ \\text{ s.t } \\alpha + \\beta=1 \n",
    "    \\end{align}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving optimization problems\n",
    "* There are many types of algorithm to solve various types of problems\n",
    "  * Some problems can be solve with multiple methods\n",
    "  * choice depends on number of variables, number of constraints\n",
    "  * Also depends on some specific structure, ex, sparsity: very few variables involved in the constraints\n",
    "  * Smooth objective and constraints $\\neq$ easy problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least-squares and linear programming\n",
    "* Least square is ubiquitous in science/engineering ! it is unconstrained:\n",
    "  \\begin{align}\n",
    "    \\text{minimize } f_0 = \\|Ax-b\\|_2^2 = \\sum_{i=0}^{k-1} \\left(a_i^t \\cdot x - b_i\\right)^2\n",
    "  \\end{align}\n",
    "* Least square was very successfull in part because of its implicit bayesian  interpretation.\n",
    "* It can be considered as a maximum log-likelihood estimator for homoscedastic normal distribution\n",
    "* Is is smooth, twice differentiable, strongly convex\n",
    "  * derivative of the smooth convex function reads\n",
    "    \\begin{align}\n",
    "      f_0(x) &= \\frac{1}{2} \\|Ax-b\\|_2^2\\\\\n",
    "      &=  \\frac{1}{2} x^t A^t A x + b^t b - x^t A^t b \\\\\n",
    "      f_0'(x) &= A^t A x - A^t b\n",
    "    \\end{align}\n",
    "  * Derivative of convex function vanishes at the minimum: $f_0'(x) = 0 \\Leftrightarrow A^t A x = A^t b$\n",
    "* It has at least a closed form solution (Moore penrose pseudo inverse):$A^+ b$ where $A^+ = \\left(A^t A \\right)^{-1} A^t$\n",
    "* For a well conditionned matrix, solution can be found in $n^2 k$ operations\n",
    "* This later condition can be seen as a one shot newton method (second order method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least square as a statistical estimator\n",
    "* Lets explain this\n",
    "  * $b = Ax^* + \\epsilon$ with $\\epsilon$ the outcome of a random process that follows an homoscedastic multivariate normal law: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$\n",
    "  * Bayes theorem: $p(x|b) = \\frac{p(x) \\times p(b|x)}{p(b)}$\n",
    "  * $b$ the observation vector, probability $p(b)$ without statistical apriori, is supposed equiprobable: $p(b)=\\alpha_0$\n",
    "  * $x$ the candidate solution, without apriori, also supposed equiprobable over $\\mathbb{R}^{k}$: $p(x)=\\alpha_1$.\n",
    "  * Marginal version of the conditional probability $p(b_i|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp^{-\\frac{((Ax)_i-b_i )^2}{2\\sigma^2}}$\n",
    "  * All $b_i $ are independants with the same distribution, we can write $ p(b|x) = \\prod_{i=0}^{k-1} p(b_i|x)$  \n",
    "  * Can be written in a vectorial fashion: $p(b|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{\\|Ax-b\\|_2^2}{2\\sigma^2}}$\n",
    "  * The likelihood of $x$ then reads:  \n",
    "  \\begin{equation}\n",
    "    p(x|b) = \\frac{\\frac{\\alpha_1}{\\sqrt{2\\pi\\sigma^2}} exp^{-\\frac{\\|Ax-b\\|_2^2}{2\\sigma^2}}}{\\alpha_0}\n",
    "  \\end{equation}\n",
    "  * Maximizing the logarithm of this likelihood, without the constants, amounts to:\n",
    "  \\begin{align}\n",
    "    &\\underset{x \\in \\mathbb{R}^k}{max} \\quad -\\frac{1}{2}||Ax - b||_{2}^{2} \\\\\n",
    "    \\Leftrightarrow \\quad &\\underset{x \\in \\mathbb{R}^k}{min} \\quad \\frac{1}{2}||Ax - b||_{2}^{2}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least square generalization 1\n",
    "* The weighted least square (heteroscedastic case)\n",
    "  * \\begin{align}\n",
    "      L(b) &= \\frac{1}{\\sqrt{(2\\pi)^n det(\\Sigma)}} e^{-\\frac{1}{2} (Ax-b)^{\\intercal}\\Sigma^{-1}(Ax-b)} \\\\\n",
    "    \\end{align}\n",
    "  * $\\Sigma$ a covariance matrix, symmetric, positive semi definite, can be diagonalized in an orthogonal basis : $\\Sigma = Q^t D Q$, such that $\\Sigma^{-1} = Q^t D^{-1} Q$ :   \n",
    "    \\begin{align}\n",
    "      &\\underset{x \\in \\mathbb{R}^k}{max} \\quad -\\frac{1}{2} (Ax-b)^t\\Sigma^{-1}(Ax-b) \\\\\n",
    "      \\Leftrightarrow \\quad &\\underset{x \\in \\mathbb{R}^n}{min} \\quad \\frac{1}{2} (Ax-b)^t Q^t D^{-1} Q (Ax-b) \\\\\n",
    "      \\Leftrightarrow \\quad &\\underset{x \\in \\mathbb{R}^n}{min} \\quad \\frac{1}{2} (Ax-b)^t Q^t D^{-\\frac{1}{2}}D^{-\\frac{1}{2}} Q (Ax-b) \\\\\n",
    "      \\Leftrightarrow \\quad &\\underset{x \\in \\mathbb{R}^n}{min} \\quad \\frac{1}{2} \\|D^{-\\frac{1}{2}} Q Ax - D^{-\\frac{1}{2}} Qb\\|_2^2 \\\\\n",
    "      \\Leftrightarrow \\quad &\\underset{x \\in \\mathbb{R}^n}{min} \\quad \\frac{1}{2} \\|A'x - b'\\|_2^2\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least square generalization 2\n",
    "* The Tychonov regularization that favors minimum energy solution, also has a statistical interpretation\n",
    "  \\begin{align}\n",
    "     \\underset{x \\in \\mathbb{R}^k, \\alpha \\geq 0}{min} \\quad \\frac{1}{2} \\|Ax-b\\|_2^2 + \\alpha \\|x\\|_2^2 \\\\\n",
    "  \\end{align}\n",
    "* derivative of the smooth convex function now reads\n",
    "    \\begin{align}\n",
    "      f_0(x) &= \\frac{1}{2} \\|Ax-b\\|_2^2 + \\alpha \\|x\\|_2^2\\\\\n",
    "      &=  \\frac{1}{2} x^t A^t A x + \\alpha x^t x + b^t b - x^t A^t b \\\\\n",
    "      &=  \\frac{1}{2} x^t (A^t A + 2 \\alpha Id) x + b^t b - x^t A^t b \\\\\n",
    "      f_0'(x) &= (A^t A + 2 \\alpha Id) x - A^t b\n",
    "    \\end{align}\n",
    "  * Derivative of convex function vanishes at the minimum: $f_0'(x) = 0 \\Leftrightarrow (A^t A + 2 \\alpha Id) x = A^t b$\n",
    "  * It has also a closed form solution uder symmetric positive definitiveness assumption :$A^+ b$ where $A^+ = \\left(A^t A + 2 \\alpha Id \\right)^{-1} A^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear programming\n",
    "* A linear program reads\n",
    "  \\begin{align}\n",
    "    \\text{minimize} \\quad & c^t x\\\\\n",
    "    \\text{s.t} \\quad & a_i^t x \\leq b_i, i=0,1,\\dots m-1\n",
    "  \\end{align}\n",
    "* No simple analytical solution, but well known algorithms in solvers: simplex method and interior point methods\n",
    "* n^2 m complexity for interior point methods in the worst case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to read the book\n",
    "\n",
    "> *Our main goal is to help the reader develop a working knowledge of\n",
    "convex optimization, i.e., to develop the skills and background needed\n",
    "to recognize, formulate, and solve convex optimization problems.*\n",
    "\n",
    "* The book is divided into three main parts, titled Theory, Applications, and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Random examples\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
