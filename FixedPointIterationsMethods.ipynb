{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Point iteration methods\n",
    "\n",
    "Fixed point iteration method stand for a very large class of algorithm that aims at iteratively solving a fixed point problem of the form $AX=B$ with $A$ an operator, and $B$ a given set of data.\n",
    "\n",
    "In this short report, we will only be studying the case where $A$ is a linear operator represented by a square matrix.\n",
    "\n",
    "## Jacobi and Gauss-Seidel methods\n",
    "\n",
    "### Jacobi Method\n",
    "\t \n",
    "Jacobi and Gauss-Seidel method assume that one can decompose the $n \\times n$ matrix $M$ into a sum of matrices $M = G + H$ where $G$ is full rank, and easily invertible, for instance it is set to a diagonal matrix in the Jacobi method.\n",
    "\n",
    "Anyway, this lead us to a fixed point search in the high dimensional space $\\mathbb{R}^n$:\n",
    "\n",
    "\\begin{align*}\n",
    "    M \\vec{x} - \\vec{y} &= 0\\\\\n",
    "    G \\vec{x} + H \\vec{x} &= \\vec{y} \\\\\n",
    "    G \\vec{x} &= -H \\vec{x} + \\vec{y} \\\\\n",
    "    \\vec{x} &= - G^{-1} H \\vec{x} + G^{-1} \\vec{y} \\\\\n",
    "    \\vec{x} &= M' \\vec{x} + \\vec{y'}\n",
    "\\end{align*}\n",
    "\t\n",
    "Where $M' = - G^{-1} H$ and $\\vec{y'} = G^{-1} \\vec{y}$.\n",
    "\n",
    "This fixed point search can be carried out using a simple iterative scheme\n",
    "\t\n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = M' \\vec{x^k} + \\vec{y'}\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = G^{-1} \\left( \\vec{y} - H \\vec{x^k} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "According to the Picard fixed point theorem, this class of algorithm converges if the operator $T : \\vec{x} \\rightarrow M' \\vec{x} + \\vec{y'}$ is a strict contraction, i.e if there exists $\\rho \\in [0,1[$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\forall (x,x') \\in \\mathbb{R}^n \\times \\mathbb{R}^n, x \\neq x' :\\quad &\\|Tx - Tx'\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "    &\\|M'x + \\vec{y'} - M'x' - \\vec{y'}\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "    &\\|M'x - M'x'\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "    &\\|M' (x - x')\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "    &\\|M'\\| \\|(x - x')\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "    &\\|M'\\| \\leq \\rho\n",
    "\\end{align*}\n",
    "\n",
    "This condition can be interpreted as a $[0,1[$-Lipshitz continuity property of the linear application $M'$, which is equivalent to a constraint over the operator norm of $M'$, that reduces to a constraint over its largest singular value: $0 \\leq \\sigma(M')_{max} < 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determinant of M is 54.0\n",
      "Condition number of M is 14.5981649059\n",
      "spectral radius of M1 is 8.35889894354\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's try to solve a simple linear system using Jacobi iterations\n",
    "\"\"\"\n",
    "\n",
    "M = np.array([[1,1,1,1],\n",
    "            [1,1,4,4],\n",
    "            [1,4,1,4],\n",
    "            [1,4,4,1]])\n",
    "Y = np.ones(M.shape[0])\n",
    "\n",
    "print \"determinant of M is \"+str(np.linalg.det(M))\n",
    "print \"Condition number of M is \"+str(np.linalg.cond(M))\n",
    "\n",
    "#Now perform the decomposition\n",
    "G = np.diag(np.diagonal(M))\n",
    "H = M-G\n",
    "#New variables\n",
    "M1=np.dot(-np.diag(1./np.diagonal(G)),H)\n",
    "Y1=np.dot(np.diag(1./np.diagonal(G)),Y)\n",
    "omega = 0.2\n",
    "u,lamb,vh=np.linalg.svd(M1)\n",
    "spectRadM1 = lamb[0]\n",
    "print \"spectral radius of M1 is \"+str(spectRadM1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Jacobi Method\n",
    "\n",
    "The problem of the Jacobi Method is the strong assumption that $\\rho(M')<1$. Because this assumption is very strong, it rarely holds, and in those case, the method behave very poorly (exponential divergence).\n",
    "\n",
    "One of the evolution that has been proposed in various iterative methods is the over/under relaxation. The idea is simply to consider that, from the point of view of the current solution, the new iterate may be in the right direction toward the solution, but it goes too far. The simple solution is then, to pick-up the next solution between the current one, and the nex Jacobi estimate, using a convex combination of the two.\n",
    "\n",
    "This method can be formalized this way:\n",
    "\n",
    "\\begin{align*}\n",
    "      \\vec{x^{k+1}} &= \\omega( \\underbrace{M' \\vec{x^k} + \\vec{y'}}_{\\text{next Jacobi iterate}} ) + (1-\\omega)\\vec{x^{k}} \\\\\n",
    "        &= \\vec{x^{k}} + \\omega \\left( \\underbrace{M' \\vec{x^k} + \\vec{y'}}_{\\text{next Jacobi iterate}} - \\vec{x^{k}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where we can choose $\\omega>1$ to speed-up the convergence in some cases, and $\\omega<1$ when the spectral radius condition does not hold, and we wish to use Jacobi method anyway. But in this case, there does not seem to be values of $\\omega$ that ensure convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a solution in 80/2000 iteration within absolute precision 1e-05\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Implementing the Jacobi and weighted Jacobi Method\n",
    "\"\"\"\n",
    "Xk=np.zeros(M.shape[1])\n",
    "nbIter = 2000\n",
    "i = 0\n",
    "eps=1e-5\n",
    "\n",
    "while (i<nbIter) and not np.allclose(np.dot(M,Xk),Y,atol=eps):\n",
    "    nextIterate = np.dot(M1,Xk)+Y1\n",
    "    #Classic jacobi method\n",
    "    if( spectRadM1 < 1 ):\n",
    "        Xk = nextIterate\n",
    "    #Weighted jacobi method\n",
    "    else:\n",
    "        Xk += omega*(nextIterate - Xk)\n",
    "    i += 1\n",
    "\n",
    "if i<nbIter:\n",
    "    print \"Found a solution in \"+str(i)+\"/\"+str(nbIter)+\" iteration within absolute precision \"+str(eps)\n",
    "else:\n",
    "    print \"Jacobi iteration failed, residual norm is \"+str(np.linalg.norm(np.dot(M,Xk)-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Successive Over-Relaxation Method (SOR)\n",
    "\n",
    "In practice, the weighted Jacobi method is not really efficient either, Young and Frankel, derived an interesting algorithm, called Successive Over-Relaxation, or SOR, based on a simple trick we will explane later:\n",
    "\n",
    "Let's get back to our first matrix decomposition, and slightly modify it:\n",
    "\n",
    "\\begin{align*}\n",
    "    (G + Id + H) \\vec{x} &= \\vec{y} \\\\\n",
    "    \\omega (G + Id + H) \\vec{x} &= \\omega \\vec{y} \\\\\n",
    "    (G + (\\omega-1)G + \\omega Id + \\omega H) \\vec{x} &= \\omega \\vec{y} \\\\\n",
    "    (G + \\omega Id ) \\vec{x} &= \\omega \\vec{y} - ((\\omega-1)G + \\omega H)\\vec{x} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "With $\\omega$ a relaxation parameter that can be used for underrelaxation ($\\omega<1$) or overrelaxation ($\\omega>1$).\n",
    "The update equation become:\n",
    "\\begin{align*}\n",
    "    \\vec{x^{k+1}} &= - (G+\\omega Id)^{-1}((\\omega-1)G + \\omega H)\\vec{x^{k}} + \\omega (G+\\omega Id)^{-1}\\vec{y} \\\\\n",
    "    \\vec{x^{k+1}} &= M'\\vec{x} + \\vec{y'}\n",
    "\\end{align*}\n",
    "\n",
    "Where $M' = - (G+\\omega Id)^{-1}((\\omega-1)G + \\omega H)$ and $\\vec{y'} = \\omega (G+\\omega Id)^{-1}\\vec{y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a solution in 107/2000 iteration within absolute precision 0.001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing the SOR variation of Jacobi method\n",
    "\"\"\"\n",
    "\n",
    "#Now perform the new decomposition\n",
    "G = np.diag(np.diagonal(M))\n",
    "H = M-G-np.identity(H.shape[0])\n",
    "omega = 0.1\n",
    "gwId = np.diag(1./(np.diagonal(G)+omega))\n",
    "M1=-np.dot(gwId,(omega-1.)*G+omega*H)\n",
    "Y1=omega*np.dot(gwId,Y)\n",
    "\n",
    "#Now solve\n",
    "Xk=np.zeros(M.shape[1])\n",
    "nbIter = 2000\n",
    "i = 0\n",
    "eps=1e-3\n",
    "\n",
    "while (i<nbIter) and not np.allclose(np.dot(M,Xk),Y,atol=eps):\n",
    "    #SOR Jacobi method\n",
    "    Xk = np.dot(M1,Xk)+Y1\n",
    "    i += 1\n",
    "\n",
    "if i<nbIter:\n",
    "    print \"Found a solution in \"+str(i)+\"/\"+str(nbIter)+\" iteration within absolute precision \"+str(eps)\n",
    "else:\n",
    "    print \"Jacobi iteration failed, residual norm is \"+str(np.linalg.norm(np.dot(M,Xk)-Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the SOR trick\n",
    "\n",
    "Where does this $\\omega$ trick comes from ?\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "In 2015, a nice refinement to the basic Jacobi method has been derived, see [https://arxiv.org/abs/1511.04292](Scheduled Relaxation Jacobi method: improvements and applications).\n",
    "\n",
    "We can notice that the author exploited the famous Chebyshev polynomial acceleration scheme we present in the notebook called ChebyshevPolynomialAcceleration.\n",
    "\n",
    "In our numerical experimentation, the Jacobi method as implemented here, performed really bad in most of the case with large randomized systems, it is not really recommended to use it for real life problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss Seidel method\n",
    "\n",
    "In the gauss seidel method, it is assumed that the matrix $M$ can be decomposed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  M = D + (L + U)\n",
    "\\end{equation}\n",
    "\n",
    "Where, $L$ is a strictly lower triangular matrix, $D$ is a non singular diagonal matrix, and $U$ is a stricly upper triangular matrix. The generic method exposed in the previous part can then be applied.\n",
    "\n",
    "\n",
    "If we denote $X_i$ as the $i^{th}$ row of the matrix $X$, and $x_i$ the $i^{th}$ component of the vector $x$, we can express each Gauss Seidel algorithm as:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Solve $M \\vec{x} - \\vec{y} = 0$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M = L + D + U$ and $ D_i \\neq \\vec{0} \\forall i \\in 0,1,\\dots n-1$\n",
    "      \\STATE $x^0 \\leftarrow \\vec{0}$\n",
    "      \\WHILE{$ \\| M \\vec{x^k} - \\vec{y} \\| \\geq \\epsilon_{criterion}$}\n",
    "    \\FORALL{$i \\in 0,1,\\dots n-1$}\n",
    "      \\STATE $x_i^{k} \\leftarrow \\frac{y_i - \\vec{L_i + D_i}.\\vec{x^k}}{U_{ii}}$\n",
    "    \\ENDFOR\n",
    "    \\STATE $k \\leftarrow k+1$\n",
    "      \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "The inner loop in this algorithm, performed at each iteration, can itself be viewed as a coordinate descent method, solving the following problem:\n",
    "\t\n",
    "\\begin{align*}\n",
    "  (L + D) \\vec{x^k} + U \\vec{x^{k+1}} - \\vec{y} = 0 \\\\\n",
    "  \\vec{x^{k+1}} = U^-1 ( \\vec{y} - (L + D) \\vec{x^k} )\n",
    "\\end{align*}\n",
    "\n",
    "### Analysis\n",
    "\n",
    "If $M\\vec{x} - \\vec{y} = 0$ has a solution, the Gauss Seidel method converges if $M$ is symmetric, positive definite, or, as formalized in chapter 1 of Convex Optimization from Boyd & Vandenberghe, it lies in the positive definite cone $S^n_{++}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-c41bea2302b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.rand (numpy/random/mtrand/mtrand.c:17636)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.random_sample (numpy/random/mtrand/mtrand.c:13908)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont0_array (numpy/random/mtrand/mtrand.c:2055)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's try to solve a Positive Definite (PD) linear system\n",
    "using Gauss-Seidel iterations\n",
    "\"\"\"\n",
    "size=10\n",
    "M = np.random.rand((size,size))\n",
    "M = np.dot(M,M.T)\n",
    "Y = np.ones(size)\n",
    "\n",
    "#Now perform the decomposition\n",
    "D = np.diag(np.diagonal(M))\n",
    "ltMask = numpy.tri(L.shape[0])\n",
    "L = ltMask*(M-D)\n",
    "U = (1-ltMask)*M\n",
    "\n",
    "#New variables\n",
    "M1=np.dot(-np.diag(1./np.diagonal(G)),H)\n",
    "Y1=np.dot(np.diag(1./np.diagonal(G)),Y)\n",
    "omega = 0.2\n",
    "u,lamb,vh=np.linalg.svd(M1)\n",
    "spectRadM1 = lamb[0]\n",
    "print \"spectral radius of M1 is \"+str(spectRadM1)\n",
    "\n",
    "\n",
    "#Now solve\n",
    "Xk=np.zeros(M.shape[1])\n",
    "nbIter = 2000\n",
    "i = 0\n",
    "eps=1e-5\n",
    "\n",
    "while (i<nbIter) and not np.allclose(np.dot(M,Xk),Y,atol=eps):\n",
    "    nextIterate = np.dot(M1,Xk)+Y1\n",
    "    #Classic jacobi method\n",
    "    if( spectRadM1 < 1 ):\n",
    "        Xk = nextIterate\n",
    "    #Weighted jacobi method\n",
    "    else:\n",
    "        Xk += omega*(nextIterate - Xk)\n",
    "    i += 1\n",
    "\n",
    "if i<nbIter:\n",
    "    print \"Found a solution in \"+str(i)+\"/\"+str(nbIter)+\" iteration within absolute precision \"+str(eps)\n",
    "else:\n",
    "    print \"Jacobi iteration failed, residual norm is \"+str(np.linalg.norm(np.dot(M,Xk)-Y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
