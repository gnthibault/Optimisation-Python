{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov space based methods\n",
    "\n",
    "\n",
    "## Taylor expansion: from scalar to matrices\n",
    "\t\n",
    "Taylor expansion can easily be applied over differentiable scalar functions of one variables, this leads for instance to the following expansion, for $x$ close to $0$:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\frac{1}{1-x} &= 1 + x + x^2 + x^3 + ... \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} x^n\n",
    "\\end{align*}\n",
    "\n",
    "It is interesting to notice that the right part of this equality can also be interpreted as the sum of all terms of a geometric series with a common ratio of $x$, which is valid when $|x| < 1$.\n",
    "\n",
    "For the sake of our argument, we will assume that, $M$ is symmetric, so that all its power can be simultaneously diagonalised in the same eigen space, so that we juste have to extrapolate the scalar case to each diagonal elements.\n",
    "The previous expansion extended to the symmetric matrices case over a real field then reads:\n",
    "\n",
    "\\begin{align*}\n",
    "  (Id - M)^{-1} &= Id + M + M^2 + M^3 + ... \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} M^n\n",
    "\\end{align*}\n",
    "\n",
    "This expansion is valid for matrices whose operator norm $\\|M\\| = |\\lambda(M)_{max}|  < 1 $, and of course apply for nilpotent matrices, which have all their eigenvalues identically equal to zero, their characteristic polynomial being $x^n$.\n",
    "\n",
    "Using $|\\lambda(M)_{max}| =  c \\neq 0 $, excluding the already handled case of nilpotent matrices, we can extend the previous power series to all symmetric matrices M:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left( Id - \\left( \\frac{M}{c}\\right) \\right)^{-1} &= Id + \\frac{M}{c} + \\left(\\frac{M}{c} \\right)^2 + \\left(\\frac{M}{c} \\right)^3 + \\; \\dots \\\\\n",
    "    (c Id - M)^{-1} &= \\frac{1}{c} \\left( Id + \\frac{M}{c} + \\left(\\frac{M}{c} \\right)^2 + \\left(\\frac{M}{c} \\right)^3 + ... \\right) \\\\\n",
    "    &= \\frac{1}{c} \\sum_{n=0}^{\\infty} \\left(\\frac{M}{c} \\right)^n\n",
    "\\end{align*}\n",
    "\n",
    "Withou loss of generality, a variable change $M = c Id - P$, where $P$ is also symmetric, gives us\n",
    "\n",
    "\\begin{equation}\n",
    "    P^{-1} = \\frac{1}{c} \\sum_{n=0}^{\\infty} \\left(\\frac{c Id - P}{c} \\right)^n\n",
    "\\end{equation}\n",
    "\n",
    "We will see how this Taylor expansion is linked to preconditionned iterations in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eig are [ 1.  1.  1.  1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f49d9828fd0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEZCAYAAACjPJNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns/CE45xzrll4wnHOOdcsPOE455xrFv8P/VgThuDASAYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d9c4b2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let see if this method work, on a well conditionned matrix:\n",
    "to do so, we directly generates random symmetric matrix,\n",
    "and then tune its spetrum to an arbitrarily well conditionned\n",
    "matrix\n",
    "\"\"\"\n",
    "size = 5\n",
    "P = np.random.rand(size,size)\n",
    "P = np.dot(P,P.T)\n",
    "[v,Q] = np.linalg.eigh(P) #eig decomposition\n",
    "P = np.dot(Q,np.dot(np.diag(np.cumprod(1.000*np.ones(size))),Q.T))\n",
    "\n",
    "print \"eig are \"+str(np.linalg.eigvals(P))\n",
    "c=np.linalg.eigvals(P)[0]\n",
    "\n",
    "realP1 = np.linalg.inv(P)\n",
    "ratio = (c-P)/c\n",
    "Pn = np.identity(size)\n",
    "P1 = Pn.copy()\n",
    "frobError = [np.linalg.norm(realP1-P1/c)]\n",
    "\n",
    "#Taylor serie sum\n",
    "nb_taylor = 5\n",
    "for i in range(nb_taylor-1):\n",
    "    Pn = np.dot(Pn,ratio)\n",
    "    P1 += Pn\n",
    "    frobError.append(np.linalg.norm(realP1-P1/c))\n",
    "\n",
    "plt.figure()\n",
    "plt.title( \"Approximation error of P inverse, functions of Taylor serie order\")\n",
    "plt.xticks(range(nb_taylor))\n",
    "plt.xlabel(\"Number of terms in the Taylor serie\")\n",
    "plt.ylabel(\"Estimation error (Frobenius norm)\")\n",
    "plt.plot(range(nb_taylor), frobError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krylov Subspaces methods\n",
    "\n",
    "### Introduction: preconditionned iterations\n",
    "\n",
    "#### Yet another fixed point iteration method\n",
    "\n",
    "The approach seen in the notebook called FixedPointIterationsMethods, with a-priori decomposition of the matrix $M$ that lead to a fixed point iterations can also be generalized as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "  M \\vec{x} - \\vec{y} &= 0\\\\\n",
    "  (P-(P-M)) \\vec{x} - \\vec{y} &= 0\\\\\n",
    "  P \\vec{x} &= (P-M) \\vec{x} + \\vec{y} \\\\\n",
    "  P^{-1} P \\vec{x} &= P^{-1}(P-M) \\vec{x} + P^{-1} \\vec{y} \\\\\n",
    "  \\vec{x} &= \\vec{x} + P^{-1} ( \\vec{y} - M \\vec{x} )\n",
    "\\end{align*}\n",
    "\n",
    "This problem can simply be recasted as a fixed point search problem, that can be solved using the following iterations:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{k+1}} = \\vec{x^k} + P^{-1} ( \\vec{y} - M \\vec{x^k} ) \\\\\n",
    "  \\vec{x^{k+1}} = \\vec{x^k} + P^{-1} \\vec{r^k}\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = (Id-P^{-1}M)\\vec{x^k} + P^{-1}\\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "Where $ \\vec{r^k} = \\vec{y} - M \\vec{x^k} $ is called the residual and $P$ is called the preconditionner.\n",
    "\n",
    "As seen in the notebook FixedPointIterationsMethods, according to the Picard fixed point theorem, the preconditioned iterations algorithm converges if the operator $T : \\vec{x} \\rightarrow ( Id . + P^{-1} ( \\vec{y} - M . ) ) \\vec{x}$ is a strict contraction, i.e if there exists $\\rho \\in [0,1[$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "  \\forall (x,x') \\in \\mathbb{R}^n \\times \\mathbb{R}^n \\; &\\|Tx - Tx'\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "  &\\|(Id-P^{-1}M)(x-x')+P^{-1}\\vec{y}-P^{-1}\\vec{y}\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "  &\\|(Id-P^{-1}M)(x-x')\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This condition reduces to the constraint that the maximal singular value of $(Id-P^{-1}M) .$ should lie inside $[0,1[$. Using the property stating that if two matrices commutes, then they are diagonalizable in the same eigenspace, we can simply, notice that $Id$ matrix commutes with every matrix, and then say that the eigen values of the sum $Id-P^{-1}M$ are the sum of the eigen values of $Id$, which are identically $1$ and $-P^{-1}M$.\\\\\n",
    "\n",
    "This general property of commuting matrices can be trivially obtained for the specific case of the Identity:\n",
    "\n",
    "\\begin{align*}\n",
    "  (Id-A) \\vec{x} &= x - \\lambda \\vec{x} \\forall \\lambda \\in \\lambda(A)_i, i \\in [0,\\dots,n] \\\\\n",
    "  (Id-A) \\vec{x} &= (1-\\lambda) \\vec{x} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This help us to derive the initial constraint over $Id-P^{-1}M$ as a constraint only over $P^{-1}M$:\n",
    "\t\n",
    "\\begin{equation}\n",
    "  0 \\le \\lambda(P^{-1}M)_{max} \\leq 1  \n",
    "\\end{equation}\n",
    "\n",
    "In addition, the preconditionner could feature some useful properties, like the fact that $P^{-1}$ is easy to compute, and eventually $P$ has a low condition number, or $P$ may even carry some a-priori informations over the problem.\n",
    "\n",
    "We can notice that, if we choose the preconditionner such that $P{-1} = \\frac{1}{\\lambda(M^\\intercal M)_{max}} M^\\intercal $ we get an instance of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Krylov subspaces, and how are they linked to the solution of a linear system\n",
    "  \n",
    "Let reconsider the equation of preconditionned iterations, where we will incorporate the preconditionner $P$ directly to $M$ and $y$ such that we have our new $M$ equivalent to $P^{-1}M$ and our new $y$ equivalent to $P^{-1}\\vec{y}$. The preconditionned iterations now reads\n",
    " \n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = (Id-M)\\vec{x^k} + \\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "assuming $\\vec{x^{0}} = \\vec{y}$, we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{0}} &= \\vec{y} \\\\\n",
    "  \\vec{x^{1}} &= (Id-M)\\vec{y} + \\vec{y} \\\\\n",
    "  \\vec{x^{2}} &= (Id-M)^2\\vec{y} + (Id-M)\\vec{y} + \\vec{y} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, using the matrix version of the Taylor expansion exposed in the previous section, we can give another proof of the convergence of the $x^k$ series:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{k}} &= \\sum_{n=0}^{k} (Id-M)^n \\vec{y} \\\\\n",
    "  \\underset{k\\rightarrow \\infty}{lim} \\; \\vec{x^{k}} &= (Id-(Id-M))^{-1} \\vec{y} \\\\\n",
    "  \\underset{k\\rightarrow \\infty}{lim} \\; \\vec{x^{k}} &= M^{-1} \\vec{y} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The fact that the inverse of $M$ in $\\vec{y}$ can be expressed in a basis made of vectors of the form $M^n \\vec{y}, n \\in 0,1,\\dots$ was first discovered by Krylov, using a more general approach based on the Cayley-Hamilton theorem, which do not impose condition over the spectral radius, and where the polynomial in $M$ was the characteristic polynomial of the matrix $M$.\n",
    "      \n",
    "In the general case, the space $\\mathcal{K}_r$ spanned by the $r$ first vectors : $M^n \\vec{y}, n \\in 0,1,\\dots, r-1 $ is known as the order-r Krylov subspace, and it can be shown that its basis, made of the vectors $M^n \\vec{y}, n \\in 0,1,\\dots, r-1$ is free while $r \\leq r_{max}$.\\\\\n",
    "\n",
    "First, proving that the basis of $\\mathcal{K}_1$ is free is direct, because there is only one vector, then, as linear combinations of Krylov basis generates a vector in $\\mathbb{R}^n$ for the $n \\times n$ matrix $M$ Krylov subspace $\\mathcal{K}_r$ dimension cannot exceed $n$ in any case, so a free basis of $\\mathcal{K}_r$ cannot contain more than $n$ vectors.\n",
    "\n",
    "Now we have to show that if only one more vector $M^{k+1} \\vec{y}$ makes the basis $M^n \\vec{y}, n \\in 0,1,\\dots,k, k+1$ linearly dependant, then $k+1=r_{max}$ and all following basis vectors will also be linearly dependant.\n",
    "\n",
    "We will give the intuition of the proof using a reccurence strategy showing that if $M^{k+1} \\vec{y} \\in \\mathcal{K}_{r_{max}}$ then $ \\forall q > 0, M^{k+q} \\in \\mathcal{K}_{r_{max}}$:\\\\\n",
    "\n",
    "We begin assuming that the property is true for one $q_0$ : $\\exists q_0 \\leq 0 \\backslash M^{r+q_0} \\in \\mathcal{K}_r$ and see what happen for $q_0+1$:\n",
    "\n",
    "\\begin{align*}\n",
    "  M^{r+q_0} \\vec{y} &\\in \\mathcal{K}_r \\\\\n",
    "  M^{r+q_0} \\vec{y} &= \\sum_{n=0}^{r-1} \\alpha_n M^{n} \\vec{y} \\\\\n",
    "  M^{r+q_0+1} \\vec{y} &= M \\sum_{n=0}^{r-1} \\alpha_n M^{n} \\vec{y} \\\\\n",
    "  &= \\sum_{n=0}^{r-1} \\alpha_n M^{n+1} \\vec{y} \\\\\n",
    "  &= \\sum_{n=1}^{r-1} \\alpha_{n-1} M^{n} \\vec{y} + \\alpha_{r-1} M^{r} \\vec{y} \\; \\text{ and as } r-1 < r \\leq r+q_0 \\\\\n",
    "  &= \\sum_{n=1}^{r-1} \\alpha_{n-1} M^{n} \\vec{y} + \\alpha_{r} \\sum_{n=0}^{r-1} \\beta_n M^{n} \\vec{y}\\\\\n",
    "  &= \\sum_{n=0}^{r-1} \\gamma_{n} M^{n} \\vec{y}\\\\\n",
    "  M^{r+q_0+1} &\\in \\mathcal{K}_r \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This property shows us that, in $\\mathbb{R}^n$, the $\\mathcal{K}_{r}$ subspace is possibly of lower dimension than $n$, and that a solution of a linear set of equations may be expressed or approximated within this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Krylov basis and numerical stability\n",
    "\n",
    "\tFrom the previous section ~ref{sub::KrylovSubspaceExplanation}, we can derive the Krylov matrix $K$:\n",
    "\t\n",
    "\t\\begin{equation}\n",
    "\t  \\begin{pmatrix} \\vec{y} M\\vec{y} M^2\\vec{y} \\dots M^{n-1}\\vec{y} \\end{pmatrix}\n",
    "\t\\end{equation}\n",
    "\n",
    "\tUnfortunately, in many cases, this basis cannot be used as this to express the solution for numerical reasons: assuming $M$ can be diagonalized and has $n$ different eigenvalues $\\lambda(M)_0, \\lambda(M)_1, \\dots \\lambda(M)_{n-1}$ associated with the eigenvectors $ \\vec{v_0},  \\vec{v_1}, \\dots  \\vec{v_{n-1}}$ we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{y} = \\sum_{i=0}^{n-1} \\alpha_i  \\vec{v_i} \\\\\n",
    "  M^n \\vec{y} = \\sum_{i=0}^{n-1} \\alpha_i \\lambda(M)_i^n  \\vec{v_1}\n",
    "\\end{align*}\n",
    "\n",
    "\tUnfortunately, when the ratio $ \\left( \\frac{\\alpha_{max}}{\\alpha_i} \\right) \\left( \\frac{\\lambda(M)_{max}}{\\lambda(M)_i} \\right)^n $ where $max$ stands for the index of the maximum eigenvalue, exceed $2^n$ where n is the number of significand bits in the floating point representation of a computer, the accuracy of any krylov subspace method would collapse.\n",
    "\tIn this case, all numerical approximation of the Krylov basis vectors exceeding order $n$ becomes colinear to the maximum eigenvector $ \\vec{v_{max}}$.\n",
    "\tIn practice, this drawback has a high probability of occurrence especially in high dimensional settings, with for instance $k \\gg n$ dimensions, where the first $k$ vectors of the Krylov basis are linearly independants, though potentially carrying some informations about the solution.\n",
    "\n",
    "\t  \\subsubsection{Arnoldi orthogonalization strategy for Krylov basis}\n",
    "\n",
    "\tTo overcome the numerical stability problem, Arnoldi designed a method that mixed Krylov basis method and Gram-Schmidt orthonormalization process.\n",
    "\tThis simple method can be summarized as follows:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Generate Arnoldi basis vectors $\\vec{v_j}$}\n",
    "    \\begin{algorithmic}\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j < n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\FORALL{$i \\in 0,\\dots j-1$}\n",
    "        \\STATE $\\alpha = \\langle \\vec{w},\\vec{v_i} \\rangle $\n",
    "        \\STATE $\\vec{w} = \\vec{w} - \\alpha \\vec{v_i}$\n",
    "      \\ENDFOR\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{\\| \\vec{w} \\|}$\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "\tLet's define $h_{ij}$ such that: \\\\\n",
    "\t\n",
    "\\begin{align*}\n",
    "  h_{ij} = \n",
    "  \\begin{cases}\n",
    "    \\langle M\\vec{v_j},\\vec{v_i} \\rangle \\; \\text{ if } \\; j \\geq i\\\\\n",
    "    \\| M\\vec{v_j} - \\sum_{k=0}^{j} \\langle M\\vec{v_j},\\vec{v_k} \\rangle \\vec{v_k} \\| \\; \\text{ if } \\; i = j+1\\\\\n",
    "    0 \\; \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Where the two first cases corresponds respectively to the orthogonalization coefficients, and the normalization coefficients of the Gram-Schmidt process described earlier.\n",
    "\n",
    "From there, we can define a $p+1 \\times p$ matrix $H_{p+1 p}$ as follows:\n",
    "\n",
    "\\begin{equation} \\label{ArnoldiHPPMatrix}\n",
    "  H_{p+1 p} =\n",
    "  \\begin{pmatrix}\n",
    "   h_{0 0} & h_{0 1} & \\dots  & h_{0 p} \\\\\n",
    "   h_{1 0} & h_{1 1} & \\dots  & h_{1 p} \\\\\n",
    "      0\t   & h_{2 1} & \\ddots & \\vdots \\\\\n",
    "      0\t   & \\ddots  & \\ddots & h_{pp} \\\\\n",
    "      0    &    0    &   0    & h_{p+1 p} \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\tIf we define $V_p$ as the matrix containing all first $p$ Arnoldi basis vectors $ \\vec{v_0},\\vec{v_1},\\dots,\\vec{v_{p-1}}$ as column vectors, given the orthonormal property obtain through Gram-Schmidt, we know that $V_p$ is a unitary matrix, so we have\n",
    "\t\n",
    "\\begin{equation} \\label{eq:ArnoldiUnitaryMatrix}\n",
    "  V_p^\\intercal V_p = Id_p\n",
    "\\end{equation}\n",
    "\n",
    "Using the matrix $H_{p+1 p}$, we can write:\n",
    "\n",
    "\\begin{equation} \\label{eq:ArnoldiHPPlusOneRelationShip}\n",
    "  M V_p = V_{p+1} H_{p+1p}\n",
    "\\end{equation}\n",
    "\n",
    "\tThe matrix product $V_{p+1} H_{p+1p}$ allows us to perform the column-wise inversion of the Gram-Schmidt process over $V_{p+1}$ to recover $M V_p$.\n",
    "\n",
    "From the property seen in eq ~\\ref{eq:ArnoldiHPPlusOneRelationShip}, we can write:\n",
    "\n",
    "\\begin{align*} \\label{eq:ArnoldiChangeOfBasisForM}\n",
    "  M V_p = V_{p+1} H_{p+1p} \\\\\n",
    "  V_p ^\\intercal M V_p = V_p ^intercal V_{p+1} H_{p+1p}\n",
    "\\end{align*}\n",
    "\n",
    "\tUsing the unitary property seen in eq ~\\ref{eq:ArnoldiUnitaryMatrix}, we see that $V_p^\\intercal V_{p+1}$ is a $p \\times p+1$ matrix composed of a $p \\times p$ identity matrix and a last $p+1 ^{th} $ column containing, at each row $i$, the result of $ \\langle \\vec{v_{p+1}},\\vec{v_i} \\rangle, i < j $ which are all zeros.\n",
    "We can deduce that $V_p^\\intercal V_{p+1}$ will discard the last row of $H_{p+1p}$, and then give $H_{p p}$ :\n",
    "\n",
    "\\begin{equation} \\label{eq:ArnoldiHPRelationShip}\n",
    "  H_{p p} = V_{p} ^\\intercal M V_{p}\n",
    "\\end{equation}\n",
    "\n",
    "\tThis result is important because, it shows that $H_{p p}$, is the expression of the projection of the linear operator represented by $M$ over the order-p Krylov subspace in the orthonormal basis of Arnolid vectors $V_p$, which can be seen as a rotation matrix. \\\\\n",
    "\tA point that will be very important in the next section, is that, by construction $H_{p p}$ is a Hessenberg superior matrix,\n",
    "\n",
    "### Symmetric matrices and Lanczos method for Krylov basis\n",
    "  \n",
    "\tIf the matrix $M$ is symmetric, following the relation found in eq ~\\ref{eq:ArnoldiChangeOfBasisForM} regarding to the unitary change of basis $V_p$, it can be easily shown that $H_{p p}$ is also a symmetric matrix:\n",
    "\t\n",
    "\\begin{align*}\n",
    "  H_{p p} = V_{p} ^\\intercal M V_{p}\n",
    "  H_{p p}^\\intercal &= (V_{p} ^\\intercal M V_{p})^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p} M^\\intercal V_{p}^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p} M V_{p}^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p}^\\intercal M V_{p} \\\\\n",
    "  H_{p p}^\\intercal &= H_{p p} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tAs, by construction, as seen in eq ~\\ref{ArnoldiHPPMatrix}, $H_{p p}$ is a Hessenberg superior matrix, we can conclude that, in the case of $M$ being, symmetric, we have that $H_{p p}$ is both Hessenberg superior and inferior, so it is a tridiagonal symmetric matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "  H_{p p} =\n",
    "  \\begin{pmatrix}\n",
    "    h_{0 0} & h_{0 1} &    0    &    0   &  \\dots &   \\dots    &   0      \\\\\n",
    "    h_{1 0} & h_{1 1} & h_{1 2} &    0   & \\ddots &   \\ddots   & \\vdots   \\\\\n",
    "      0\t    & h_{2 1} & h_{2 2} & \\ddots & \\ddots &   \\ddots   & \\vdots   \\\\\n",
    "    \\vdots  &    0    & h_{3 2} & \\ddots & \\ddots &    0       &    0     \\\\\n",
    "    \\vdots  & \\ddots  &     0   & \\ddots & \\ddots & h_{p-2p-1} &    0     \\\\\n",
    "    \\vdots  & \\ddots  & \\ddots  & \\ddots & \\ddots & h_{p-1p-1} & h_{p-1p} \\\\\n",
    "       0    & \\dots   &  \\dots  &  \\dots &    0   & h_{pp-1}   & h_{pp}   \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\tThis property has a huge impact over the Arnoldi orthogonalisation algorithm seen previously: we can see that the Gram-Schmidt orthogonalization process for the vector $M \\vec{v_j}$ will only imply the computation of the correlation with the two previous vectors of the process: $h_{j-2j}$ and $h_{j-1j}$ instead of all previously constructed vectors. The Arnoldi algorithm in this case now reads:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Generate Arnoldi basis vectors $\\vec{v_j}$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j \\leq n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\IF{$j > 1$}\n",
    "        \\STATE $h_{j-2j-1} = h_{j-1j-2}$\n",
    "        \\STATE $\\vec{w} = \\vec{w} - h_{j-2j} \\vec{v_{j-2}}$\n",
    "      \\ENDIF\n",
    "      \\STATE $h_{j-1j-1} = \\langle \\vec{w},\\vec{v_{j-1}} \\rangle $\n",
    "      \\STATE $\\vec{w} = \\vec{w} - h_{j-1j-1} \\vec{v_{j-1}}$\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $h_{jj-1} = \\| \\vec{w} \\|$\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{h_{jj-1}}$\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "\tThe Lanczos method for computing a numerically stable basis of the Krylov subspace then have a constant computation burden at every iteration, instead of a linearly growing number of correlation to compute at each iterations.\n",
    "\n",
    "\tWe can also add an important comment about the fact that $M$, $V_{p} ^\\intercal M V_{p}$ and $H_{p p}$ have the same eigenvalues, which could be easily proven using the unitary property of $V_p$ : $V_p^\\intercal V_p = Id_p$.\n",
    "\tIt appears that finding the eigenvalues of the tridiagonal $H_{p p}$ is often much simpler than performing a SVD directly over $M$, see for instance the Differential Quotient Difference with Shifts (DQDS) algorithm or more recent work by Coakley in \\cite{coakley2013fast} that present a fast $n log(n)$ divide and conquer algorithm to find eigenvalues of such matrices.\n",
    "\n",
    "### Krylov basis and practical resolution of linear equalities\n",
    "\n",
    "\tAs seen in section ~\\ref{sub::KrylovSubspaceExplanation}, the solution of $M^{-1} \\vec{y}$ of $M \\vec{x} = \\vec{y}$ can be expressed in terms of $M^p \\vec{y}, p = 0,1,\\dots,r-1$, which led us to define the order-r Krylov basis.\n",
    "\t\n",
    "\tWe have then seen a way to express the p-order Krylov basis in a more stable way using the Arnoldi basis, and its Lanczos version $V_p$. We can now define $\\vec{x_p}$, the approximation of the solution of $M \\vec{x} - \\vec{y} = 0$ in the order-p Arnoldi basis:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vec{x_p} = V_p \\vec{s_p}\n",
    "\\end{equation}\n",
    "\n",
    "From this can be defined common error metrics over this approximate solution, using the following vectors:\n",
    "\n",
    "\\begin{itemize}\n",
    "  \\item The error vector: $\\vec{e_p} = \\vec{x} - \\vec{x_p}$\n",
    "  \\item The residual vector: $\\vec{r_p} = \\vec{y} - M \\vec{x_p} $\n",
    "\\end{itemize}\n",
    "\n",
    "Krylov subspace related methods aims at minimizing $\\vec{e_p}$ or $\\vec{r_p}$ under appropriate norms.\\\\\n",
    "One of the most reknown method, first derived by Lanczos aims at minimizing $Lcz(\\vec{x_p})$ the scalar product of those two error vector, that can be also interpreted of the squared norm of the error vector using the scalar product defined by the matrix $M$:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= \\| \\vec{e_p} \\|_M^2 \\\\\n",
    "  &= \\| \\vec{x} - \\vec{x_p} \\|_M^2 \\\\\n",
    "  &= ( \\vec{x} - \\vec{x_p})^\\intercal M (\\vec{x} - \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . (M \\vec{x} - M \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . ( \\vec{y} - M \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . \\vec{r_p}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tThe Lanczos method can be viewed as the task of finding the vector $\\vec{x_p} \\in \\mathcal{K}_p$ such that its distance to $\\vec{x}$ is minimum in the inner product space $\\mathcal{M}^n$ defined by $M$ from $\\mathbb{R}^n$, for this method, we need $M$ to be symmetric and positive definite.\n",
    "\tThe vector $\\vec{x_p}$ that satisfy this definition is the orthogonal projection of $x$ over $\\mathcal{K}_p$ using the inner product $\\langle .,. \\rangle_M$.\n",
    "\n",
    "\tFollowing the definition of the inner product $\\langle .,. \\rangle_M$ and the definition of the orthogonal projection operator,\n",
    "\t$\n",
    "\t  \\begin{aligned}\n",
    "\t    P_{\\mathcal{K}_p} : \\mathcal{M}^n &\\rightarrow \\mathcal{K}_p \\\\\n",
    "\t    \\vec{x} &\\rightarrow \\vec{x_p}\n",
    "\t  \\end{aligned}\n",
    "\t$ in the space $\\mathcal{M}^n$ we have that the range $\\mathcal{K}_p$ and the nullspace $\\mathcal{M}^n \\setminus \\mathcal{K}_p$ are orthogonal subspaces in direct sum, which means for us that $\\mathcal{M}^n = \\mathcal{K}_p \\bigoplus_{M} \\mathcal{M}^n \\setminus \\mathcal{K}_p$ and :\n",
    "\n",
    "\\begin{equation}\n",
    "  \\forall (a,b) \\in \\mathcal{M}^n \\times \\mathcal{M}^n \\setminus \\mathcal{K}_p, \\langle a,b \\rangle_M = 0\n",
    "\\end{equation}\n",
    "  \n",
    "\tBy construction, it is obvious that $ \\vec{e_p} = \\vec{x} - \\vec{x_p} \\in \\mathcal{M}^n \\setminus \\mathcal{K}_p$ so we have\n",
    "\t\n",
    "\\begin{align*} \\label{eq:ProofResidueOrthogonalToKp}\n",
    "  \\langle \\vec{x_p}, \\vec{x} - \\vec{x_p} \\rangle_M &= 0 \\forall \\vec{x_p} \\in \\mathcal{K}_p, \\text{ and } x \\in \\mathbb{R}^n \\\\\n",
    "  \\vec{x_p} M ( \\vec{x} - \\vec{x_p} ) &= 0 \\\\\n",
    "  \\vec{x_p} . (\\vec{y} - M \\vec{x_p}) &= 0 \\\\\n",
    "  \\vec{x_p} . \\vec{r_p} &= 0\n",
    "\\end{align*}\n",
    "\n",
    "\tSo we have that, in $\\mathbb{R}^n$, the residual $\\vec{r_p}$ is always orthogonal to all vectors in $\\mathcal{K}_p$.\n",
    "\n",
    "### Lanczos iterations : tridiagonal matrix factorization\n",
    "\n",
    "Let's recall that $Lcz(\\vec{x_p})$ reads:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= ( \\vec{x} - \\vec{x_p})^\\intercal M (\\vec{x} - \\vec{x_p}) \\\\\n",
    "  &= (\\vec{x_p}^\\intercal M \\vec{x_p} + \\vec{x}^\\intercal M \\vec{x} - 2 \\vec{x_p}^\\intercal M \\vec{x}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can further develop this expression using $\\vec{x_p} = V_p \\vec{s_p}$, and delete the constant terms to get:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= \\vec{x_p}^\\intercal M \\vec{x_p} + \\vec{x}^\\intercal M \\vec{x} - 2 \\vec{x_p}^\\intercal M \\vec{x} \\\\\n",
    "  Lcz(\\vec{s_p}) &= \\vec{s_p}^\\intercal V_p^\\intercal M V_p \\vec{s_p} - 2 \\vec{s_p}^\\intercal V_p^\\intercal \\vec{y} \\\\\n",
    "  &= 2( \\frac{1}{2} \\langle V_p^\\intercal M V_p \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle ) \\\\\n",
    "  &= 2( \\frac{1}{2} \\langle H_{pp} \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle ) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Whose minimization can be reduced to\n",
    "\n",
    "\\begin{equation}\n",
    " Lcz(\\vec{s_p})= \\frac{1}{2} \\langle H_{pp} \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle \\\\\n",
    "\\end{equation}\n",
    "\n",
    "which is a famous case of convex, and differentiable objective whose jacobian along $\\vec{s_p}$ is:\n",
    "\n",
    "\\begin{equation}  \\label{eq:KrylovBasisMethodExplained}\n",
    "  \\frac{d Lcz(\\vec{s_p})}{d \\vec{x_p}} = H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "That vanish when $\\vec{s_p}$ is the solution of $H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} = 0$\n",
    "\n",
    "The positive definiteness of the tridiagonal $H_{pp}$ can be exploited by factorizing it using an alternative Choleski decomposition following the pattern $L_pD_pL_p^\\intercal$, where $D_p$ is a diagonal matrix, and $L_p$ is an inferior bidiagonal matrix, with all diagonal entries equal to zeros.\n",
    "\n",
    "We can perform the following iterative factorization with, at first step:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix}\n",
    "    h_{00}\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    h_{00}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    1\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "And at following steps, we can incrementally build the tridiagonal $H_{pp}$ matrix, and its alternative Choleski decomposition $L_pD_pL_p^\\intercal$:\n",
    "\n",
    "\\begin{align*}\n",
    "  H_{pp} &=\n",
    "  \\begin{pmatrix}\n",
    "    &   &            &   & \t    & 0       \\\\\n",
    "    &   & H_{p-1p-1} &   & \t    & \\vdots  \\\\\n",
    "    &   &            &   & \t    & 0       \\\\\n",
    "    &   &            &   & \t    & h_{p-1p}\\\\\n",
    "    & 0 & \\dots      & 0 & h_{pp-1} & h_{pp}\n",
    "  \\end{pmatrix} \\\\ &=\n",
    "  \\begin{pmatrix}\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    &   & L_{p-1}&    & \t &  \\vdots \\\\\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    & 0 & \\dots  & 0  & l_{pp-1} &    1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    &   & D_{p-1}& \t&   &  \\vdots \\\\\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    & 0 & \\dots  & \\dots & 0 &  d_{pp}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    &   &        \t  & \t  &   &    0      \\\\\n",
    "    &   & L_{p}^\\intercal & \t  &   &  \\vdots   \\\\\n",
    "    &   &        \t  & \t  &   &    0      \\\\\n",
    "    &   &       \t  & \t  &   &  l_{pp-1} \\\\\n",
    "    & 0 & \\dots  \t  & \\dots & 0 &     1 \n",
    "  \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\t\n",
    "Where $l_{pp-1} = \\frac{h_{pp-1}}{d_{p-1p-1}}$ and $d_{pp}=h_{pp}-l_{pp-1}*h_{pp-1}$\n",
    "\n",
    "In the meantime, the vector $\\vec{y_p} = V_p^\\intercal \\vec{y}$ has also to be updated as follows:\n",
    "\n",
    "\\begin{equation} \\label{eq:UpdateYpLanczos}\n",
    "  \\vec{y_p} =\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "      \\\\\n",
    "      \\vec{y_{p-1}}\n",
    "      \\\\\n",
    "      \\\\\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\langle \\vec{v_p},\\vec{y} \\rangle  \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Lanczos iterations in practice\n",
    "  \n",
    "We now have everything we need to design an algorithm that iteratively build:\n",
    "\n",
    "\\begin{itemize}\n",
    " \\item The Arnoldi orthogonalization of the Krylob basis $V_p$\n",
    " \\item The tridiagonal $H_{pp}$ matrix\n",
    " \\item The triangular matrix $L_p$, part of the $L_pD_pL_p^\\intercal$ decomposition of $H_{pp}$\n",
    " \\item The triangular matrix $D_p$, part of the $L_pD_pL_p^\\intercal$ decomposition of $H_{pp}$\n",
    " \\item The projection of $\\vec{y}$ over the basis $V_p$ : $\\vec{y_p}$\n",
    "\\end{itemize}\n",
    "\n",
    "Which can be implemented as follows:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Solve $M \\vec{x} - \\vec{y} = 0$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $vec{y_p} = \\vec{0}$\n",
    "      \\STATE $vec{L_p} = O_{n\\times n}$\n",
    "      \\STATE $vec{D_p} = O_{n\\times n}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j \\leq n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\IF{$j > 1$}\n",
    "        \\STATE $H_{j-2j-1} = H_{j-1j-2}$\n",
    "        \\STATE $\\vec{w} = \\vec{w} - H_{j-2j} \\vec{v_{j-2}}$\n",
    "      \\ENDIF\n",
    "      \\STATE $H_{j-1j-1} = \\langle \\vec{w},\\vec{v_{j-1}} \\rangle $\n",
    "      \\STATE $\\vec{w} = \\vec{w} - H_{j-1j-1} \\vec{v_{j-1}}$\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $H_{jj-1} = \\| \\vec{w} \\|$\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{H_{jj-1}}$\n",
    "        \\STATE $\\vec{y_p}[j] = \\langle \\vec{v_j},\\vec{y} \\rangle $\n",
    "        \\IF{$j > 1$}\n",
    "          \\STATE $L_{j-1j-2} = \\frac{H_{j-1j-2}}{D_{j-2j-2}}$\n",
    "          \\STATE $D_{j-1j-1} = H_{j-1j-1}-L_{j-1j-2}*H_{j-1,j-2}$\n",
    "        \\ELSE\n",
    "          \\STATE $D_{j-1j-1} = H_{j-1j-1}$\n",
    "        \\ENDIF\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "      \\STATE \\textbf{Last Part}\n",
    "      \\STATE Solve diagonalized system: $L_pD_pL_p^\\intercal \\vec{s_p} = \\vec{y_p}$\n",
    "      \\STATE Solution to initial problem is $\\vec{x_p} = V_p^\\intercal \\vec{s_p}$\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "## Conjugate Gradient approach\n",
    "\n",
    "\tAlthough the previous algorithm features some interesting properties, we can notice that it is not exactly a method that iteratively builds a solution vector. Instead, it iteratively builds a new problem, where $\\vec{s_p}$, the $M$-projection of the solution $\\hat{x}$ in the Krylov sub-space $\\mathcal{K}_p$ expressed in the basis $V_p$ can be solved whithout too much effort, thanks to the triangular/diagonal structure of the $LDL^\\intercal$ Choleski factorization.\n",
    "\n",
    "\tAn alternative formulation where the current solution $\\vec{x_p}$ would be iteratively constructed in the Krylov subspace $\\mathcal{K}_p$ using sequential vectors $\\vec{v_k}, k = 0,1,\\dots,p-1$ from the basis $V_p$ would allow us to save the computation needed to solve the full projection problem at each iteration, and the main update would look like:\n",
    "  \n",
    "\\begin{equation} \\label{eq:CoordinateConstructionSolution}\n",
    "  \\vec{x_{p+1}} = \\vec{x_{p}} + \\alpha_p \\vec{v_{p}}\n",
    "\\end{equation}\n",
    "\n",
    "This is the basic idea of the conjugate gradient algorithm, we will explain here:\n",
    "\n",
    "\\paragraph{General Idea of Conjugate Gradient in the framework of Krylov methods} \\label{paragraph:IdeaOfConjGrad}\n",
    "We must recall that the solution vector $\\vec{s_p}$ at each step is the orthogonal projection of the general solution $\\hat{x}$ over the Arnoldi basis $V_p$.\n",
    "But it is orthogonal with respect to a specific inner product definition that uses $M \\in S^n_{++}$, although the Arnoldi basis $V_p$ column vectors are orthogonal with respect to the canonical inner product that uses the identity matrix, so there is no guarantee that the first coordinates of $\\vec{s_p}$ in $V_p$ will remain the same at the same position in $\\vec{s_{p+1}}$ expressed in the new basis $V_{p+1}$.\n",
    "This is why, at each iteration of the previous algorithm, the linear combination of vectors from $V_p$ had to be fully recomputed by solving a structured linear set of equations.\n",
    "\n",
    "To overcome this problem, let's first recall that the problem to solve at each iteration as seen previously, reads:\n",
    "      \n",
    "\\begin{align*}\n",
    "  H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} = 0 \\\\\n",
    "  H_{pp} \\vec{s_p} = V_p^\\intercal \\vec{y} \\\\\n",
    "  H_{pp} \\vec{s_p} = \\vec{y_p}\n",
    "\\end{align*}\n",
    "\n",
    "\tAs $\\vec{y_p}$ and $\\vec{y_{p+1}}$ only differs in the last coordinate that have been added to $\\vec{y_{p+1}}$, as seen in eq. ~\\ref{eq:UpdateYpLanczos}, if we want to have a coordinate-wise resolution of the problem in the basis $V_p$, the same property should apply to $H_{pp} \\vec{s_p}$, which could be translated in:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{pmatrix}\n",
    "    1 \t   & 0 \t    & \\dots  & 0 \\\\\n",
    "    0 \t   & \\ddots & \\ddots & \\vdots \\\\\n",
    "    \\vdots & \\ddots & 1      & 0 \\\\\n",
    "    0 \t   & \\dots  & 0      & 0 \\\\\n",
    "  \\end{pmatrix}\n",
    "  H_{p+1p+1} \\vec{s_{p+1}} = H_{pp} \\vec{s_p} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Which imply that $H_{pp}$ should be constructed iteratively so that $H_{pp}$ and $H_{p+1p+1}$ only differs from the last line and the last column, but also that $H_{pp}$ should be diagonal for every $p$.\n",
    "\n",
    "To achieve this simple construction scheme, it is obvious that $H_{pp}$ should be diagonal. To do so, we have to find a new Krylov basis $W_p$ such that our new $H_{pp}$ is diagonal:\n",
    "\n",
    "\\begin{equation}\n",
    "  H_{pp} = W_{p} ^\\intercal M W_{p}\n",
    "\\end{equation}\n",
    "\n",
    "\tFortunately, we don't need to take the Krylov basis design problem from the beginning to handle this constraint. We have already seen in eq ~\\ref{eq:ProofResidueOrthogonalToKp} that the residue $\\vec{r_p}$ is orthogonal to $\\mathcal{K}_p$, and by construction, $\\vec{r_p} \\in \\mathcal{K}_{p+1}$ :\n",
    "\n",
    "\t\\begin{equation}\n",
    "\t  \\vec{r_p} = M \\vec{x_p} - \\vec{y}\n",
    "\t\\end{equation}\n",
    "\n",
    "\twith $\\vec{y} \\in \\mathcal{K}_1$, $\\vec{x_p} \\in \\mathcal{K}_p$ so $M \\vec{x_p} \\in \\mathcal{K}_{p+1}$. We conclude that $\\vec{r_p}$ lies in the orthogonal complement of $\\mathcal{K}_p$ in $\\mathcal{K}_{p+1}$ that we can write $\\mathcal{K}_{p+1} \\setminus \\mathcal{K}_p$ which is of dimension $1$ and has an orthonormal basis $\\vec{v_{p}}$. So every $\\vec{r_p}$ can be written $\\alpha \\vec{v_{p}}$.\n",
    "\n",
    "This last remark can be directly translated into a matrix equality, let $R_p$ be the matrix of all ordered $p$ first residual vectors $\\vec{r_k}, k \\in 0,1,\\dots, p-1$, we can write:\n",
    "    \n",
    "\\begin{equation}\n",
    "  R_p = V_p \\Delta_p\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\Delta_p$ is a diagonal matrix, and we can now extend properties valid for the decomposition of $H_{pp}$ :\n",
    "\n",
    "\\begin{align*}\n",
    "  &R_p^\\intercal M R_p \\\\\n",
    "  = &\\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p \\\\\n",
    "  = &\\Delta_p^\\intercal H_{pp} \\Delta_p \\\\\n",
    "  = &\\tilde{H}_{pp} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tWhere the matrix $\\tilde{H}_{pp}$ that is equivalent to the projection of the linear operator $M$ over $\\mathcal{K}_p$ expressed in the basis $R_p$ is still tridiagonal and positive definite, and admits an alternative Choleski decomposition of the form $ \\tilde{H}_{pp} = \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal $ such that we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  &R_p^\\intercal M R_p &= \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal \\\\\n",
    "  \\Leftrightarrow & \\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p &= \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal \\\\\n",
    "  \\Leftrightarrow & \\tilde{L}_p^{-1} \\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p \\tilde{L}_p^{-\\intercal} &= \\tilde{D}_p \\\\\n",
    "  \\Leftrightarrow & W_{p} &= R_p \\tilde{L}_p^{-\\intercal} \n",
    "\\end{align*}\n",
    "\n",
    "Column vectors from $W_{p}$ are linear combinations of residual vectors, they form a basis of $\\mathcal{K}_p$ and they features the desired property stated in paragraph ~\\ref{paragraph:IdeaOfConjGrad} : they are all orthogonal to each other with respect to the inner product $\\langle .,. \\rangle_M$, $\\mathcal{M}^n$.\n",
    "\t\n",
    "Knowing that $\\tilde{L}_p$ is bidiagonal inferior by construction, as seen in eq ~\\ref{eq:TridiagonalCholeskiFactorizationMatrices}, we can also derive a short recurrence pattern over $\\vec{w_0}, \\vec{w_1}, \\dots, \\vec{w_{p-1}}$ the column vector of $W_p$, assuming that $\\gamma_0, \\gamma_1, \\dots, \\gamma_{p-1}$ are the subdiagonal elements of $\\tilde{L}_p$:\n",
    "\n",
    "\\begin{align*}\n",
    "  W_p \\tilde{L}_p^t &= R_p \\\\\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_0}\\\\&\\end{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_1}\\\\&\\end{pmatrix}\n",
    "    \\dots\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_{p-1}}\\\\&\\end{pmatrix}\n",
    "    \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    1 & \\gamma_0 & 0 & \\dots & 0 \\\\\n",
    "    0 & 1 & \\gamma_1 & \\ddots & \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "    \\vdots & \\ddots & \\ddots & 1 & \\gamma_{p-2} \\\\\n",
    "    0 & \\dots & \\dots & 0 & 1 \\\\\n",
    "  \\end{pmatrix} &=\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_0}\\\\&\\end{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_1}\\\\&\\end{pmatrix}\n",
    "    \\dots\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_{p-1}}\\\\&\\end{pmatrix}\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This formulation may give us a way to construct $w_{j+1}$ with a simple iterative scheme, assuming that $\\vec{x_0} = \\vec{0}$:\n",
    "\n",
    "\\begin{align*}\n",
    "  &W_p \\tilde{L}_p^{\\intercal} &= R_p \\\\\n",
    "  \\Rightarrow & \\vec{r_0} &= \\vec{w_0} = \\vec{y} - M \\vec{x_0} \\; \\text{ and } \\\\\n",
    "  & \\vec{r_{j+1}} &= \\vec{w_{j+1}} + \\gamma_{j} \\vec{w_j} \\\\\n",
    "  \\Leftrightarrow & \\vec{w_{j+1}} &= \\vec{r_{j+1}} - \\gamma_{j} \\vec{w_j} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This relationship between $\\vec{w_{j+1}}$ and $\\vec{w_j}$ can be further exploited, if we use the fact that all vectors from $W_p$ are pairwise orthogonal with respect to the inner product defined by $M$:\n",
    "\n",
    "\\begin{align*}\n",
    "  & \\langle \\vec{w_{j+1}},\\vec{w_j} \\rangle_M &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j+1}} - \\gamma_{j} \\vec{w_j},\\vec{w_j} \\rangle_M &= 0 \\\\\n",
    "  \\Leftrightarrow & \\vec{r_{j+1}}^\\intercal M \\vec{w_j} - \\gamma_{j} \\vec{w_j}^\\intercal M \\vec{w_j} &= 0 \\\\\n",
    "  \\Leftrightarrow & \\gamma_{j} &= \\frac{\\vec{r_{j+1}}^\\intercal M \\vec{w_j}}{\\vec{w_j}^\\intercal M \\vec{w_j}}\n",
    "\\end{align*}\n",
    "\n",
    "\tWe now need to see if $\\vec{r_{j+1}}$ can be found using a simple recursion. To do so, we have to use the coordinate-wise construction of the solution exposed in eq ~\\ref{eq:CoordinateConstructionSolution} :\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x_{j+1}} &= \\vec{x_{j}} + \\alpha_j \\vec{w_{j}} \\\\\n",
    "  \\vec{y} - M \\vec{x_{j+1}} &= \\vec{y} - M \\vec{x_{j}} - \\alpha_j M \\vec{w_{j}} \\\\\n",
    "  \\vec{r_{j+1}} &= \\vec{r_{j}} - \\alpha_j M \\vec{w_{j}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tHere, we can use the fact that $\\vec{r_{p}}$ is orthogonal to $\\mathcal{K}_p$, and especially $\\vec{r_{j}}$ is orthogonal to $\\vec{w_{j-1}}$ so that we can write:\n",
    "\n",
    "\\begin{align*}\n",
    "  & \\langle \\vec{r_{j+1}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j}} - \\alpha_{j} M \\vec{w_{j}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j}},\\vec{w_{j}} \\rangle - \\alpha_{j} \\langle M \\vec{w_{j}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\vec{r_{j}}^\\intercal \\vec{w_{j}} - \\alpha_{j} \\vec{w_{j}}^\\intercal M \\vec{w_{j}} &= 0 \\\\\n",
    "  \\Leftrightarrow & \\alpha_{j} &= \\frac{ \\vec{r_{j}}^\\intercal \\vec{w_{j}} }{ \\vec{w_{j}}^\\intercal M \\vec{w_{j}} }\n",
    "\\end{align*}\n",
    "\n",
    "\t\n",
    "### Conjugate Gradient algorithm in practice\n",
    "\n",
    "The force of the Conjugate gradient algorithm lies in the fact that it iteratively builds a diagonal problem which can be solved one coordinate per iteration, without ``forgetting'' about the previous iterations. More interestingly, it is not solved in the canonical coordinate system of $\\mathbb{R}^n$ but instead in a basis of pairwise orthogonal vectors with respect to the matrix $M$, called ``conjugate vectors'', which are iteratively built using the gradient of the function $f(\\vec{x})$ we want to minimize at the current solution estimate : $f(\\vec{x}) = \\frac{1}{2} (\\vec{x}-\\vec{\\hat{x}} ) (M \\vec{x} - \\vec{y})  $.\n",
    "\n",
    "By design, these vectors are able to catch a large part of the error at every iteration, and especially, if the matrix $M$ defines an ellipsoid with an important anisotropy, the conjugate strategy helps to avoid the drawback of descent methods that are often ``stuck'' in valleys.\n",
    "\n",
    "The algorithm reads:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Solve $M \\vec{x} - \\vec{y} = 0$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{r_0} = \\vec{y}$\n",
    "      \\STATE $\\vec{w_0} = \\vec{r_0}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$j \\leq n$}\n",
    "      \\STATE $\\alpha_{i-1} = \\frac{ \\vec{r_{i-1}}^\\intercal \\vec{w_{i-1}} }{ \\vec{w_{i-1}}^\\intercal M \\vec{w_{i-1}} }$\n",
    "      \\STATE $\\vec{x_i} = \\vec{x_{i-1}} + \\alpha_{i-1} \\vec{w_{j-1}}$\n",
    "      \\STATE $\\vec{r_i} = \\vec{r_{i-1}} - \\alpha_{i-1} M\\vec{w_{j-1}}$\n",
    "      \\IF{$\\vec{r_i} \\neq \\vec{0}$}\n",
    "        \\STATE $\\gamma_{i-1} = \\frac{ \\vec{r_{i}}^\\intercal M \\vec{w_{i-1}} }{ \\vec{w_{i-1}}^\\intercal M \\vec{w_{i-1}} }$\n",
    "        \\STATE $\\vec{w_i} = \\vec{r_{i}} - \\gamma_{i-1} \\vec{w_{j-1}}$\n",
    "      \\ELSE\n",
    "          \\STATE End iterations\n",
    "      \\ENDIF\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
