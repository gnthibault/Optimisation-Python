{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov space based methods\n",
    "\n",
    "\n",
    "## Taylor expansion: from scalar to matrices\n",
    "\t\n",
    "Taylor expansion can easily be applied over differentiable scalar functions of one variables, this leads for instance to the following expansion, for $x$ close to $0$:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\frac{1}{1-x} &= 1 + x + x^2 + x^3 + ... \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} x^n\n",
    "\\end{align*}\n",
    "\n",
    "It is interesting to notice that the right part of this equality can also be interpreted as the sum of all terms of a geometric series with a common ratio of $x$, which is valid when $|x| < 1$.\n",
    "\n",
    "Using the method of telescoping series, the previous expansion can be extended to the matrix case over a real field as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "  (Id - M)^{-1} &= Id + M + M^2 + M^3 + ... \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} M^n\n",
    "\\end{align*}\n",
    "\n",
    "This expansion is valid for square matrices whose operator norm $\\|M\\| = |\\lambda(M)_{max}|  < 1 $, and of course apply for nilpotent matrices, which have all their eigenvalues identically equal to zero, their characteristic polynomial being $x^n$.\n",
    "\n",
    "Using $|\\lambda(M)_{max}| =  c \\neq 0 $, excluding the already handled case of nilpotent matrices, we can extend the previous power series to all matrices:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left( Id - \\left( \\frac{M}{c}\\right) \\right)^{-1} &= Id + \\frac{M}{c} + \\left(\\frac{M}{c} \\right)^2 + \\left(\\frac{M}{c} \\right)^3 + \\; \\dots \\\\\n",
    "    (c Id - M)^{-1} &= \\frac{1}{c} \\left( Id + \\frac{M}{c} + \\left(\\frac{M}{c} \\right)^2 + \\left(\\frac{M}{c} \\right)^3 + ... \\right) \\\\\n",
    "    &= \\frac{1}{c} \\sum_{n=0}^{\\infty} \\left(\\frac{M}{c} \\right)^n\n",
    "\\end{align*}\n",
    "\n",
    "We will see how this Taylor expansion is linked to preconditionned iterations in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krylov Subspaces methods\n",
    "\n",
    "### Introduction: preconditionned iterations\n",
    "\n",
    "#### Yet another fixed point iteration method\n",
    "\n",
    "The approach seen in the notebook called FixedPointIterationsMethods, with a-priori decomposition of the matrix $M$ that lead to a fixed point iterations can also be generalized as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "  M \\vec{x} - \\vec{y} &= 0\\\\\n",
    "  (P-(P-M)) \\vec{x} - \\vec{y} &= 0\\\\\n",
    "  P \\vec{x} &= (P-M) \\vec{x} + \\vec{y} \\\\\n",
    "  P^{-1} P \\vec{x} &= P^{-1}(P-M) \\vec{x} + P^{-1} \\vec{y} \\\\\n",
    "  \\vec{x} &= \\vec{x} + P^{-1} ( \\vec{y} - M \\vec{x} )\n",
    "\\end{align*}\n",
    "\n",
    "This problem can simply be recasted as a fixed point search problem, that can be solved using the following iterations:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{k+1}} = \\vec{x^k} + P^{-1} ( \\vec{y} - M \\vec{x^k} ) \\\\\n",
    "  \\vec{x^{k+1}} = \\vec{x^k} + P^{-1} \\vec{r^k}\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = (Id-P^{-1}M)\\vec{x^k} + P^{-1}\\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "Where $ \\vec{r^k} = \\vec{y} - M \\vec{x^k} $ is called the residual and $P$ is called the preconditionner.\n",
    "\n",
    "As seen in the notebook FixedPointIterationsMethods, according to the Picard fixed point theorem, the preconditioned iterations algorithm converges if the operator $T : \\vec{x} \\rightarrow ( Id . + P^{-1} ( \\vec{y} - M . ) ) \\vec{x}$ is a strict contraction, i.e if there exists $\\rho \\in [0,1[$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "  \\forall (x,x') \\in \\mathbb{R}^n \\times \\mathbb{R}^n \\; &\\|Tx - Tx'\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "  &\\|(Id-P^{-1}M)(x-x')+P^{-1}\\vec{y}-P^{-1}\\vec{y}\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "  &\\|(Id-P^{-1}M)(x-x')\\| \\leq \\rho \\|x-x'\\| \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This condition reduces to the constraint that the maximal singular value of $(Id-P^{-1}M) .$ should lie inside $[0,1[$. Using the property stating that if two matrices commutes, then they are diagonalizable in the same eigenspace, we can simply, notice that $Id$ matrix commutes with every matrix, and then say that the eigen values of the sum $Id-P^{-1}M$ are the sum of the eigen values of $Id$, which are identically $1$ and $-P^{-1}M$.\\\\\n",
    "\n",
    "This general property of commuting matrices can be trivially obtained for the specific case of the Identity:\n",
    "\n",
    "\\begin{align*}\n",
    "  (Id-A) \\vec{x} &= x - \\lambda \\vec{x} \\forall \\lambda \\in \\lambda(A)_i, i \\in [0,\\dots,n] \\\\\n",
    "  (Id-A) \\vec{x} &= (1-\\lambda) \\vec{x} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This help us to derive the initial constraint over $Id-P^{-1}M$ as a constraint only over $P^{-1}M$:\n",
    "\t\n",
    "\\begin{equation}\n",
    "  0 \\le \\lambda(P^{-1}M)_{max} \\leq 1  \n",
    "\\end{equation}\n",
    "\n",
    "In addition, the preconditionner could feature some useful properties, like the fact that $P^{-1}$ is easy to compute, and eventually $P$ has a low condition number, or $P$ may even carry some a-priori informations over the problem.\n",
    "\n",
    "We can notice that, if we choose the preconditionner such that $P{-1} = \\frac{1}{\\lambda(M^\\intercal M)_{max}} M^\\intercal $ we get an instance of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Krylov subspaces, and how are they linked to the solution of a linear system\n",
    "  \n",
    "Let reconsider the equation of preconditionned iterations, where we will incorporate the preconditionner $P$ directly to $M$ and $y$ such that we have our new $M$ equivalent to $P^{-1}M$ and our new $y$ equivalent to $P^{-1}\\vec{y}$. The preconditionned iterations now reads\n",
    " \n",
    "\\begin{equation}\n",
    "  \\vec{x^{k+1}} = (Id-M)\\vec{x^k} + \\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "assuming $\\vec{x^{0}} = \\vec{y}$, we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{0}} &= \\vec{y} \\\\\n",
    "  \\vec{x^{1}} &= (Id-M)\\vec{y} + \\vec{y} \\\\\n",
    "  \\vec{x^{2}} &= (Id-M)^2\\vec{y} + (Id-M)\\vec{y} + \\vec{y} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, using the matrix version of the Taylor expansion exposed in the previous section, we can give another proof of the convergence of the $x^k$ series:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x^{k}} &= \\sum_{n=0}^{k} (Id-M)^n \\vec{y} \\\\\n",
    "  \\underset{k\\rightarrow \\infty}{lim} \\; \\vec{x^{k}} &= (Id-(Id-M))^{-1} \\vec{y} \\\\\n",
    "  \\underset{k\\rightarrow \\infty}{lim} \\; \\vec{x^{k}} &= M^{-1} \\vec{y} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The fact that the inverse of $M$ in $\\vec{y}$ can be expressed in a basis made of vectors of the form $M^n \\vec{y}, n \\in 0,1,\\dots$ was first discovered by Krylov, using a more general approach based on the Cayley-Hamilton theorem, which do not impose condition over the spectral radius, and where the polynomial in $M$ was the characteristic polynomial of the matrix $M$.\n",
    "      \n",
    "In the general case, the space $\\mathcal{K}_r$ spanned by the $r$ first vectors : $M^n \\vec{y}, n \\in 0,1,\\dots, r-1 $ is known as the order-r Krylov subspace, and it can be shown that its basis, made of the vectors $M^n \\vec{y}, n \\in 0,1,\\dots, r-1$ is free while $r \\leq r_{max}$.\\\\\n",
    "\n",
    "First, proving that the basis of $\\mathcal{K}_1$ is free is direct, because there is only one vector, then, as linear combinations of Krylov basis generates a vector in $\\mathbb{R}^n$ for the $n \\times n$ matrix $M$ Krylov subspace $\\mathcal{K}_r$ dimension cannot exceed $n$ in any case, so a free basis of $\\mathcal{K}_r$ cannot contain more than $n$ vectors.\n",
    "\n",
    "Now we have to show that if only one more vector $M^{k+1} \\vec{y}$ makes the basis $M^n \\vec{y}, n \\in 0,1,\\dots,k, k+1$ linearly dependant, then $k+1=r_{max}$ and all following basis vectors will also be linearly dependant.\n",
    "\n",
    "We will give the intuition of the proof using a reccurence strategy showing that if $M^{k+1} \\vec{y} \\in \\mathcal{K}_{r_{max}}$ then $ \\forall q > 0, M^{k+q} \\in \\mathcal{K}_{r_{max}}$:\\\\\n",
    "\n",
    "We begin assuming that the property is true for one $q_0$ : $\\exists q_0 \\leq 0 \\backslash M^{r+q_0} \\in \\mathcal{K}_r$ and see what happen for $q_0+1$:\n",
    "\n",
    "\\begin{align*}\n",
    "  M^{r+q_0} \\vec{y} &\\in \\mathcal{K}_r \\\\\n",
    "  M^{r+q_0} \\vec{y} &= \\sum_{n=0}^{r-1} \\alpha_n M^{n} \\vec{y} \\\\\n",
    "  M^{r+q_0+1} \\vec{y} &= M \\sum_{n=0}^{r-1} \\alpha_n M^{n} \\vec{y} \\\\\n",
    "  &= \\sum_{n=0}^{r-1} \\alpha_n M^{n+1} \\vec{y} \\\\\n",
    "  &= \\sum_{n=1}^{r-1} \\alpha_{n-1} M^{n} \\vec{y} + \\alpha_{r-1} M^{r} \\vec{y} \\; \\text{ and as } r-1 < r \\leq r+q_0 \\\\\n",
    "  &= \\sum_{n=1}^{r-1} \\alpha_{n-1} M^{n} \\vec{y} + \\alpha_{r} \\sum_{n=0}^{r-1} \\beta_n M^{n} \\vec{y}\\\\\n",
    "  &= \\sum_{n=0}^{r-1} \\gamma_{n} M^{n} \\vec{y}\\\\\n",
    "  M^{r+q_0+1} &\\in \\mathcal{K}_r \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This property shows us that, in $\\mathbb{R}^n$, the $\\mathcal{K}_{r}$ subspace is possibly of lower dimension than $n$, and that a solution of a linear set of equations may be expressed or approximated within this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Krylov basis and numerical stability\n",
    "\n",
    "\tFrom the previous section ~ref{sub::KrylovSubspaceExplanation}, we can derive the Krylov matrix $K$:\n",
    "\t\n",
    "\t\\begin{equation}\n",
    "\t  \\begin{pmatrix} \\vec{y} M\\vec{y} M^2\\vec{y} \\dots M^{n-1}\\vec{y} \\end{pmatrix}\n",
    "\t\\end{equation}\n",
    "\n",
    "\tUnfortunately, in many cases, this basis cannot be used as this to express the solution for numerical reasons: assuming $M$ can be diagonalized and has $n$ different eigenvalues $\\lambda(M)_0, \\lambda(M)_1, \\dots \\lambda(M)_{n-1}$ associated with the eigenvectors $ \\vec{v_0},  \\vec{v_1}, \\dots  \\vec{v_{n-1}}$ we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{y} = \\sum_{i=0}^{n-1} \\alpha_i  \\vec{v_i} \\\\\n",
    "  M^n \\vec{y} = \\sum_{i=0}^{n-1} \\alpha_i \\lambda(M)_i^n  \\vec{v_1}\n",
    "\\end{align*}\n",
    "\n",
    "\tUnfortunately, when the ratio $ \\left( \\frac{\\alpha_{max}}{\\alpha_i} \\right) \\left( \\frac{\\lambda(M)_{max}}{\\lambda(M)_i} \\right)^n $ where $max$ stands for the index of the maximum eigenvalue, exceed $2^n$ where n is the number of significand bits in the floating point representation of a computer, the accuracy of any krylov subspace method would collapse.\n",
    "\tIn this case, all numerical approximation of the Krylov basis vectors exceeding order $n$ becomes colinear to the maximum eigenvector $ \\vec{v_{max}}$.\n",
    "\tIn practice, this drawback has a high probability of occurrence especially in high dimensional settings, with for instance $k \\gg n$ dimensions, where the first $k$ vectors of the Krylov basis are linearly independants, though potentially carrying some informations about the solution.\n",
    "\n",
    "\t  \\subsubsection{Arnoldi orthogonalization strategy for Krylov basis}\n",
    "\n",
    "\tTo overcome the numerical stability problem, Arnoldi designed a method that mixed Krylov basis method and Gram-Schmidt orthonormalization process.\n",
    "\tThis simple method can be summarized as follows:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Generate Arnoldi basis vectors $\\vec{v_j}$}\n",
    "    \\begin{algorithmic}\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j < n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\FORALL{$i \\in 0,\\dots j-1$}\n",
    "        \\STATE $\\alpha = \\langle \\vec{w},\\vec{v_i} \\rangle $\n",
    "        \\STATE $\\vec{w} = \\vec{w} - \\alpha \\vec{v_i}$\n",
    "      \\ENDFOR\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{\\| \\vec{w} \\|}$\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "\tLet's define $h_{ij}$ such that: \\\\\n",
    "\t\n",
    "\\begin{align*}\n",
    "  h_{ij} = \n",
    "  \\begin{cases}\n",
    "    \\langle M\\vec{v_j},\\vec{v_i} \\rangle \\; \\text{ if } \\; j \\geq i\\\\\n",
    "    \\| M\\vec{v_j} - \\sum_{k=0}^{j} \\langle M\\vec{v_j},\\vec{v_k} \\rangle \\vec{v_k} \\| \\; \\text{ if } \\; i = j+1\\\\\n",
    "    0 \\; \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Where the two first cases corresponds respectively to the orthogonalization coefficients, and the normalization coefficients of the Gram-Schmidt process described earlier.\n",
    "\n",
    "From there, we can define a $p+1 \\times p$ matrix $H_{p+1 p}$ as follows:\n",
    "\n",
    "\\begin{equation} \\label{ArnoldiHPPMatrix}\n",
    "  H_{p+1 p} =\n",
    "  \\begin{pmatrix}\n",
    "   h_{0 0} & h_{0 1} & \\dots  & h_{0 p} \\\\\n",
    "   h_{1 0} & h_{1 1} & \\dots  & h_{1 p} \\\\\n",
    "      0\t   & h_{2 1} & \\ddots & \\vdots \\\\\n",
    "      0\t   & \\ddots  & \\ddots & h_{pp} \\\\\n",
    "      0    &    0    &   0    & h_{p+1 p} \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\tIf we define $V_p$ as the matrix containing all first $p$ Arnoldi basis vectors $ \\vec{v_0},\\vec{v_1},\\dots,\\vec{v_{p-1}}$ as column vectors, given the orthonormal property obtain through Gram-Schmidt, we know that $V_p$ is a unitary matrix, so we have\n",
    "\t\n",
    "\\begin{equation} \\label{eq:ArnoldiUnitaryMatrix}\n",
    "  V_p^\\intercal V_p = Id_p\n",
    "\\end{equation}\n",
    "\n",
    "Using the matrix $H_{p+1 p}$, we can write:\n",
    "\n",
    "\\begin{equation} \\label{eq:ArnoldiHPPlusOneRelationShip}\n",
    "  M V_p = V_{p+1} H_{p+1p}\n",
    "\\end{equation}\n",
    "\n",
    "\tThe matrix product $V_{p+1} H_{p+1p}$ allows us to perform the column-wise inversion of the Gram-Schmidt process over $V_{p+1}$ to recover $M V_p$.\n",
    "\n",
    "From the property seen in eq ~\\ref{eq:ArnoldiHPPlusOneRelationShip}, we can write:\n",
    "\n",
    "\\begin{align*} \\label{eq:ArnoldiChangeOfBasisForM}\n",
    "  M V_p = V_{p+1} H_{p+1p} \\\\\n",
    "  V_p ^\\intercal M V_p = V_p ^intercal V_{p+1} H_{p+1p}\n",
    "\\end{align*}\n",
    "\n",
    "\tUsing the unitary property seen in eq ~\\ref{eq:ArnoldiUnitaryMatrix}, we see that $V_p^\\intercal V_{p+1}$ is a $p \\times p+1$ matrix composed of a $p \\times p$ identity matrix and a last $p+1 ^{th} $ column containing, at each row $i$, the result of $ \\langle \\vec{v_{p+1}},\\vec{v_i} \\rangle, i < j $ which are all zeros.\n",
    "We can deduce that $V_p^\\intercal V_{p+1}$ will discard the last row of $H_{p+1p}$, and then give $H_{p p}$ :\n",
    "\n",
    "\\begin{equation} \\label{eq:ArnoldiHPRelationShip}\n",
    "  H_{p p} = V_{p} ^\\intercal M V_{p}\n",
    "\\end{equation}\n",
    "\n",
    "\tThis result is important because, it shows that $H_{p p}$, is the expression of the projection of the linear operator represented by $M$ over the order-p Krylov subspace in the orthonormal basis of Arnolid vectors $V_p$, which can be seen as a rotation matrix. \\\\\n",
    "\tA point that will be very important in the next section, is that, by construction $H_{p p}$ is a Hessenberg superior matrix,\n",
    "\n",
    "### Symmetric matrices and Lanczos method for Krylov basis\n",
    "  \n",
    "\tIf the matrix $M$ is symmetric, following the relation found in eq ~\\ref{eq:ArnoldiChangeOfBasisForM} regarding to the unitary change of basis $V_p$, it can be easily shown that $H_{p p}$ is also a symmetric matrix:\n",
    "\t\n",
    "\\begin{align*}\n",
    "  H_{p p} = V_{p} ^\\intercal M V_{p}\n",
    "  H_{p p}^\\intercal &= (V_{p} ^\\intercal M V_{p})^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p} M^\\intercal V_{p}^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p} M V_{p}^\\intercal \\\\\n",
    "  H_{p p}^\\intercal &= V_{p}^\\intercal M V_{p} \\\\\n",
    "  H_{p p}^\\intercal &= H_{p p} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tAs, by construction, as seen in eq ~\\ref{ArnoldiHPPMatrix}, $H_{p p}$ is a Hessenberg superior matrix, we can conclude that, in the case of $M$ being, symmetric, we have that $H_{p p}$ is both Hessenberg superior and inferior, so it is a tridiagonal symmetric matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "  H_{p p} =\n",
    "  \\begin{pmatrix}\n",
    "    h_{0 0} & h_{0 1} &    0    &    0   &  \\dots &   \\dots    &   0      \\\\\n",
    "    h_{1 0} & h_{1 1} & h_{1 2} &    0   & \\ddots &   \\ddots   & \\vdots   \\\\\n",
    "      0\t    & h_{2 1} & h_{2 2} & \\ddots & \\ddots &   \\ddots   & \\vdots   \\\\\n",
    "    \\vdots  &    0    & h_{3 2} & \\ddots & \\ddots &    0       &    0     \\\\\n",
    "    \\vdots  & \\ddots  &     0   & \\ddots & \\ddots & h_{p-2p-1} &    0     \\\\\n",
    "    \\vdots  & \\ddots  & \\ddots  & \\ddots & \\ddots & h_{p-1p-1} & h_{p-1p} \\\\\n",
    "       0    & \\dots   &  \\dots  &  \\dots &    0   & h_{pp-1}   & h_{pp}   \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\tThis property has a huge impact over the Arnoldi orthogonalisation algorithm seen previously: we can see that the Gram-Schmidt orthogonalization process for the vector $M \\vec{v_j}$ will only imply the computation of the correlation with the two previous vectors of the process: $h_{j-2j}$ and $h_{j-1j}$ instead of all previously constructed vectors. The Arnoldi algorithm in this case now reads:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Generate Arnoldi basis vectors $\\vec{v_j}$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j \\leq n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\IF{$j > 1$}\n",
    "        \\STATE $h_{j-2j-1} = h_{j-1j-2}$\n",
    "        \\STATE $\\vec{w} = \\vec{w} - h_{j-2j} \\vec{v_{j-2}}$\n",
    "      \\ENDIF\n",
    "      \\STATE $h_{j-1j-1} = \\langle \\vec{w},\\vec{v_{j-1}} \\rangle $\n",
    "      \\STATE $\\vec{w} = \\vec{w} - h_{j-1j-1} \\vec{v_{j-1}}$\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $h_{jj-1} = \\| \\vec{w} \\|$\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{h_{jj-1}}$\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "\tThe Lanczos method for computing a numerically stable basis of the Krylov subspace then have a constant computation burden at every iteration, instead of a linearly growing number of correlation to compute at each iterations.\n",
    "\n",
    "\tWe can also add an important comment about the fact that $M$, $V_{p} ^\\intercal M V_{p}$ and $H_{p p}$ have the same eigenvalues, which could be easily proven using the unitary property of $V_p$ : $V_p^\\intercal V_p = Id_p$.\n",
    "\tIt appears that finding the eigenvalues of the tridiagonal $H_{p p}$ is often much simpler than performing a SVD directly over $M$, see for instance the Differential Quotient Difference with Shifts (DQDS) algorithm or more recent work by Coakley in \\cite{coakley2013fast} that present a fast $n log(n)$ divide and conquer algorithm to find eigenvalues of such matrices.\n",
    "\n",
    "### Krylov basis and practical resolution of linear equalities\n",
    "\n",
    "\tAs seen in section ~\\ref{sub::KrylovSubspaceExplanation}, the solution of $M^{-1} \\vec{y}$ of $M \\vec{x} = \\vec{y}$ can be expressed in terms of $M^p \\vec{y}, p = 0,1,\\dots,r-1$, which led us to define the order-r Krylov basis.\n",
    "\t\n",
    "\tWe have then seen a way to express the p-order Krylov basis in a more stable way using the Arnoldi basis, and its Lanczos version $V_p$. We can now define $\\vec{x_p}$, the approximation of the solution of $M \\vec{x} - \\vec{y} = 0$ in the order-p Arnoldi basis:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vec{x_p} = V_p \\vec{s_p}\n",
    "\\end{equation}\n",
    "\n",
    "From this can be defined common error metrics over this approximate solution, using the following vectors:\n",
    "\n",
    "\\begin{itemize}\n",
    "  \\item The error vector: $\\vec{e_p} = \\vec{x} - \\vec{x_p}$\n",
    "  \\item The residual vector: $\\vec{r_p} = \\vec{y} - M \\vec{x_p} $\n",
    "\\end{itemize}\n",
    "\n",
    "Krylov subspace related methods aims at minimizing $\\vec{e_p}$ or $\\vec{r_p}$ under appropriate norms.\\\\\n",
    "One of the most reknown method, first derived by Lanczos aims at minimizing $Lcz(\\vec{x_p})$ the scalar product of those two error vector, that can be also interpreted of the squared norm of the error vector using the scalar product defined by the matrix $M$:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= \\| \\vec{e_p} \\|_M^2 \\\\\n",
    "  &= \\| \\vec{x} - \\vec{x_p} \\|_M^2 \\\\\n",
    "  &= ( \\vec{x} - \\vec{x_p})^\\intercal M (\\vec{x} - \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . (M \\vec{x} - M \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . ( \\vec{y} - M \\vec{x_p})\\\\\n",
    "  &= \\vec{e_p}^\\intercal . \\vec{r_p}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tThe Lanczos method can be viewed as the task of finding the vector $\\vec{x_p} \\in \\mathcal{K}_p$ such that its distance to $\\vec{x}$ is minimum in the inner product space $\\mathcal{M}^n$ defined by $M$ from $\\mathbb{R}^n$, for this method, we need $M$ to be symmetric and positive definite.\n",
    "\tThe vector $\\vec{x_p}$ that satisfy this definition is the orthogonal projection of $x$ over $\\mathcal{K}_p$ using the inner product $\\langle .,. \\rangle_M$.\n",
    "\n",
    "\tFollowing the definition of the inner product $\\langle .,. \\rangle_M$ and the definition of the orthogonal projection operator,\n",
    "\t$\n",
    "\t  \\begin{aligned}\n",
    "\t    P_{\\mathcal{K}_p} : \\mathcal{M}^n &\\rightarrow \\mathcal{K}_p \\\\\n",
    "\t    \\vec{x} &\\rightarrow \\vec{x_p}\n",
    "\t  \\end{aligned}\n",
    "\t$ in the space $\\mathcal{M}^n$ we have that the range $\\mathcal{K}_p$ and the nullspace $\\mathcal{M}^n \\setminus \\mathcal{K}_p$ are orthogonal subspaces in direct sum, which means for us that $\\mathcal{M}^n = \\mathcal{K}_p \\bigoplus_{M} \\mathcal{M}^n \\setminus \\mathcal{K}_p$ and :\n",
    "\n",
    "\\begin{equation}\n",
    "  \\forall (a,b) \\in \\mathcal{M}^n \\times \\mathcal{M}^n \\setminus \\mathcal{K}_p, \\langle a,b \\rangle_M = 0\n",
    "\\end{equation}\n",
    "  \n",
    "\tBy construction, it is obvious that $ \\vec{e_p} = \\vec{x} - \\vec{x_p} \\in \\mathcal{M}^n \\setminus \\mathcal{K}_p$ so we have\n",
    "\t\n",
    "\\begin{align*} \\label{eq:ProofResidueOrthogonalToKp}\n",
    "  \\langle \\vec{x_p}, \\vec{x} - \\vec{x_p} \\rangle_M &= 0 \\forall \\vec{x_p} \\in \\mathcal{K}_p, \\text{ and } x \\in \\mathbb{R}^n \\\\\n",
    "  \\vec{x_p} M ( \\vec{x} - \\vec{x_p} ) &= 0 \\\\\n",
    "  \\vec{x_p} . (\\vec{y} - M \\vec{x_p}) &= 0 \\\\\n",
    "  \\vec{x_p} . \\vec{r_p} &= 0\n",
    "\\end{align*}\n",
    "\n",
    "\tSo we have that, in $\\mathbb{R}^n$, the residual $\\vec{r_p}$ is always orthogonal to all vectors in $\\mathcal{K}_p$.\n",
    "\n",
    "### Lanczos iterations : tridiagonal matrix factorization\n",
    "\n",
    "Let's recall that $Lcz(\\vec{x_p})$ reads:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= ( \\vec{x} - \\vec{x_p})^\\intercal M (\\vec{x} - \\vec{x_p}) \\\\\n",
    "  &= (\\vec{x_p}^\\intercal M \\vec{x_p} + \\vec{x}^\\intercal M \\vec{x} - 2 \\vec{x_p}^\\intercal M \\vec{x}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can further develop this expression using $\\vec{x_p} = V_p \\vec{s_p}$, and delete the constant terms to get:\n",
    "\n",
    "\\begin{align*}\n",
    "  Lcz(\\vec{x_p}) &= \\vec{x_p}^\\intercal M \\vec{x_p} + \\vec{x}^\\intercal M \\vec{x} - 2 \\vec{x_p}^\\intercal M \\vec{x} \\\\\n",
    "  Lcz(\\vec{s_p}) &= \\vec{s_p}^\\intercal V_p^\\intercal M V_p \\vec{s_p} - 2 \\vec{s_p}^\\intercal V_p^\\intercal \\vec{y} \\\\\n",
    "  &= 2( \\frac{1}{2} \\langle V_p^\\intercal M V_p \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle ) \\\\\n",
    "  &= 2( \\frac{1}{2} \\langle H_{pp} \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle ) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Whose minimization can be reduced to\n",
    "\n",
    "\\begin{equation}\n",
    " Lcz(\\vec{s_p})= \\frac{1}{2} \\langle H_{pp} \\vec{s_p}, \\vec{s_p} \\rangle - \\langle V_p^\\intercal \\vec{y}, \\vec{s_p} \\rangle \\\\\n",
    "\\end{equation}\n",
    "\n",
    "which is a famous case of convex, and differentiable objective whose jacobian along $\\vec{s_p}$ is:\n",
    "\n",
    "\\begin{equation}  \\label{eq:KrylovBasisMethodExplained}\n",
    "  \\frac{d Lcz(\\vec{s_p})}{d \\vec{x_p}} = H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "That vanish when $\\vec{s_p}$ is the solution of $H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} = 0$\n",
    "\n",
    "The positive definiteness of the tridiagonal $H_{pp}$ can be exploited by factorizing it using an alternative Choleski decomposition following the pattern $L_pD_pL_p^\\intercal$, where $D_p$ is a diagonal matrix, and $L_p$ is an inferior bidiagonal matrix, with all diagonal entries equal to zeros.\n",
    "\n",
    "We can perform the following iterative factorization with, at first step:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix}\n",
    "    h_{00}\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    h_{00}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    1\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "And at following steps, we can incrementally build the tridiagonal $H_{pp}$ matrix, and its alternative Choleski decomposition $L_pD_pL_p^\\intercal$:\n",
    "\n",
    "\\begin{align*}\n",
    "  H_{pp} &=\n",
    "  \\begin{pmatrix}\n",
    "    &   &            &   & \t    & 0       \\\\\n",
    "    &   & H_{p-1p-1} &   & \t    & \\vdots  \\\\\n",
    "    &   &            &   & \t    & 0       \\\\\n",
    "    &   &            &   & \t    & h_{p-1p}\\\\\n",
    "    & 0 & \\dots      & 0 & h_{pp-1} & h_{pp}\n",
    "  \\end{pmatrix} \\\\ &=\n",
    "  \\begin{pmatrix}\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    &   & L_{p-1}&    & \t &  \\vdots \\\\\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    &   &        &    & \t &    0    \\\\\n",
    "    & 0 & \\dots  & 0  & l_{pp-1} &    1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    &   & D_{p-1}& \t&   &  \\vdots \\\\\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    &   &        & \t&   &    0    \\\\\n",
    "    & 0 & \\dots  & \\dots & 0 &  d_{pp}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    &   &        \t  & \t  &   &    0      \\\\\n",
    "    &   & L_{p}^\\intercal & \t  &   &  \\vdots   \\\\\n",
    "    &   &        \t  & \t  &   &    0      \\\\\n",
    "    &   &       \t  & \t  &   &  l_{pp-1} \\\\\n",
    "    & 0 & \\dots  \t  & \\dots & 0 &     1 \n",
    "  \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\t\n",
    "Where $l_{pp-1} = \\frac{h_{pp-1}}{d_{p-1p-1}}$ and $d_{pp}=h_{pp}-l_{pp-1}*h_{pp-1}$\n",
    "\n",
    "In the meantime, the vector $\\vec{y_p} = V_p^\\intercal \\vec{y}$ has also to be updated as follows:\n",
    "\n",
    "\\begin{equation} \\label{eq:UpdateYpLanczos}\n",
    "  \\vec{y_p} =\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "      \\\\\n",
    "      \\vec{y_{p-1}}\n",
    "      \\\\\n",
    "      \\\\\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\langle \\vec{v_p},\\vec{y} \\rangle  \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Lanczos iterations in practice\n",
    "  \n",
    "We now have everything we need to design an algorithm that iteratively build:\n",
    "\n",
    "\\begin{itemize}\n",
    " \\item The Arnoldi orthogonalization of the Krylob basis $V_p$\n",
    " \\item The tridiagonal $H_{pp}$ matrix\n",
    " \\item The triangular matrix $L_p$, part of the $L_pD_pL_p^\\intercal$ decomposition of $H_{pp}$\n",
    " \\item The triangular matrix $D_p$, part of the $L_pD_pL_p^\\intercal$ decomposition of $H_{pp}$\n",
    " \\item The projection of $\\vec{y}$ over the basis $V_p$ : $\\vec{y_p}$\n",
    "\\end{itemize}\n",
    "\n",
    "Which can be implemented as follows:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Solve $M \\vec{x} - \\vec{y} = 0$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{v_0} = \\frac{\\vec{y}}{\\| \\vec{y} \\|}$\n",
    "      \\STATE $vec{y_p} = \\vec{0}$\n",
    "      \\STATE $vec{L_p} = O_{n\\times n}$\n",
    "      \\STATE $vec{D_p} = O_{n\\times n}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{0}}$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$\\vec{w} \\neq \\vec{0}$ and $j \\leq n$}\n",
    "      \\STATE $\\vec{w} = M \\vec{v_{j-1}}$\n",
    "      \\IF{$j > 1$}\n",
    "        \\STATE $H_{j-2j-1} = H_{j-1j-2}$\n",
    "        \\STATE $\\vec{w} = \\vec{w} - H_{j-2j} \\vec{v_{j-2}}$\n",
    "      \\ENDIF\n",
    "      \\STATE $H_{j-1j-1} = \\langle \\vec{w},\\vec{v_{j-1}} \\rangle $\n",
    "      \\STATE $\\vec{w} = \\vec{w} - H_{j-1j-1} \\vec{v_{j-1}}$\n",
    "      \\IF{$\\vec{w} \\neq \\vec{0}$}\n",
    "        \\STATE $H_{jj-1} = \\| \\vec{w} \\|$\n",
    "        \\STATE $\\vec{v_j} = \\frac{\\vec{w}}{H_{jj-1}}$\n",
    "        \\STATE $\\vec{y_p}[j] = \\langle \\vec{v_j},\\vec{y} \\rangle $\n",
    "        \\IF{$j > 1$}\n",
    "          \\STATE $L_{j-1j-2} = \\frac{H_{j-1j-2}}{D_{j-2j-2}}$\n",
    "          \\STATE $D_{j-1j-1} = H_{j-1j-1}-L_{j-1j-2}*H_{j-1,j-2}$\n",
    "        \\ELSE\n",
    "          \\STATE $D_{j-1j-1} = H_{j-1j-1}$\n",
    "        \\ENDIF\n",
    "      \\ENDIF\n",
    "        \\STATE $j=j+1$\n",
    "       \\ENDWHILE\n",
    "      \\STATE \\textbf{Last Part}\n",
    "      \\STATE Solve diagonalized system: $L_pD_pL_p^\\intercal \\vec{s_p} = \\vec{y_p}$\n",
    "      \\STATE Solution to initial problem is $\\vec{x_p} = V_p^\\intercal \\vec{s_p}$\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}\n",
    "\n",
    "## Conjugate Gradient approach\n",
    "\n",
    "\tAlthough the previous algorithm features some interesting properties, we can notice that it is not exactly a method that iteratively builds a solution vector. Instead, it iteratively builds a new problem, where $\\vec{s_p}$, the $M$-projection of the solution $\\hat{x}$ in the Krylov sub-space $\\mathcal{K}_p$ expressed in the basis $V_p$ can be solved whithout too much effort, thanks to the triangular/diagonal structure of the $LDL^\\intercal$ Choleski factorization.\n",
    "\n",
    "\tAn alternative formulation where the current solution $\\vec{x_p}$ would be iteratively constructed in the Krylov subspace $\\mathcal{K}_p$ using sequential vectors $\\vec{v_k}, k = 0,1,\\dots,p-1$ from the basis $V_p$ would allow us to save the computation needed to solve the full projection problem at each iteration, and the main update would look like:\n",
    "  \n",
    "\\begin{equation} \\label{eq:CoordinateConstructionSolution}\n",
    "  \\vec{x_{p+1}} = \\vec{x_{p}} + \\alpha_p \\vec{v_{p}}\n",
    "\\end{equation}\n",
    "\n",
    "This is the basic idea of the conjugate gradient algorithm, we will explain here:\n",
    "\n",
    "\\paragraph{General Idea of Conjugate Gradient in the framework of Krylov methods} \\label{paragraph:IdeaOfConjGrad}\n",
    "We must recall that the solution vector $\\vec{s_p}$ at each step is the orthogonal projection of the general solution $\\hat{x}$ over the Arnoldi basis $V_p$.\n",
    "But it is orthogonal with respect to a specific inner product definition that uses $M \\in S^n_{++}$, although the Arnoldi basis $V_p$ column vectors are orthogonal with respect to the canonical inner product that uses the identity matrix, so there is no guarantee that the first coordinates of $\\vec{s_p}$ in $V_p$ will remain the same at the same position in $\\vec{s_{p+1}}$ expressed in the new basis $V_{p+1}$.\n",
    "This is why, at each iteration of the previous algorithm, the linear combination of vectors from $V_p$ had to be fully recomputed by solving a structured linear set of equations.\n",
    "\n",
    "To overcome this problem, let's first recall that the problem to solve at each iteration as seen previously, reads:\n",
    "      \n",
    "\\begin{align*}\n",
    "  H_{pp} \\vec{s_p} - V_p^\\intercal \\vec{y} = 0 \\\\\n",
    "  H_{pp} \\vec{s_p} = V_p^\\intercal \\vec{y} \\\\\n",
    "  H_{pp} \\vec{s_p} = \\vec{y_p}\n",
    "\\end{align*}\n",
    "\n",
    "\tAs $\\vec{y_p}$ and $\\vec{y_{p+1}}$ only differs in the last coordinate that have been added to $\\vec{y_{p+1}}$, as seen in eq. ~\\ref{eq:UpdateYpLanczos}, if we want to have a coordinate-wise resolution of the problem in the basis $V_p$, the same property should apply to $H_{pp} \\vec{s_p}$, which could be translated in:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{pmatrix}\n",
    "    1 \t   & 0 \t    & \\dots  & 0 \\\\\n",
    "    0 \t   & \\ddots & \\ddots & \\vdots \\\\\n",
    "    \\vdots & \\ddots & 1      & 0 \\\\\n",
    "    0 \t   & \\dots  & 0      & 0 \\\\\n",
    "  \\end{pmatrix}\n",
    "  H_{p+1p+1} \\vec{s_{p+1}} = H_{pp} \\vec{s_p} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Which imply that $H_{pp}$ should be constructed iteratively so that $H_{pp}$ and $H_{p+1p+1}$ only differs from the last line and the last column, but also that $H_{pp}$ should be diagonal for every $p$.\n",
    "\n",
    "To achieve this simple construction scheme, it is obvious that $H_{pp}$ should be diagonal. To do so, we have to find a new Krylov basis $W_p$ such that our new $H_{pp}$ is diagonal:\n",
    "\n",
    "\\begin{equation}\n",
    "  H_{pp} = W_{p} ^\\intercal M W_{p}\n",
    "\\end{equation}\n",
    "\n",
    "\tFortunately, we don't need to take the Krylov basis design problem from the beginning to handle this constraint. We have already seen in eq ~\\ref{eq:ProofResidueOrthogonalToKp} that the residue $\\vec{r_p}$ is orthogonal to $\\mathcal{K}_p$, and by construction, $\\vec{r_p} \\in \\mathcal{K}_{p+1}$ :\n",
    "\n",
    "\t\\begin{equation}\n",
    "\t  \\vec{r_p} = M \\vec{x_p} - \\vec{y}\n",
    "\t\\end{equation}\n",
    "\n",
    "\twith $\\vec{y} \\in \\mathcal{K}_1$, $\\vec{x_p} \\in \\mathcal{K}_p$ so $M \\vec{x_p} \\in \\mathcal{K}_{p+1}$. We conclude that $\\vec{r_p}$ lies in the orthogonal complement of $\\mathcal{K}_p$ in $\\mathcal{K}_{p+1}$ that we can write $\\mathcal{K}_{p+1} \\setminus \\mathcal{K}_p$ which is of dimension $1$ and has an orthonormal basis $\\vec{v_{p}}$. So every $\\vec{r_p}$ can be written $\\alpha \\vec{v_{p}}$.\n",
    "\n",
    "This last remark can be directly translated into a matrix equality, let $R_p$ be the matrix of all ordered $p$ first residual vectors $\\vec{r_k}, k \\in 0,1,\\dots, p-1$, we can write:\n",
    "    \n",
    "\\begin{equation}\n",
    "  R_p = V_p \\Delta_p\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\Delta_p$ is a diagonal matrix, and we can now extend properties valid for the decomposition of $H_{pp}$ :\n",
    "\n",
    "\\begin{align*}\n",
    "  &R_p^\\intercal M R_p \\\\\n",
    "  = &\\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p \\\\\n",
    "  = &\\Delta_p^\\intercal H_{pp} \\Delta_p \\\\\n",
    "  = &\\tilde{H}_{pp} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tWhere the matrix $\\tilde{H}_{pp}$ that is equivalent to the projection of the linear operator $M$ over $\\mathcal{K}_p$ expressed in the basis $R_p$ is still tridiagonal and positive definite, and admits an alternative Choleski decomposition of the form $ \\tilde{H}_{pp} = \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal $ such that we can write\n",
    "\n",
    "\\begin{align*}\n",
    "  &R_p^\\intercal M R_p &= \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal \\\\\n",
    "  \\Leftrightarrow & \\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p &= \\tilde{L}_p \\tilde{D}_p \\tilde{L}_p^\\intercal \\\\\n",
    "  \\Leftrightarrow & \\tilde{L}_p^{-1} \\Delta_p^\\intercal V_p^\\intercal M V_p \\Delta_p \\tilde{L}_p^{-\\intercal} &= \\tilde{D}_p \\\\\n",
    "  \\Leftrightarrow & W_{p} &= R_p \\tilde{L}_p^{-\\intercal} \n",
    "\\end{align*}\n",
    "\n",
    "Column vectors from $W_{p}$ are linear combinations of residual vectors, they form a basis of $\\mathcal{K}_p$ and they features the desired property stated in paragraph ~\\ref{paragraph:IdeaOfConjGrad} : they are all orthogonal to each other with respect to the inner product $\\langle .,. \\rangle_M$, $\\mathcal{M}^n$.\n",
    "\t\n",
    "Knowing that $\\tilde{L}_p$ is bidiagonal inferior by construction, as seen in eq ~\\ref{eq:TridiagonalCholeskiFactorizationMatrices}, we can also derive a short recurrence pattern over $\\vec{w_0}, \\vec{w_1}, \\dots, \\vec{w_{p-1}}$ the column vector of $W_p$, assuming that $\\gamma_0, \\gamma_1, \\dots, \\gamma_{p-1}$ are the subdiagonal elements of $\\tilde{L}_p$:\n",
    "\n",
    "\\begin{align*}\n",
    "  W_p \\tilde{L}_p^t &= R_p \\\\\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_0}\\\\&\\end{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_1}\\\\&\\end{pmatrix}\n",
    "    \\dots\n",
    "    \\begin{pmatrix}&\\\\&\\vec{w_{p-1}}\\\\&\\end{pmatrix}\n",
    "    \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    1 & \\gamma_0 & 0 & \\dots & 0 \\\\\n",
    "    0 & 1 & \\gamma_1 & \\ddots & \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "    \\vdots & \\ddots & \\ddots & 1 & \\gamma_{p-2} \\\\\n",
    "    0 & \\dots & \\dots & 0 & 1 \\\\\n",
    "  \\end{pmatrix} &=\n",
    "  \\begin{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_0}\\\\&\\end{pmatrix}\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_1}\\\\&\\end{pmatrix}\n",
    "    \\dots\n",
    "    \\begin{pmatrix}&\\\\&\\vec{r_{p-1}}\\\\&\\end{pmatrix}\n",
    "  \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This formulation may give us a way to construct $w_{j+1}$ with a simple iterative scheme, assuming that $\\vec{x_0} = \\vec{0}$:\n",
    "\n",
    "\\begin{align*}\n",
    "  &W_p \\tilde{L}_p^{\\intercal} &= R_p \\\\\n",
    "  \\Rightarrow & \\vec{r_0} &= \\vec{w_0} = \\vec{y} - M \\vec{x_0} \\; \\text{ and } \\\\\n",
    "  & \\vec{r_{j+1}} &= \\vec{w_{j+1}} + \\gamma_{j} \\vec{w_j} \\\\\n",
    "  \\Leftrightarrow & \\vec{w_{j+1}} &= \\vec{r_{j+1}} - \\gamma_{j} \\vec{w_j} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This relationship between $\\vec{w_{j+1}}$ and $\\vec{w_j}$ can be further exploited, if we use the fact that all vectors from $W_p$ are pairwise orthogonal with respect to the inner product defined by $M$:\n",
    "\n",
    "\\begin{align*}\n",
    "  & \\langle \\vec{w_{j+1}},\\vec{w_j} \\rangle_M &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j+1}} - \\gamma_{j} \\vec{w_j},\\vec{w_j} \\rangle_M &= 0 \\\\\n",
    "  \\Leftrightarrow & \\vec{r_{j+1}}^\\intercal M \\vec{w_j} - \\gamma_{j} \\vec{w_j}^\\intercal M \\vec{w_j} &= 0 \\\\\n",
    "  \\Leftrightarrow & \\gamma_{j} &= \\frac{\\vec{r_{j+1}}^\\intercal M \\vec{w_j}}{\\vec{w_j}^\\intercal M \\vec{w_j}}\n",
    "\\end{align*}\n",
    "\n",
    "\tWe now need to see if $\\vec{r_{j+1}}$ can be found using a simple recursion. To do so, we have to use the coordinate-wise construction of the solution exposed in eq ~\\ref{eq:CoordinateConstructionSolution} :\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec{x_{j+1}} &= \\vec{x_{j}} + \\alpha_j \\vec{w_{j}} \\\\\n",
    "  \\vec{y} - M \\vec{x_{j+1}} &= \\vec{y} - M \\vec{x_{j}} - \\alpha_j M \\vec{w_{j}} \\\\\n",
    "  \\vec{r_{j+1}} &= \\vec{r_{j}} - \\alpha_j M \\vec{w_{j}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\tHere, we can use the fact that $\\vec{r_{p}}$ is orthogonal to $\\mathcal{K}_p$, and especially $\\vec{r_{j}}$ is orthogonal to $\\vec{w_{j-1}}$ so that we can write:\n",
    "\n",
    "\\begin{align*}\n",
    "  & \\langle \\vec{r_{j+1}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j}} - \\alpha_{j} M \\vec{w_{j}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\langle \\vec{r_{j}},\\vec{w_{j}} \\rangle - \\alpha_{j} \\langle M \\vec{w_{j}},\\vec{w_{j}} \\rangle &= 0 \\\\\n",
    "  \\Leftrightarrow & \\vec{r_{j}}^\\intercal \\vec{w_{j}} - \\alpha_{j} \\vec{w_{j}}^\\intercal M \\vec{w_{j}} &= 0 \\\\\n",
    "  \\Leftrightarrow & \\alpha_{j} &= \\frac{ \\vec{r_{j}}^\\intercal \\vec{w_{j}} }{ \\vec{w_{j}}^\\intercal M \\vec{w_{j}} }\n",
    "\\end{align*}\n",
    "\n",
    "\t\n",
    "### Conjugate Gradient algorithm in practice\n",
    "\n",
    "The force of the Conjugate gradient algorithm lies in the fact that it iteratively builds a diagonal problem which can be solved one coordinate per iteration, without ``forgetting'' about the previous iterations. More interestingly, it is not solved in the canonical coordinate system of $\\mathbb{R}^n$ but instead in a basis of pairwise orthogonal vectors with respect to the matrix $M$, called ``conjugate vectors'', which are iteratively built using the gradient of the function $f(\\vec{x})$ we want to minimize at the current solution estimate : $f(\\vec{x}) = \\frac{1}{2} (\\vec{x}-\\vec{\\hat{x}} ) (M \\vec{x} - \\vec{y})  $.\n",
    "\n",
    "By design, these vectors are able to catch a large part of the error at every iteration, and especially, if the matrix $M$ defines an ellipsoid with an important anisotropy, the conjugate strategy helps to avoid the drawback of descent methods that are often ``stuck'' in valleys.\n",
    "\n",
    "The algorithm reads:\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{Solve $M \\vec{x} - \\vec{y} = 0$}\n",
    "    \\begin{algorithmic}\n",
    "      \\REQUIRE $M \\in S^n_{++}$ the positive definite cone\n",
    "      \\STATE \\textbf{Initialization}\n",
    "      \\STATE $\\vec{r_0} = \\vec{y}$\n",
    "      \\STATE $\\vec{w_0} = \\vec{r_0}$\n",
    "      \\STATE $j = 1$\n",
    "      \\STATE \\textbf{Iterations}\n",
    "      \\WHILE{$j \\leq n$}\n",
    "      \\STATE $\\alpha_{i-1} = \\frac{ \\vec{r_{i-1}}^\\intercal \\vec{w_{i-1}} }{ \\vec{w_{i-1}}^\\intercal M \\vec{w_{i-1}} }$\n",
    "      \\STATE $\\vec{x_i} = \\vec{x_{i-1}} + \\alpha_{i-1} \\vec{w_{j-1}}$\n",
    "      \\STATE $\\vec{r_i} = \\vec{r_{i-1}} - \\alpha_{i-1} M\\vec{w_{j-1}}$\n",
    "      \\IF{$\\vec{r_i} \\neq \\vec{0}$}\n",
    "        \\STATE $\\gamma_{i-1} = \\frac{ \\vec{r_{i}}^\\intercal M \\vec{w_{i-1}} }{ \\vec{w_{i-1}}^\\intercal M \\vec{w_{i-1}} }$\n",
    "        \\STATE $\\vec{w_i} = \\vec{r_{i}} - \\gamma_{i-1} \\vec{w_{j-1}}$\n",
    "      \\ELSE\n",
    "          \\STATE End iterations\n",
    "      \\ENDIF\n",
    "       \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "\\end{algorithm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
