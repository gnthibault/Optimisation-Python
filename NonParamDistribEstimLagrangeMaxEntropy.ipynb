{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange Multipliers for Maximum Entropy distribution estimation\n",
    "\n",
    "This notebook has been inspired by a really nice lecture on this topic: [MIT course](http://www-mtl.mit.edu/Courses/6.050/2003/notes/chapter10.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A problem of probability\n",
    "\n",
    "Many problems arising in the field of engineering, imaging, or data science imply the estimation of a mixture of, let's say $N$ components in the same bucket.<br>\n",
    "The mixture can be described with a point\n",
    "$ p \\begin{pmatrix}\n",
    "             p_0\\\\\n",
    "             p_1\\\\\n",
    "             \\vdots\\\\\n",
    "             p_{N-1}\n",
    "    \\end{pmatrix}\n",
    "\\in [0,1]^N$, such that $\\sum_{i=0}^{N-1} p_i = 1$\n",
    "This kind of mathematical object is also known as the probability simplex, see [this book](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf), and is also written as\n",
    "$$\n",
    "    \\{ p \\in \\mathbb{R}^N, p \\succeq 0, \\mathbb{1}^\\intercal p = 1 \\}\n",
    "$$\n",
    "\n",
    "In the more general framework of linear programming, this definition actually imply N+1 constraints:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &-p_i \\leq 0, \\text{ for } i = 0,1,\\dots, N-1\\\\\n",
    "    &\\mathbb{1}^\\intercal p = 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional constraints of the problem\n",
    "Although we know that all probability should sum to 1, we generally sick for a specific mixture that should meet some additional linear constraints.\n",
    "\n",
    "For instance, imagine that we have four advertisement-related url-links on a webpage, and everytime a person click on one of the links, it generates a given amount of money in an online wallet, let say\n",
    "- $a_0$ = 0.06\\$ for link 0\n",
    "- $a_1$ = 0.28\\$ for link 1\n",
    "- $a_2$ = 0.15\\$ for link 2\n",
    "- $a_3$ = 0.005\\$ for link 3\n",
    "\n",
    "Unfortunately, the advertisement company that rewards you for the click doesn't want to or cannot tell you which link generated the money you get, it only tells you the average click reward, ie:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    M &= \\sum_{i=0}^{N-1} a_i p_i \\\\\n",
    "    &= p^\\intercal a\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In many cases, we have $N$ unknowns $p_0,\\dots,p_{N-1}$, $K_1$ linear equality constraints and $K_2$ linear inequality defining a convex polytope, and often $K_1<N$.\n",
    "In this kind of settings, the problem has often infinitely many feasible solution, this is why we would like to set up an optimization problem, with a prior knowledge,for instance, related to a metric.\n",
    "\n",
    "Some people may think about the $l1$ or $l2$ regularization term that they may have seen on robust regression methods, respectively LASSO or ridge regression.\n",
    "This may apply, ie, we may have reason to believe that either a few links generates most of the rewards (sparsity), or we may on the contrary limit the total energy of the solution so that no link will end up with all the reward in the regression, which can occur if multiple links have a very similar reward.\n",
    "Those may be valid apriori depending on the topic, but there is another one which is much more elegant, and it is called the maximum entopy principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy principle\n",
    "\n",
    "We gave a friendly beginner introduction to information theory and statistics in the notebook called \"InformationTheoryOptimization\", so let's just recall the definition of the entropy of a discrete distribution $p$:\n",
    "$$\n",
    "    H(x) = - \\sum_{i=0}^{N-1} p_i log(p_i)\n",
    "$$\n",
    "\n",
    "If the $log$ is in base 2, the entropy of a distribution give the mean number of bits we should use to encode one outcome of the random process, with a perfect code.\n",
    "The higher is the entropy, the lesser we have clue on the outcome of the random process.\n",
    "\n",
    "We can use this concept in order to \"regularize\" somehow our problem, by restricting, among all feasible solution of the initial linear problem, the one that carry the minimum quantity of information in the information theory sense.\n",
    "This allows to reduce the bias we may get by imposing a less neutral apriori, in the sense of information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our problem\n",
    "\n",
    "Let's now write down our maximum entropy problem:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize } & -\\sum_{i=0}^{N-1} p_i log(p_i)\\\\\n",
    "    \\text{subject to } & -p_i \\leq 0, \\text{ for } i = 0,1,\\dots, N-1\\\\\n",
    "    & \\mathbb{1}^\\intercal p = 1 \\\\\n",
    "    & a^\\intercal p = M\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Fortunately, the entropy function is convex in $p$, and, linear constraints defines a convex set, our problem is then convex, see Boyd&Vandenberghe book, p362."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Lagrange multipliers for solving the convex problem\n",
    "\n",
    "Lagrange multipliers stands for a generic method for solving constrained optimization problems, where optimization objective, or equality/inequality constraints aren't necessarily linear.\n",
    "\n",
    "In short, it aims at relaxing constraints by integrating them to the objective while weighting them with a new set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
