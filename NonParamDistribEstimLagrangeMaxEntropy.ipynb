{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange Multipliers for Maximum Entropy distribution estimation\n",
    "\n",
    "This notebook has been inspired by a really nice lecture on this topic: [MIT course](http://www-mtl.mit.edu/Courses/6.050/2003/notes/chapter10.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A problem of probability\n",
    "\n",
    "Many problems arising in the field of engineering, imaging, or data science imply the estimation of a mixture of, let's say $N$ components in the same bucket.<br>\n",
    "The mixture can be described with a point\n",
    "$ p \\begin{pmatrix}\n",
    "             p_0\\\\\n",
    "             p_1\\\\\n",
    "             \\vdots\\\\\n",
    "             p_{N-1}\n",
    "    \\end{pmatrix}\n",
    "\\in [0,1]^N$, such that $\\sum_{i=0}^{N-1} p_i = 1$\n",
    "This kind of mathematical object is also known as the probability simplex, see [this book](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf), and is also written as\n",
    "$$\n",
    "    \\{ p \\in \\mathbb{R}^N, p \\succeq 0, \\mathbb{1}^\\intercal p = 1 \\}\n",
    "$$\n",
    "\n",
    "In the more general framework of linear programming, this definition actually imply N+1 constraints:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &-p_i \\leq 0, \\text{ for } i = 0,1,\\dots, N-1\\\\\n",
    "    &\\mathbb{1}^\\intercal p = 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional constraints of the problem\n",
    "Although we know that all probability should sum to 1, we generally sick for a specific mixture that should meet some additional linear constraints.\n",
    "\n",
    "For instance, imagine that we have four advertisement-related url-links on a webpage, and everytime a person click on one of the links, it generates a given amount of money in an online wallet, let say\n",
    "- $a_0$ = 0.06\\$ for link 0\n",
    "- $a_1$ = 0.28\\$ for link 1\n",
    "- $a_2$ = 0.15\\$ for link 2\n",
    "- $a_3$ = 0.005\\$ for link 3\n",
    "\n",
    "Unfortunately, the advertisement company that rewards you for the click doesn't want to or cannot tell you which link generated the money you get, it only tells you the average click reward, ie:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    M &= \\sum_{i=0}^{N-1} a_i p_i \\\\\n",
    "    &= p^\\intercal a\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In many cases, we have $N$ unknowns $p_0,\\dots,p_{N-1}$, $K_1$ linear equality constraints and $K_2$ linear inequality defining a convex polytope, and often $K_1<N$.\n",
    "In this kind of settings, the problem has often infinitely many feasible solution, this is why we would like to set up an optimization problem, with a prior knowledge,for instance, related to a metric.\n",
    "\n",
    "Some people may think about the $l1$ or $l2$ regularization term that they may have seen on robust regression methods, respectively LASSO or ridge regression.\n",
    "This may apply, ie, we may have reason to believe that either a few links generates most of the rewards (sparsity), or we may on the contrary limit the total energy of the solution so that no link will end up with all the reward in the regression, which can occur if multiple links have a very similar reward.\n",
    "Those may be valid apriori depending on the topic, but there is another one which is much more elegant, and it is called the maximum entopy principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy principle\n",
    "\n",
    "We gave a friendly beginner introduction to information theory and statistics in the notebook called \"InformationTheoryOptimization\", so let's just recall the definition of the entropy of a discrete distribution $x$:\n",
    "$$\n",
    "    H(x) = - \\sum_{i=0}^{N-1} x_i log(x_i)\n",
    "$$\n",
    "\n",
    "If the $log$ is in base 2, the entropy of a distribution give the mean number of bits we should use to encode one outcome of the random process, with a perfect code.\n",
    "The higher is the entropy, the lesser we have clue on the outcome of the random process.\n",
    "\n",
    "We can use this concept in order to \"regularize\" somehow our problem, by restricting, among all feasible solution of the initial linear problem, the one that carry the minimum quantity of information in the information theory sense.\n",
    "This allows to reduce the bias we may get by imposing a less neutral apriori, in the sense of information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our problem\n",
    "\n",
    "Let's now write down our maximum entropy problem:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{minimize } & f_0(x) = \\sum_{i=0}^{N-1} x_i log(x_i)\\\\\n",
    "    \\text{subject to } & A x \\preceq b \\\\\n",
    "    & \\mathbb{1}^\\intercal x = 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\text{dom } f_0 = \\mathbb{R}^{n}_{++}$ <br>\n",
    "Fortunately, the entropy function is convex in $x$, and, linear constraints defines a convex set, our problem is then convex, see Boyd&Vandenberghe book, p362."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex conjugate of entropy\n",
    "\n",
    "As seen in the notebook \"ForwardBackwardDual\" we have seen the definition of the convex conjugate: given a convex, proper, and lower semi-continuous function $f$ defined on $\\mathbb{R}^n_{+}$, its\n",
    "conjugate $f^*$ is the convex function defined as\n",
    "$$ \\forall y \\in \\mathbb{R}^n, \\quad f^*(y) = \\underset{x \\in \\mathbb{R}^n_{+}}{sup} \\  x^{\\intercal}y - f(x) $$\n",
    "\n",
    "Which give the following derivation in a simple one dimensional case where $f(x) = x log(x)$\n",
    "\\begin{align*}\n",
    "    f^*(y) &= \\underset{x \\in \\mathbb{R}_{+}}{sup} \\  xy - f(x)\\\\\n",
    "           &= \\underset{x \\in \\mathbb{R}_{+}}{sup} \\  g(x,y)\\\\\n",
    "           & \\text{Where }\\\\\n",
    "    g(x,y) &= xy - x log(x)\\\\\n",
    "\\end{align*}\n",
    "Assuming $f(0)=0$ it is easy to see that $g(x,y)$ has an upper bound on $x \\in \\mathbb{R}_{+}$ for all given $y \\in \\mathbb{R}$, let's take a look at its derivative:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial g(x,y)}{\\partial x} &= y - (log(x) + x\\frac{1}{x})\\\\\n",
    "        &= y - log(x) - 1\n",
    "\\end{align*}\n",
    "We see that this derivative is strictly monotonic over $\\mathbb{R}_{+}$ because of the monotonicity of $log(x)$ and that it shoud be 0-valued only once over $\\mathbb{R}_{+}$ also because of the behaviour of $log(x)$. Equating this derivative to zero in order to find the supremum gives us:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial g(x,y)}{\\partial x} &= 0\\\\\n",
    "        y - log(x) - 1 &= 0\\\\\n",
    "        log(x) &= y - 1\\\\\n",
    "        x &= e^{y - 1}\\\\\n",
    "\\end{align*}\n",
    "So we can finally write\n",
    "\\begin{align*}\n",
    "    f^*(y) &= \\underset{x \\in \\mathbb{R}_{+}}{sup} \\  xy - f(x)\\\\\n",
    "           &= e^{y - 1}y - e^{y - 1} log(e^{y - 1})\\\\\n",
    "           &= ye^{y - 1} - ye^{y - 1} + e^{y - 1}\\\\\n",
    "           &= e^{y - 1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex conjugate and Lagrange duality\n",
    "\n",
    "#### General case: the link between Lagrange dual problem, and the conjugate\n",
    "It is interesting to make the link between the convex conjugate function and the Lagrange dual function. Let's consider a general optimization problem of the form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{minimize } & f_0(x)\\\\\n",
    "    \\text{subject to } & A x \\preceq b \\\\\n",
    "    & C x = d \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Using the conjugate of $f_0$, we can write the dual fonction for the previous problem as:\n",
    "\n",
    "\\begin{align*}\n",
    "    g(\\lambda, \\nu) &= \\underset{x}{inf}\\left( f_0(x) + \\lambda^{\\intercal} (Ax-b) + \\nu^{\\intercal}(Cx-d) \\right)\\\\\n",
    "    &= -b^{\\intercal}\\lambda - d^{\\intercal}\\nu + \\underset{x}{inf}\\left( \\underbrace{ f_0(x) + (A^{\\intercal}\\lambda+C^{\\intercal}\\nu)^{\\intercal}x}_{\\textrm{recognize } -( y^{\\intercal}x - f(x))} \\right) \\\\\n",
    "    &= -b^{\\intercal}\\lambda - d^{\\intercal}\\nu - f_0^*(-A^{\\intercal}\\lambda-C^{\\intercal}\\nu))\n",
    "\\end{align*}\n",
    "\n",
    "Where the domain of $g$ follows from the domain of $f_0^*$:\n",
    "$$\n",
    "    \\textrm{dom} \\; g = \\{(\\lambda,\\nu) | -A^{\\intercal}\\lambda-C^{\\intercal}\\nu \\in \\textrm{dom}\\; f_0^* \\}\n",
    "$$\n",
    "\n",
    "#### The specific case of entropy maximization (of negative entropy minimization)\n",
    "\n",
    "In the specific case of negative entropy minimization, we already have derived the conjugate of the one dimensional entropy : $f(x) = xlog(x) \\iff f^*(y) = e^{y-1}$ <br>\n",
    "\n",
    "The derivation of the multidimensional (vectorial) case, which is only a sum of scalar functions is straightforward:\n",
    "$$\n",
    "    f(x) = \\sum_{i=0}^{n-1} x_i log(x_i) \\iff f^*(y) = \\sum_{i=0}^{n-1} e^{y_i-1}\n",
    "$$\n",
    "With $\\text{dom } f_0^* = \\mathbb{R}^{n}$ <br>\n",
    "\n",
    "We can now derive the dual problem of our entropy maximization from the general expression we found earlier, that depends on the conjugate, where $C = \\mathbb{1}^{\\intercal}$ and $ d = 1$ so that $\\nu$ becomes a scalar:\n",
    "\\begin{align*}\n",
    "    g(\\lambda, \\nu) &= -b^{\\intercal}\\lambda - \\nu - f_0^*(-A^{\\intercal}\\lambda-\\mathbb{1}\\nu) \\\\\n",
    "    &= -b^{\\intercal}\\lambda - \\nu - \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda-\\nu-1} \\\\\n",
    "    &= -b^{\\intercal}\\lambda - \\nu - e^{-\\nu-1} \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \n",
    "\\end{align*}\n",
    "\n",
    "Where $a_i$ is the $i^th$ column of $A$. <br>\n",
    "Here we must specify that $\\text{dom} \\; g = \\{(\\lambda,\\nu) | -A^{\\intercal}\\lambda - \\mathbb{1}\\nu \\in \\mathbb{R}^n \\}$ but this amounts to $\\text{dom} \\; g = \\mathbb{R}^n \\times \\mathbb{R}$ which is not particularly restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the dual problem\n",
    "\n",
    "Thanks to the previous part, we established the dual optimization problem of the entropy maximization which is:\n",
    "\\begin{align*}\n",
    "    &\\text{maximize} \\quad & g(\\lambda, \\nu) \\\\\n",
    "    &\\text{subject to} & \\lambda \\succeq 0\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "$$\n",
    "    g(\\lambda, \\nu) = -b^{\\intercal}\\lambda - \\nu - e^{-\\nu-1} \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \n",
    "$$\n",
    "\n",
    "We can use the fact that $\\nu$ is a scalar to see if we can obtain the optimal coordinate of the solution in this direction, let's take a look at the partial derivative:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial g(\\lambda, \\nu)}{\\partial \\nu} &= \\frac{\\partial \\left( -b^{\\intercal}\\lambda - \\nu - e^{-\\nu-1} \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda}\\right) }{\\partial \\nu} \\\\\n",
    "    &= -0-1 - \\frac{\\partial \\left(e^{-\\nu-1} \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda}\\right) }{\\partial \\nu} \\\\\n",
    "    &= e^{-\\nu-1} \\left( \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \\right) -1\n",
    "\\end{align*}\n",
    "\n",
    "Equating this partial derivative to zero give us:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial g(\\lambda, \\nu)}{\\partial \\nu} &= 0 \\\\\n",
    "    e^{-\\nu-1} \\left( \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \\right)-1 &= 0\\\\\n",
    "    e^{-\\nu-1} &= \\frac{1}{\\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda}} \\\\\n",
    "    -\\nu-1 &= log \\left( \\frac{1}{\\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda}} \\right)\\\\\n",
    "    \\nu &= -log \\left( \\frac{1}{\\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda}} \\right) -1 \\\\\n",
    "    \\nu &= log \\left( \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \\right) -1\n",
    "\\end{align*}\n",
    "\n",
    "This optimal value of $\\nu$ can be injected into the dual problem, that immediadtly simplifies and is now only functions of $\\lambda$:\n",
    "\\begin{align*}\n",
    "    &\\text{maximize} \\quad & -b^{\\intercal}\\lambda - log \\left( \\sum_{i=0}^{n-1} e^{-a_i^{\\intercal}\\lambda} \\right) \\\\\n",
    "    &\\text{subject to} & \\lambda \\succeq 0\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the primal, using the dual problem solution\n",
    "\n",
    "If strong duality holds for this problem, primal solution $p^*$ and dual solution $d*$ are equal. As the primal problem is convex, Slater's conditions told us that strong duality holds if the problem is strictly feasible, ie:\n",
    "$$\n",
    "    \\exists x \\in \\mathbb{R}_{+}^n \\; \\text{ such that } \\; \\mathbb{1}^{\\intercal}x = 1, Ax \\prec b\n",
    "$$\n",
    "\n",
    "For now, let's assume that there is a feasible point, so that strong duality holds, and that we know the dual solution $(\\lambda^*,\\nu^*)$, we can write the lagrangian at this point:\n",
    "\n",
    "$$\n",
    "    L(x,\\lambda^*,\\nu^*) = \\sum_{i=0}^{n-1} x_i log(x_i) + {\\lambda^*}^{\\intercal} (Ax-b) + \\nu^* (\\mathbb{1}^{\\intercal } x-1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
