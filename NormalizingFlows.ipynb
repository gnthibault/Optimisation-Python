{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows for Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is somehow a follow up the InformationTheoryOptimization notebook. Many more basic concepts exposed there will be used here. But we will give a short reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes theorem\n",
    "\n",
    "Although Bayesian statistics is not really a novel approach, the recent rise of probabilistic programming libraries, availability of a lot of data, and powerful computer to run markov chains, made the tedious task of running bayesian inference algorithms a lot easier.\n",
    "\n",
    "Let's recall some simple elements about the Bayes theorem that we have been in other notebooks:\n",
    "\n",
    "For two given random variables, $X$ and $Y$, we can write :\n",
    "\\begin{equation}\n",
    "    P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We could use a slightly more formal version, that includes a model $\\mathcal{M}$ that we believe is the underlying model for the random variable $x$, parametrized by $\\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\theta|x,\\mathcal{M}) = \\frac{P(x|\\theta, \\mathcal{M})P(\\theta|\\mathcal{M})}{P(x|\\mathcal{M})}\n",
    "\\end{equation}\n",
    "\n",
    "* Usually, $\\theta$ is the random variable that we would like to caracterize. It usually consist in a set of parameters for a given statistical model (mean and variance for a normal distribution for instance).\n",
    "\n",
    "* On the contrary $x$, called the evidence, is usually derived from a known dataset, or from sampling a real life process. We will see shortly how we can define an empirical distribution from a set of samples. \n",
    "\n",
    "* We call $P(x|\\theta)$ the likelihood of $x$, given the model distribution  $\\theta$. This is the likelihood (or its logarithm) we tried to maximize with various instances of expectation maximization algorithm in the InformationTheory notebook by finding the optimal model parameters $\\theta$.\n",
    "Maximum likelihood had some success in the past for some instance of problems where likelihood or its logarithm were concave and differentiable, and a gradient based methods allowed to find a global maximum.\n",
    "\n",
    "* We call $P(\\theta|x)$ the posterior distribution of $\\theta$, given a known (usually empirical distribution of data) $x$. As opposite to the prior, it gives an idea of the probability of the model AFTER some data ($x$) has been seen.\n",
    "\n",
    "* We call $P(\\theta)$ the a-priori distribution for the variable $\\theta$. This one can be derived if we have a-priori knowledge on the model parameters $\\theta$, it may consist in apriori knowledge of some surrogate parameters that we integrate as a marginal distribution and scale. A prior usually allows us to compute the probability of a given set of parameters $\\theta \\in \\mathbb{R}^n$ that feed the model $\\mathcal{M}$.\n",
    "\n",
    "A priori usually writes:\n",
    "  \\begin{align*}\n",
    "    y &\\rightarrow P(\\theta=y) \\\\\n",
    "    \\mathbb{R}^n &\\mapsto [0,1]\n",
    "  \\end{align*}\n",
    "  \n",
    "A concrete example is for instance, the use of the framework of random markov field (or Gibbs random field) in a n-dimensional space like an image, were we can consider the pixels as a set of vertices of a graph, and only neighbouring pixels are connected by edges. The probability of a given graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is given by $P(\\mathcal{G}) = \\alpha e^{-\\beta U(\\mathcal{G})}$ where $\\alpha$ and $\\beta$ are normalization factor, and $U(\\mathcal{G}) = \\sum_{c\\in C} V_c(\\mathcal(G))$ is a sum of clique potentials $V_c(\\mathcal(G))$ over all possible cliques $C$. A very common instance of Gibbs measure is the gaussian-like distribution where, we have, for each neighbouring pair of pixel $p_1-p_2$ (a clique): $V_{p_1-p_2}(\\mathcal{G}) = (\\mathcal{G}_{p_1}-\\mathcal{G}_{p_2})^2$\n",
    "When one has no apriori on $\\theta$, then the highest entropy hypothesis is implicitly used. More pragmatically, we write $P(\\theta=y) \\sim Uniform(\\mathrm{supp} \\theta)$ where $\\mathrm{supp} y)$ is the support of $\\theta$\n",
    "Notice that, given a discrete process, as long as we received new data, the prior distribution can be taken just as being the previous step posterior.\n",
    "\n",
    "* Usually $P(x)$ is the empirical distribution that correspond to a given set of outcomes (actual data). It represents the probability of the given outcome of the experiment regardless of the value of the underlying model $\\theta$. We can see $P(x)$ as a marginalization of $\\theta$ in the joint distribution $P(x,\\theta)=P(x|\\theta)P(\\theta)$ i.e. $P(x) = \\int_{\\theta} P(x,\\theta)$ such that it acts as a constant used to normalize likelihood  / prior product to make sure the result posterior is a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference and the sampling task\n",
    "\n",
    "As briefly seen in the InformationTheory notebook, in the general case, to compute the posterior over the whole support of $\\theta$ is extremely complex problem, especially if $\\theta$ is a vector from a high dimension space, and the actual distribution of the posterior is very complex (not smooth, many local dense areas, etc...). MCMC types of methods often comes into play here, where one computes $P(\\theta=y_0|x)$ for a given $y_0$, then jumps to another point in space $y_1$, with a given probably, dependant on $P(\\theta=y|x)$.\n",
    "Little by little one can get a discrete approximation of the posterior, but it can be very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler divergence, also called the relative entropy or discrimination information, can be computed between two distributions, say $x$ and $y$ and writes $ D_{KL}(x\\|y) $.\n",
    "This metric represents the expectation of the difference of the number of bits needed to encode a symbol from the distribution x, weither the encoding is optimal for the distribution x or y.\n",
    "\n",
    "The general case reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\mathbb{E}_x \\left[ log \\left( \\frac{x}{y} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "In the discrete case it reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\sum_{i\\in I} p(x_i) log(p(x_i)) -\\sum_{i\\in I} p(x_i) log(p(y_i)) \\\\\n",
    "                 &= -H(x) + H_y(x)\n",
    "\\end{align}\n",
    "or equivalently\n",
    "$$\n",
    "    D_{KL}(x\\|y) = \\sum_{i\\in I} p(x_i) log\\left(\\frac{p(x_i)}{p(y_i)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### KL Divergence and maximum likelihood\n",
    "This chapter was inspired by [this article](http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/)\n",
    "by Liangjie Hong\n",
    "\n",
    "#### Definitions\n",
    "Let's consider $P(x_i|\\theta)$ a distribution used to generate (or sample) $N$ points $x_i, i=0,\\dots,N-1$\n",
    "\n",
    "We can define a general distribution model as:\n",
    "$$\n",
    "    P_\\theta(x) = P(x|\\theta)\n",
    "$$\n",
    "And, to handle data from real world, we will define the following empirical distribution (also called probability mass function):\n",
    "$$\n",
    "    P_D(x) = \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i)\n",
    "$$\n",
    "\n",
    "#### KL Divergence between model and data\n",
    "We can now express the Kullback Leibler divergence between the model distribution and the empirical one:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) &= \\int P_D(x) log(P_D(x)) dx -\\int P_D(x) log(P_\\theta(x)) dx \\\\\n",
    "    &= H(P_D(x)) - \\int P_D(x) log(P_\\theta(x)) dx\n",
    "\\end{align}\n",
    "\n",
    "As $H(P_D(x))$ is a constant term, relatd to the empirical distribution of real data, we will call it $\\epsilon$, and we may get rid of it later.\n",
    "Instead we will mainly consider, the $\\theta$, ie, the model related term:\n",
    "$$\n",
    "   D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) = \\epsilon -\\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2}\n",
    "$$\n",
    "\n",
    "Thanks to the definition of the empirical distribution, we can express this continuous expression as a discrete summation:\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2} &= \\int \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i) log(P(x|\\theta)) dx \\\\\n",
    "    &= \\sum_{i=0}^{N-1} \\frac{1}{N} log(P(x_i|\\theta)) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{i=0}^{N-1} log(P(x_i|\\theta))\n",
    "\\end{align}\n",
    "\n",
    "The last line amounts to the log-likelihood of the dataset. We can then conclude that maximizing the log-likelihood of a dataset, relatively to a set of distribution parameters $\\theta$ and a given dataset $x$ is equivalent to minimizing the Kullback Leibler divergence between the empirical distribution and the model distribution.\n",
    "\n",
    "### Practical KL Divergence between two data sets\n",
    "We are interested in computing the Kullback Leibler divergence between a model $X$ and a real dataset $Y$, indexed by $i$ where $i$ is the index of a specific event whose occurence probability is $X_i$ according to the model, and $Y_i$ in practice.\n",
    "\n",
    "As in the definition of the empirical distribution, our datasets must verify:\n",
    "$$\n",
    "    \\sum_{i=0}^{N-1} X_i = 1\n",
    "$$\n",
    "\n",
    "then the expression of the kl divergence is simply:\n",
    "$$\n",
    "    D_{KL}(Y\\|X) = \\sum_{i=0}^{N-1} log \\left( \\frac{Y_i}{X_i} \\right) Y_i\n",
    "$$\n",
    "\n",
    "Moreover there is an important condition that must be respected: \n",
    "$$\n",
    "    X_i = 0 \\implies Y_i = 0\n",
    "$$\n",
    "If so, if the event $k$ has no occurence, we have $X_k=Y_k=0$ and $log \\left( \\frac{y_k}{x_k} \\right) y_k$ has a limit in $0$ which is $0$. Otherwise, Kullback Leibler cannot be computed.\n",
    "\n",
    "The explanation is pretty straightforward: if an event, or a symbol from $Y$ has no occurence in $X$, it has no reason to be encoded using $X$ probability distribution, then the average number of bits needed to encode a symbol from $Y$ is a nonsense, as some symbols cannot be represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a variational inference ?\n",
    "\n",
    "We are still in the framework of bayes theorem, and we want to compute $p$ our posterior that stands for $p(\\theta|x)$.\n",
    "\n",
    "In variational inference,  you create a variational distribution $q$ over your model parameters $\\theta$, that we will call latent variables here. A variational distribution is parametrized by a variational parameter, we will call $\\nu$:\n",
    "\n",
    "$$\n",
    " q(\\theta|\\nu)\n",
    "$$\n",
    "\n",
    "The first idea of variational inference, is that we will try to find $\\nu$ such that $q(\\theta|\\nu)$ becomes close to our posterior distribution.\n",
    "The closeness of those distribution will be computed using Kullbac-Liebler divergence:\n",
    "$$\n",
    "\\begin{align}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(\\theta)}{p(\\theta|x)} \\right) \\right]\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows: main references\n",
    "Here are a few very important references we might refere to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the amazing work of Mahdi Karami. The document is a PhD thesis dedicated to the study of tools for variational inference, normalizing flows, analysis and design of operator with tractable inverse jacobian computation, with additional numerical and convex optimization tools. It also contains applications to latent space representations of complex distributions met in signal/image processing, and much more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets move on to a very interesting review paper: Normalizing Flows: An Introduction and Review of Current Methods by Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1908.09257.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce4a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1908.09257.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reference paper, that exposes a lot of important properties of Normalizing flows, and design of methods to leverage them in machine learning / representation learning: Normalizing Flows for Probabilistic Modeling and Inference by George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1912.02762.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1912.02762.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a paper more targeted to application: Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows by Marco Rudolph, Bastian Wandt and Bodo Rosenhahn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/2008.12577.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/2008.12577.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
