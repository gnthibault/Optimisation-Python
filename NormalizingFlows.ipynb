{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "# Deep learning stuff\n",
    "\n",
    "# Probabilistic stuff\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows for Generative model in Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is somehow a follow up the InformationTheoryOptimization notebook. Many more basic concepts exposed there will be used here. But we will give a short reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes theorem\n",
    "\n",
    "Although Bayesian statistics is not really a novel approach, the recent rise of probabilistic programming libraries, availability of a lot of data, and powerful computer to run markov chains, made the tedious task of running bayesian inference algorithms a lot easier.\n",
    "\n",
    "Let's recall some simple elements about the Bayes theorem that we have been in other notebooks:\n",
    "\n",
    "For two given random variables, $X$ and $Y$, we can write :\n",
    "\\begin{equation}\n",
    "    P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We could use a slightly more formal version, that includes a model $\\mathcal{M}$ that we believe is the underlying model for the random variable $x$, parametrized by $\\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\theta|x,\\mathcal{M}) = \\frac{P(x|\\theta, \\mathcal{M})P(\\theta|\\mathcal{M})}{P(x|\\mathcal{M})}\n",
    "\\end{equation}\n",
    "\n",
    "* Usually, $\\theta$ is the random variable that we would like to caracterize. It usually consist in a set of parameters for a given statistical model (mean and variance for a normal distribution for instance).\n",
    "\n",
    "* On the contrary $x$, called the evidence, is usually derived from a known dataset, or from sampling a real life process. We will see shortly how we can define an empirical distribution from a set of samples. \n",
    "\n",
    "* We call $P(x|\\theta)$ the likelihood of $x$, given the model distribution  $\\theta$. This is the likelihood (or its logarithm) we tried to maximize with various instances of expectation maximization algorithm in the InformationTheory notebook by finding the optimal model parameters $\\theta$.\n",
    "Maximum likelihood had some success in the past for some instance of problems where likelihood or its logarithm were concave and differentiable, and a gradient based methods allowed to find a global maximum.\n",
    "\n",
    "* We call $P(\\theta|x)$ the posterior distribution of $\\theta$, given a known (usually empirical distribution of data) $x$. As opposite to the prior, it gives an idea of the probability of the model AFTER some data ($x$) has been seen.\n",
    "\n",
    "* We call $P(\\theta)$ the a-priori distribution for the variable $\\theta$. This one can be derived if we have a-priori knowledge on the model parameters $\\theta$, it may consist in apriori knowledge of some surrogate parameters that we integrate as a marginal distribution and scale. A prior usually allows us to compute the probability of a given set of parameters $\\theta \\in \\mathbb{R}^n$ that feed the model $\\mathcal{M}$.\n",
    "\n",
    "A priori usually writes:\n",
    "  \\begin{align*}\n",
    "    y &\\rightarrow P(\\theta=y) \\\\\n",
    "    \\mathbb{R}^n &\\mapsto [0,1]\n",
    "  \\end{align*}\n",
    "  \n",
    "A concrete example is for instance, the use of the framework of random markov field (or Gibbs random field) in a n-dimensional space like an image, were we can consider the pixels as a set of vertices of a graph, and only neighbouring pixels are connected by edges. The probability of a given graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is given by $P(\\mathcal{G}) = \\alpha e^{-\\beta U(\\mathcal{G})}$ where $\\alpha$ and $\\beta$ are normalization factor, and $U(\\mathcal{G}) = \\sum_{c\\in C} V_c(\\mathcal(G))$ is a sum of clique potentials $V_c(\\mathcal(G))$ over all possible cliques $C$. A very common instance of Gibbs measure is the gaussian-like distribution where, we have, for each neighbouring pair of pixel $p_1-p_2$ (a clique): $V_{p_1-p_2}(\\mathcal{G}) = (\\mathcal{G}_{p_1}-\\mathcal{G}_{p_2})^2$\n",
    "When one has no apriori on $\\theta$, then the highest entropy hypothesis is implicitly used. More pragmatically, we write $P(\\theta=y) \\sim Uniform(\\mathrm{supp} \\theta)$ where $\\mathrm{supp} y)$ is the support of $\\theta$\n",
    "Notice that, given a discrete process, as long as we received new data, the prior distribution can be taken just as being the previous step posterior.\n",
    "\n",
    "* Usually $P(x)$ is the empirical distribution that correspond to a given set of outcomes (actual data). It represents the probability of the given outcome of the experiment regardless of the value of the underlying model $\\theta$. We can see $P(x)$ as a marginalization of $\\theta$ in the joint distribution $P(x,\\theta)=P(x|\\theta)P(\\theta)$ i.e. $P(x) = \\int_{\\theta} P(x,\\theta)$ such that it acts as a constant used to normalize likelihood  / prior product to make sure the result posterior is a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference and the sampling task\n",
    "\n",
    "As briefly seen in the InformationTheory notebook, in the general case, to compute the posterior over the whole support of $\\theta$ is extremely complex problem, especially if $\\theta$ is a vector from a high dimension space, and the actual distribution of the posterior is very complex (not smooth, many local dense areas, etc...). MCMC types of methods often comes into play here, where one computes $P(\\theta=y_0|x)$ for a given $y_0$, then jumps to another point in space $y_1$, with a given probably, dependant on $P(\\theta=y|x)$.\n",
    "Little by little one can get a discrete approximation of the posterior, but it can be very slow.\n",
    "\n",
    "For instance, as part of tensorflow probability you get the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs one step of the Metropolis-Hastings algorithm.\n",
      "\n",
      "  The [Metropolis-Hastings algorithm](\n",
      "  https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) is a\n",
      "  Markov chain Monte Carlo (MCMC) technique which uses a proposal distribution\n",
      "  to eventually sample from a target distribution.\n",
      "\n",
      "  Note: `inner_kernel.one_step` must return `kernel_results` as a\n",
      "  `collections.namedtuple` which must:\n",
      "\n",
      "  - have a `target_log_prob` field,\n",
      "  - optionally have a `log_acceptance_correction` field, and,\n",
      "  - have only fields which are `Tensor`-valued.\n",
      "\n",
      "  The Metropolis-Hastings log acceptance-probability is computed as:\n",
      "\n",
      "  ```python\n",
      "  log_accept_ratio = (current_kernel_results.target_log_prob\n",
      "                      - previous_kernel_results.target_log_prob\n",
      "                      + current_kernel_results.log_acceptance_correction)\n",
      "  ```\n",
      "\n",
      "  If `current_kernel_results.log_acceptance_correction` does not exist, it is\n",
      "  presumed `0.` (i.e., that the proposal distribution is symmetric).\n",
      "\n",
      "  The most common use-case for `log_acceptance_correction` is in the\n",
      "  Metropolis-Hastings algorithm, i.e.,\n",
      "\n",
      "  ```none\n",
      "  accept_prob(x' | x) = p(x') / p(x) (g(x|x') / g(x'|x))\n",
      "\n",
      "  where,\n",
      "    p  represents the target distribution,\n",
      "    g  represents the proposal (conditional) distribution,\n",
      "    x' is the proposed state, and,\n",
      "    x  is current state\n",
      "  ```\n",
      "\n",
      "  The log of the parenthetical term is the `log_acceptance_correction`.\n",
      "\n",
      "  The `log_acceptance_correction` may not necessarily correspond to the ratio of\n",
      "  proposal distributions, e.g, `log_acceptance_correction` has a different\n",
      "  interpretation in Hamiltonian Monte Carlo.\n",
      "\n",
      "  #### Examples\n",
      "\n",
      "  ```python\n",
      "  import tensorflow_probability as tfp\n",
      "  hmc = tfp.mcmc.MetropolisHastings(\n",
      "      tfp.mcmc.UncalibratedHamiltonianMonteCarlo(\n",
      "          target_log_prob_fn=lambda x: -x - x**2,\n",
      "          step_size=0.1,\n",
      "          num_leapfrog_steps=3))\n",
      "  # ==> functionally equivalent to:\n",
      "  # hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
      "  #     target_log_prob_fn=lambda x: -x - x**2,\n",
      "  #     step_size=0.1,\n",
      "  #     num_leapfrog_steps=3)\n",
      "  ```\n",
      "\n",
      "  \n",
      "=================================\n",
      "Runs one step of the RWM algorithm with symmetric proposal.\n",
      "\n",
      "  Random Walk Metropolis is a gradient-free Markov chain Monte Carlo\n",
      "  (MCMC) algorithm. The algorithm involves a proposal generating step\n",
      "  `proposal_state = current_state + perturb` by a random\n",
      "  perturbation, followed by Metropolis-Hastings accept/reject step. For more\n",
      "  details see [Section 2.1 of Roberts and Rosenthal (2004)](\n",
      "  http://emis.ams.org/journals/PS/images/getdoc510c.pdf?id=35&article=15&mode=pdf).\n",
      "\n",
      "  Current class implements RWM for normal and uniform proposals. Alternatively,\n",
      "  the user can supply any custom proposal generating function.\n",
      "\n",
      "  The function `one_step` can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics\n",
      "  are governed by `target_log_prob_fn(*current_state)`. (The number of\n",
      "  independent chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  samples, _ = tfp.mcmc.sample_chain(\n",
      "    num_results=1000,\n",
      "    current_state=dtype(1),\n",
      "    kernel=tfp.mcmc.RandomWalkMetropolis(\n",
      "       target.log_prob,\n",
      "       seed=42),\n",
      "    num_burnin_steps=500,\n",
      "    parallel_iterations=1)  # For determinism.\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.math.reduce_mean(\n",
      "          tf.math.squared_difference(samples, sample_mean),\n",
      "          axis=0))\n",
      "\n",
      "  print('Estimated mean: {}'.format(sample_mean))\n",
      "  print('Estimated standard deviation: {}'.format(sample_std))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from a 2-D Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "  true_mean = dtype([0, 0])\n",
      "  true_cov = dtype([[1, 0.5],\n",
      "                    [0.5, 1]])\n",
      "  num_results = 500\n",
      "  num_chains = 100\n",
      "\n",
      "  # Target distribution is defined through the Cholesky decomposition `L`:\n",
      "  L = tf.linalg.cholesky(true_cov)\n",
      "  target = tfd.MultivariateNormalTriL(loc=true_mean, scale_tril=L)\n",
      "\n",
      "  # Initial state of the chain\n",
      "  init_state = np.ones([num_chains, 2], dtype=dtype)\n",
      "\n",
      "  # Run Random Walk Metropolis with normal proposal for `num_results`\n",
      "  # iterations for `num_chains` independent chains:\n",
      "  samples, _ = tfp.mcmc.sample_chain(\n",
      "      num_results=num_results,\n",
      "      current_state=init_state,\n",
      "      kernel=tfp.mcmc.RandomWalkMetropolis(\n",
      "          target_log_prob_fn=target.log_prob,\n",
      "          seed=54),\n",
      "      num_burnin_steps=200,\n",
      "      num_steps_between_results=1,  # Thinning.\n",
      "      parallel_iterations=1)\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  x = tf.squeeze(samples - sample_mean)\n",
      "  sample_cov = tf.matmul(tf.transpose(x, [1, 2, 0]),\n",
      "                         tf.transpose(x, [1, 0, 2])) / num_results\n",
      "\n",
      "  mean_sample_mean = tf.math.reduce_mean(sample_mean)\n",
      "  mean_sample_cov = tf.math.reduce_mean(sample_cov, axis=0)\n",
      "  x = tf.reshape(sample_cov - mean_sample_cov, [num_chains, 2 * 2])\n",
      "  cov_sample_cov = tf.reshape(tf.matmul(x, x, transpose_a=True) / num_chains,\n",
      "                              shape=[2 * 2, 2 * 2])\n",
      "\n",
      "  print('Estimated mean: {}'.format(mean_sample_mean))\n",
      "  print('Estimated avg covariance: {}'.format(mean_sample_cov))\n",
      "  print('Estimated covariance of covariance: {}'.format(cov_sample_cov))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution using Cauchy proposal.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "  num_burnin_steps = 500\n",
      "  num_chain_results = 1000\n",
      "\n",
      "  def cauchy_new_state_fn(scale, dtype):\n",
      "    cauchy = tfd.Cauchy(loc=dtype(0), scale=dtype(scale))\n",
      "    def _fn(state_parts, seed):\n",
      "      next_state_parts = []\n",
      "      seed_stream  = tfp.util.SeedStream(seed, salt='RandomCauchy')\n",
      "      for sp in state_parts:\n",
      "        next_state_parts.append(sp + cauchy.sample(\n",
      "          sample_shape=sp.shape, seed=seed_stream()))\n",
      "      return next_state_parts\n",
      "    return _fn\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  samples, _ = tfp.mcmc.sample_chain(\n",
      "      num_results=num_chain_results,\n",
      "      num_burnin_steps=num_burnin_steps,\n",
      "      current_state=dtype(1),\n",
      "      kernel=tfp.mcmc.RandomWalkMetropolis(\n",
      "          target.log_prob,\n",
      "          new_state_fn=cauchy_new_state_fn(scale=0.5, dtype=dtype),\n",
      "          seed=42),\n",
      "      parallel_iterations=1)  # For determinism.\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.math.reduce_mean(\n",
      "          tf.math.squared_difference(samples, sample_mean),\n",
      "          axis=0))\n",
      "\n",
      "  print('Estimated mean: {}'.format(sample_mean))\n",
      "  print('Estimated standard deviation: {}'.format(sample_std))\n",
      "  ```\n",
      "\n",
      "  \n",
      "=================================\n",
      "Runs one step of Metropolis-adjusted Langevin algorithm.\n",
      "\n",
      "  Metropolis-adjusted Langevin algorithm (MALA) is a Markov chain Monte Carlo\n",
      "  (MCMC) algorithm that takes a step of a discretised Langevin diffusion as a\n",
      "  proposal. This class implements one step of MALA using Euler-Maruyama method\n",
      "  for a given `current_state` and diagonal preconditioning `volatility` matrix.\n",
      "  Mathematical details and derivations can be found in\n",
      "  [Roberts and Rosenthal (1998)][1] and [Xifara et al. (2013)][2].\n",
      "\n",
      "  See `UncalibratedLangevin` class description below for details on the proposal\n",
      "  generating step of the algorithm.\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should reduce log-probabilities across\n",
      "  all event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics are\n",
      "  governed by `target_log_prob_fn(*current_state)`. (The number of independent\n",
      "  chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Simple chain with warm-up.\n",
      "\n",
      "  In this example we sample from a standard univariate normal\n",
      "  distribution using MALA with `step_size` equal to 0.75.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "\n",
      "  with tf.Session(graph=tf.Graph()) as sess:\n",
      "    # Target distribution is Standard Univariate Normal\n",
      "    target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "    # Define MALA sampler with `step_size` equal to 0.75\n",
      "    samples, _ = tfp.mcmc.sample_chain(\n",
      "        num_results=1000,\n",
      "        current_state=dtype(1),\n",
      "        kernel=tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
      "            target_log_prob_fn=target.log_prob,\n",
      "            step_size=0.75,\n",
      "            seed=42),\n",
      "        num_burnin_steps=500,\n",
      "        parallel_iterations=1)  # For determinism.\n",
      "\n",
      "    sample_mean = tf.reduce_mean(samples, axis=0)\n",
      "    sample_std = tf.sqrt(\n",
      "        tf.reduce_mean(tf.squared_difference(samples, sample_mean),\n",
      "                       axis=0))\n",
      "\n",
      "    sess.graph.finalize()  # No more graph building.\n",
      "\n",
      "    [sample_mean_, sample_std_] = sess.run([sample_mean, sample_std])\n",
      "\n",
      "  print('sample mean', sample_mean_)\n",
      "  print('sample standard deviation', sample_std_)\n",
      "  ```\n",
      "\n",
      "  ##### Same example but in eager mode.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "  import matplotlib.pyplot as plt\n",
      "\n",
      "  # Support for eager execution\n",
      "  tf.enable_eager_execution()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "  dtype = np.float32\n",
      "\n",
      "  # Target distribution is Standard Univariate Normal\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  def target_log_prob(x):\n",
      "    return target.log_prob(x)\n",
      "\n",
      "  # Define MALA sampler with `step_size` equal to 0.75\n",
      "  samples, _ = tfp.mcmc.sample_chain(\n",
      "      num_results=1000,\n",
      "      current_state=dtype(1),\n",
      "      kernel=tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
      "          target_log_prob_fn=target_log_prob,\n",
      "          step_size=0.75,\n",
      "          seed=42),\n",
      "      num_burnin_steps=500,\n",
      "      parallel_iterations=1)  # For determinism.\n",
      "\n",
      "  sample_mean = tf.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.reduce_mean(tf.squared_difference(samples, sample_mean),\n",
      "                     axis=0))\n",
      "\n",
      "  print('sample mean', sample_mean)\n",
      "  print('sample standard deviation', sample_std)\n",
      "\n",
      "  plt.title('Traceplot')\n",
      "  plt.plot(samples.numpy(), 'b')\n",
      "  plt.xlabel('Iteration')\n",
      "  plt.ylabel('Position')\n",
      "  plt.show()\n",
      "  ```\n",
      "\n",
      "  ##### Sample from a 3-D Multivariate Normal distribution.\n",
      "\n",
      "  In this example we also consider a non-constant volatility function.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "  true_mean = dtype([0, 0, 0])\n",
      "  true_cov = dtype([[1, 0.25, 0.25], [0.25, 1, 0.25], [0.25, 0.25, 1]])\n",
      "  num_results = 500\n",
      "  num_chains = 500\n",
      "\n",
      "  with tf.Session(graph=tf.Graph()) as sess:\n",
      "    # Target distribution is defined through the Cholesky decomposition\n",
      "    chol = tf.linalg.cholesky(true_cov)\n",
      "    target = tfd.MultivariateNormalTriL(loc=true_mean, scale_tril=chol)\n",
      "\n",
      "    # Assume that the state is passed as a list of tensors `x` and `y`.\n",
      "    # Then the target log-density is defined as follows:\n",
      "    def target_log_prob(x, y):\n",
      "      # Stack the input tensors together\n",
      "      z = tf.concat([x, y], axis=-1) - true_mean\n",
      "      return target.log_prob(z)\n",
      "\n",
      "    # Here we define the volatility function to be non-constant\n",
      "    def volatility_fn(x, y):\n",
      "      # Stack the input tensors together\n",
      "      return [1. / (0.5 + 0.1 * tf.sqrt(x * x)),\n",
      "              1. / (0.5 + 0.1 *tf.sqrt(y * y))]\n",
      "\n",
      "    # Initial state of the chain\n",
      "    init_state = [np.ones([num_chains, 2], dtype=dtype),\n",
      "                  np.ones([num_chains, 1], dtype=dtype)]\n",
      "\n",
      "    # Run MALA with normal proposal for `num_results` iterations for\n",
      "    # `num_chains` independent chains:\n",
      "    states, _ = tfp.mcmc.sample_chain(\n",
      "        num_results=num_results,\n",
      "        current_state=init_state,\n",
      "        kernel=tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
      "            target_log_prob_fn=target_log_prob,\n",
      "            step_size=.1,\n",
      "            volatility_fn=volatility_fn,\n",
      "            seed=42),\n",
      "        num_burnin_steps=200,\n",
      "        num_steps_between_results=1,\n",
      "        parallel_iterations=1)\n",
      "\n",
      "    states = tf.concat(states, axis=-1)\n",
      "    sample_mean = tf.reduce_mean(states, axis=[0, 1])\n",
      "    x = (states - sample_mean)[..., tf.newaxis]\n",
      "    sample_cov = tf.reduce_mean(\n",
      "        tf.matmul(x, tf.transpose(x, [0, 1, 3, 2])), [0, 1])\n",
      "\n",
      "    [sample_mean_, sample_cov_] = sess.run([\n",
      "        sample_mean, sample_cov])\n",
      "\n",
      "  print('sample mean', sample_mean_)\n",
      "  print('sample covariance matrix', sample_cov_)\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Gareth Roberts and Jeffrey Rosenthal. Optimal Scaling of Discrete\n",
      "       Approximations to Langevin Diffusions. _Journal of the Royal Statistical\n",
      "       Society: Series B (Statistical Methodology)_, 60: 255-268, 1998.\n",
      "       https://doi.org/10.1111/1467-9868.00123\n",
      "\n",
      "  [2]: T. Xifara et al. Langevin diffusions and the Metropolis-adjusted\n",
      "       Langevin algorithm. _arXiv preprint arXiv:1309.2983_, 2013.\n",
      "       https://arxiv.org/abs/1309.2983\n",
      "  \n",
      "=================================\n",
      "Runs one step of the No U-Turn Sampler.\n",
      "\n",
      "  The No U-Turn Sampler (NUTS) is an adaptive variant of the Hamiltonian Monte\n",
      "  Carlo (HMC) method for MCMC. NUTS adapts the distance traveled in response to\n",
      "  the curvature of the target density. Conceptually, one proposal consists of\n",
      "  reversibly evolving a trajectory through the sample space, continuing until\n",
      "  that trajectory turns back on itself (hence the name, 'No U-Turn'). This class\n",
      "  implements one random NUTS step from a given `current_state`.\n",
      "  Mathematical details and derivations can be found in\n",
      "  [Hoffman, Gelman (2011)][1] and [Betancourt (2018)][2].\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that a prefix of leftmost dimensions of `current_state` index independent\n",
      "  chain states (and are therefore updated independently).  The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions.  Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0][0, ...]` could have a\n",
      "  different target distribution from `current_state[0][1, ...]`.  These\n",
      "  semantics are governed by `target_log_prob_fn(*current_state)`. (The number of\n",
      "  independent chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Matthew D. Hoffman, Andrew Gelman.  The No-U-Turn Sampler: Adaptively\n",
      "  Setting Path Lengths in Hamiltonian Monte Carlo.  2011.\n",
      "  https://arxiv.org/pdf/1111.4246.pdf.\n",
      "\n",
      "  [2]: Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo.\n",
      "  _arXiv preprint arXiv:1701.02434_, 2018. https://arxiv.org/abs/1701.02434\n",
      "  \n",
      "=================================\n",
      "Runs one step of the Replica Exchange Monte Carlo.\n",
      "\n",
      "  [Replica Exchange Monte Carlo](\n",
      "  https://en.wikipedia.org/wiki/Parallel_tempering) is a Markov chain\n",
      "  Monte Carlo (MCMC) algorithm that is also known as Parallel Tempering. This\n",
      "  algorithm takes multiple samples (from tempered distributions) in parallel,\n",
      "  then swaps these samples according to the Metropolis-Hastings criterion.\n",
      "  See also the review paper [1].\n",
      "\n",
      "  The `K` replicas are parameterized in terms of `inverse_temperature`'s,\n",
      "  `(beta[0], beta[1], ..., beta[K-1])`.  If the target distribution has\n",
      "  probability density `p(x)`, the `kth` replica has density `p(x)**beta_k`.\n",
      "\n",
      "  Typically `beta[0] = 1.0`, and `1.0 > beta[1] > beta[2] > ... > 0.0`.\n",
      "  Trying geometrically decaying `beta` is good starting point.\n",
      "\n",
      "  * `beta[0] == 1` ==> First replicas samples from the target density, `p`.\n",
      "  * `beta[k] < 1`, for `k = 1, ..., K-1` ==> Other replicas sample from\n",
      "    \"flattened\" versions of `p` (peak is less high, valley less low).  These\n",
      "    distributions are somewhat closer to a uniform on the support of `p`.\n",
      "\n",
      "  By default, samples from adjacent replicas `i`, `i + 1` are used as proposals\n",
      "  for each other in a Metropolis step.  This allows the lower `beta` samples,\n",
      "  which explore less dense areas of `p`, to eventually swap state with the\n",
      "  `beta == 1` chain, allowing it to explore these new regions.\n",
      "\n",
      "  Samples from replica 0 are returned, and the others are discarded.\n",
      "\n",
      "  #### Examples\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = tf.float32\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  # Geometric decay is a good rule of thumb.\n",
      "  inverse_temperatures = 0.5**tf.range(4, dtype=dtype)\n",
      "\n",
      "  # If everything was Normal, step_size should be ~ sqrt(temperature).\n",
      "  step_size = 0.5 / tf.sqrt(inverse_temperatures)\n",
      "\n",
      "  def make_kernel_fn(target_log_prob_fn, seed):\n",
      "    return tfp.mcmc.HamiltonianMonteCarlo(\n",
      "        target_log_prob_fn=target_log_prob_fn,\n",
      "        seed=seed, step_size=step_size, num_leapfrog_steps=3)\n",
      "\n",
      "  remc = tfp.mcmc.ReplicaExchangeMC(\n",
      "      target_log_prob_fn=target.log_prob,\n",
      "      inverse_temperatures=inverse_temperatures,\n",
      "      make_kernel_fn=make_kernel_fn)\n",
      "\n",
      "  def trace_swaps(unused_state, results):\n",
      "    return (results.is_swap_proposed_adjacent,\n",
      "            results.is_swap_accepted_adjacent)\n",
      "\n",
      "  samples, is_swap_proposed_adjacent, is_swap_accepted_adjacent = (\n",
      "      tfp.mcmc.sample_chain(\n",
      "          num_results=1000,\n",
      "          current_state=1.0,\n",
      "          kernel=remc,\n",
      "          num_burnin_steps=500,\n",
      "          trace_fn=trace_swaps)\n",
      "  )\n",
      "\n",
      "  # conditional_swap_prob[k] = P[ExchangeAccepted | ExchangeProposed],\n",
      "  # for the swap between replicas k and k+1.\n",
      "  conditional_swap_prob = (\n",
      "      tf.reduce_sum(tf.cast(is_swap_accepted_adjacent, tf.float32), axis=0)\n",
      "      /\n",
      "      tf.reduce_sum(tf.cast(is_swap_proposed_adjacent, tf.float32), axis=0))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from a 2-D Mixture Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import matplotlib.pyplot as plt\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = tf.float32\n",
      "\n",
      "  target = tfd.MixtureSameFamily(\n",
      "      mixture_distribution=tfd.Categorical(probs=[0.5, 0.5]),\n",
      "      components_distribution=tfd.MultivariateNormalDiag(\n",
      "          loc=[[-1., -1], [1., 1.]],\n",
      "          scale_identity_multiplier=[0.1, 0.1]))\n",
      "\n",
      "  inverse_temperatures = 0.5**tf.range(4, dtype=dtype)\n",
      "\n",
      "  # step_size must broadcast with all batch and event dimensions of target.\n",
      "  # Here, this means it must broadcast with:\n",
      "  #  [len(inverse_temperatures)] + target.event_shape\n",
      "  step_size = 0.5 / tf.reshape(tf.sqrt(inverse_temperatures), shape=(4, 1))\n",
      "\n",
      "  def make_kernel_fn(target_log_prob_fn, seed):\n",
      "    return tfp.mcmc.HamiltonianMonteCarlo(\n",
      "        target_log_prob_fn=target_log_prob_fn,\n",
      "        seed=seed, step_size=step_size, num_leapfrog_steps=3)\n",
      "\n",
      "  remc = tfp.mcmc.ReplicaExchangeMC(\n",
      "      target_log_prob_fn=target.log_prob,\n",
      "      inverse_temperatures=inverse_temperatures,\n",
      "      make_kernel_fn=make_kernel_fn)\n",
      "\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "      num_results=1000,\n",
      "      # Start near the [1, 1] mode.  Standard HMC would get stuck there.\n",
      "      current_state=tf.ones(2, dtype=dtype),\n",
      "      kernel=remc,\n",
      "      trace_fn=None,\n",
      "      num_burnin_steps=500)\n",
      "\n",
      "  plt.figure(figsize=(8, 8))\n",
      "  plt.xlim(-2, 2)\n",
      "  plt.ylim(-2, 2)\n",
      "  plt.plot(samples_[:, 0], samples_[:, 1], '.')\n",
      "  plt.show()\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: David J. Earl, Michael W. Deem\n",
      "       Parallel Tempering: Theory, Applications, and New Perspectives\n",
      "       https://arxiv.org/abs/physics/0508111\n",
      "  \n",
      "=================================\n",
      "Runs one step of Hamiltonian Monte Carlo.\n",
      "\n",
      "  Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm\n",
      "  that takes a series of gradient-informed steps to produce a Metropolis\n",
      "  proposal. This class implements one random HMC step from a given\n",
      "  `current_state`. Mathematical details and derivations can be found in\n",
      "  [Neal (2011)][1].\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics are\n",
      "  governed by `target_log_prob_fn(*current_state)`. (The number of independent\n",
      "  chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Simple chain with warm-up.\n",
      "\n",
      "  In this example we sample from a standard univariate normal\n",
      "  distribution using HMC with adaptive step size.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "\n",
      "  tf.enable_eager_execution()\n",
      "\n",
      "  # Target distribution is proportional to: `exp(-x (1 + x))`.\n",
      "  def unnormalized_log_prob(x):\n",
      "    return -x - x**2.\n",
      "\n",
      "  # Initialize the HMC transition kernel.\n",
      "  num_results = int(10e3)\n",
      "  num_burnin_steps = int(1e3)\n",
      "  adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
      "      tfp.mcmc.HamiltonianMonteCarlo(\n",
      "          target_log_prob_fn=unnormalized_log_prob,\n",
      "          num_leapfrog_steps=3,\n",
      "          step_size=1.),\n",
      "      num_adaptation_steps=int(num_burnin_steps * 0.8))\n",
      "\n",
      "  # Run the chain (with burn-in).\n",
      "  @tf.function\n",
      "  def run_chain():\n",
      "    # Run the chain (with burn-in).\n",
      "    samples, is_accepted = tfp.mcmc.sample_chain(\n",
      "        num_results=num_results,\n",
      "        num_burnin_steps=num_burnin_steps,\n",
      "        current_state=1.,\n",
      "        kernel=adaptive_hmc,\n",
      "        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)\n",
      "\n",
      "    sample_mean = tf.reduce_mean(samples)\n",
      "    sample_stddev = tf.math.reduce_std(samples)\n",
      "    is_accepted = tf.reduce_mean(tf.cast(is_accepted, dtype=tf.float32))\n",
      "    return sample_mean, sample_stddev, is_accepted\n",
      "\n",
      "  sample_mean, sample_stddev, is_accepted = run_chain()\n",
      "\n",
      "  print('mean:{:.4f}  stddev:{:.4f}  acceptance:{:.4f}'.format(\n",
      "      sample_mean.numpy(), sample_stddev.numpy(), is_accepted.numpy()))\n",
      "  ```\n",
      "\n",
      "  ##### Estimate parameters of a more complicated posterior.\n",
      "\n",
      "  In this example, we'll use Monte-Carlo EM to find best-fit parameters. See\n",
      "  [_Convergence of a stochastic approximation version of the EM algorithm_][2]\n",
      "  for more details.\n",
      "\n",
      "  More precisely, we use HMC to form a chain conditioned on parameter `sigma`\n",
      "  and training data `{ (x[i], y[i]) : i=1...n }`. Then we use one gradient step\n",
      "  of maximum-likelihood to improve the `sigma` estimate. Then repeat the process\n",
      "  until convergence. (This procedure is a [Robbins--Monro algorithm](\n",
      "  https://en.wikipedia.org/wiki/Stochastic_approximation).)\n",
      "\n",
      "  The generative assumptions are:\n",
      "\n",
      "  ```none\n",
      "    W ~ MVN(loc=0, scale=sigma * eye(dims))\n",
      "    for i=1...num_samples:\n",
      "        X[i] ~ MVN(loc=0, scale=eye(dims))\n",
      "      eps[i] ~ Normal(loc=0, scale=1)\n",
      "        Y[i] = X[i].T * W + eps[i]\n",
      "  ```\n",
      "\n",
      "  We now implement a stochastic approximation of Expectation Maximization (SAEM)\n",
      "  using `tensorflow_probability` intrinsics. [Bernard (1999)][2]\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "\n",
      "  tf.enable_eager_execution()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  def make_training_data(num_samples, dims, sigma):\n",
      "    dt = np.asarray(sigma).dtype\n",
      "    x = np.random.randn(dims, num_samples).astype(dt)\n",
      "    w = sigma * np.random.randn(1, dims).astype(dt)\n",
      "    noise = np.random.randn(num_samples).astype(dt)\n",
      "    y = w.dot(x) + noise\n",
      "    return y[0], x, w[0]\n",
      "\n",
      "  def make_weights_prior(dims, log_sigma):\n",
      "    return tfd.MultivariateNormalDiag(\n",
      "        loc=tf.zeros([dims], dtype=log_sigma.dtype),\n",
      "        scale_identity_multiplier=tf.math.exp(log_sigma))\n",
      "\n",
      "  def make_response_likelihood(w, x):\n",
      "    if w.shape.ndims == 1:\n",
      "      y_bar = tf.matmul(w[tf.newaxis], x)[0]\n",
      "    else:\n",
      "      y_bar = tf.matmul(w, x)\n",
      "    return tfd.Normal(loc=y_bar, scale=tf.ones_like(y_bar))  # [n]\n",
      "\n",
      "  # Setup assumptions.\n",
      "  dtype = np.float32\n",
      "  num_samples = 500\n",
      "  dims = 10\n",
      "  tf.compat.v1.random.set_random_seed(10014)\n",
      "  np.random.seed(10014)\n",
      "\n",
      "  weights_prior_true_scale = np.array(0.3, dtype)\n",
      "  y, x, _ = make_training_data(\n",
      "      num_samples, dims, weights_prior_true_scale)\n",
      "\n",
      "  log_sigma = tf.Variable(0., dtype=dtype, name='log_sigma')\n",
      "\n",
      "  optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
      "\n",
      "  @tf.function\n",
      "  def mcem_iter(weights_chain_start, step_size):\n",
      "    with tf.GradientTape() as tape:\n",
      "      tape.watch(log_sigma)\n",
      "      prior = make_weights_prior(dims, log_sigma)\n",
      "\n",
      "      def unnormalized_posterior_log_prob(w):\n",
      "        likelihood = make_response_likelihood(w, x)\n",
      "        return (\n",
      "            prior.log_prob(w) +\n",
      "            tf.reduce_sum(likelihood.log_prob(y), axis=-1))  # [m]\n",
      "\n",
      "      def trace_fn(_, pkr):\n",
      "        return (\n",
      "            pkr.inner_results.log_accept_ratio,\n",
      "            pkr.inner_results.accepted_results.target_log_prob,\n",
      "            pkr.inner_results.accepted_results.step_size)\n",
      "\n",
      "      num_results = 2\n",
      "      weights, (\n",
      "          log_accept_ratio, target_log_prob, step_size) = tfp.mcmc.sample_chain(\n",
      "          num_results=num_results,\n",
      "          num_burnin_steps=0,\n",
      "          current_state=weights_chain_start,\n",
      "          kernel=tfp.mcmc.SimpleStepSizeAdaptation(\n",
      "              tfp.mcmc.HamiltonianMonteCarlo(\n",
      "                  target_log_prob_fn=unnormalized_posterior_log_prob,\n",
      "                  num_leapfrog_steps=2,\n",
      "                  step_size=step_size,\n",
      "                  state_gradients_are_stopped=True,\n",
      "              ),\n",
      "              # Adapt for the entirety of the trajectory.\n",
      "              num_adaptation_steps=2),\n",
      "          trace_fn=trace_fn,\n",
      "          parallel_iterations=1)\n",
      "\n",
      "      # We do an optimization step to propagate `log_sigma` after two HMC\n",
      "      # steps to propagate `weights`.\n",
      "      loss = -tf.reduce_mean(target_log_prob)\n",
      "\n",
      "    avg_acceptance_ratio = tf.math.exp(\n",
      "        tfp.math.reduce_logmeanexp(tf.minimum(log_accept_ratio, 0.)))\n",
      "\n",
      "    optimizer.apply_gradients(\n",
      "        [[tape.gradient(loss, log_sigma), log_sigma]])\n",
      "\n",
      "    weights_prior_estimated_scale = tf.math.exp(log_sigma)\n",
      "    return (weights_prior_estimated_scale, weights[-1], loss,\n",
      "            step_size[-1], avg_acceptance_ratio)\n",
      "\n",
      "  num_iters = int(40)\n",
      "\n",
      "  weights_prior_estimated_scale_ = np.zeros(num_iters, dtype)\n",
      "  weights_ = np.zeros([num_iters + 1, dims], dtype)\n",
      "  loss_ = np.zeros([num_iters], dtype)\n",
      "  weights_[0] = np.random.randn(dims).astype(dtype)\n",
      "  step_size_ = 0.03\n",
      "\n",
      "  for iter_ in range(num_iters):\n",
      "    [\n",
      "        weights_prior_estimated_scale_[iter_],\n",
      "        weights_[iter_ + 1],\n",
      "        loss_[iter_],\n",
      "        step_size_,\n",
      "        avg_acceptance_ratio_,\n",
      "    ] = mcem_iter(weights_[iter_], step_size_)\n",
      "    tf.compat.v1.logging.vlog(\n",
      "        1, ('iter:{:>2}  loss:{: 9.3f}  scale:{:.3f}  '\n",
      "            'step_size:{:.4f}  avg_acceptance_ratio:{:.4f}').format(\n",
      "                iter_, loss_[iter_], weights_prior_estimated_scale_[iter_],\n",
      "                step_size_, avg_acceptance_ratio_))\n",
      "\n",
      "  # Should converge to ~0.22.\n",
      "  import matplotlib.pyplot as plt\n",
      "  plt.plot(weights_prior_estimated_scale_)\n",
      "  plt.ylabel('weights_prior_estimated_scale')\n",
      "  plt.xlabel('iteration')\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Radford Neal. MCMC Using Hamiltonian Dynamics. _Handbook of Markov Chain\n",
      "       Monte Carlo_, 2011. https://arxiv.org/abs/1206.1901\n",
      "\n",
      "  [2]: Bernard Delyon, Marc Lavielle, Eric, Moulines. _Convergence of a\n",
      "       stochastic approximation version of the EM algorithm_, Ann. Statist. 27\n",
      "       (1999), no. 1, 94--128. https://projecteuclid.org/euclid.aos/1018031103\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(tfp.mcmc.MetropolisHastings.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.RandomWalkMetropolis.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.MetropolisAdjustedLangevinAlgorithm.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.NoUTurnSampler.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.ReplicaExchangeMC.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.HamiltonianMonteCarlo.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler divergence, also called the relative entropy or discrimination information, can be computed between two distributions, say $x$ and $y$ and writes $ D_{KL}(x\\|y) $.\n",
    "This metric represents the expectation of the difference of the number of bits needed to encode a symbol from the distribution x, weither the encoding is optimal for the distribution x or y.\n",
    "\n",
    "The general case reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\mathbb{E}_x \\left[ log \\left( \\frac{x}{y} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "In the discrete case it reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\sum_{i\\in I} p(x_i) log(p(x_i)) -\\sum_{i\\in I} p(x_i) log(p(y_i)) \\\\\n",
    "                 &= -H(x) + H_y(x)\n",
    "\\end{align}\n",
    "or equivalently\n",
    "$$\n",
    "    D_{KL}(x\\|y) = \\sum_{i\\in I} p(x_i) log\\left(\\frac{p(x_i)}{p(y_i)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### KL Divergence and maximum likelihood\n",
    "This chapter was inspired by [this article](http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/)\n",
    "by Liangjie Hong\n",
    "\n",
    "#### Definitions\n",
    "Let's consider $P(x_i|\\theta)$ a distribution used to generate (or sample) $N$ points $x_i, i=0,\\dots,N-1$\n",
    "\n",
    "We can define a general distribution model as:\n",
    "$$\n",
    "    P_\\theta(x) = P(x|\\theta)\n",
    "$$\n",
    "And, to handle data from real world, we will define the following empirical distribution (also called probability mass function):\n",
    "$$\n",
    "    P_D(x) = \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i)\n",
    "$$\n",
    "\n",
    "#### KL Divergence between model and data\n",
    "We can now express the Kullback Leibler divergence between the model distribution and the empirical one:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) &= \\int P_D(x) log(P_D(x)) dx -\\int P_D(x) log(P_\\theta(x)) dx \\\\\n",
    "    &= H(P_D(x)) - \\int P_D(x) log(P_\\theta(x)) dx\n",
    "\\end{align}\n",
    "\n",
    "As $H(P_D(x))$ is a constant term, relatd to the empirical distribution of real data, we will call it $\\epsilon$, and we may get rid of it later.\n",
    "Instead we will mainly consider, the $\\theta$, ie, the model related term:\n",
    "$$\n",
    "   D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) = \\epsilon -\\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2}\n",
    "$$\n",
    "\n",
    "Thanks to the definition of the empirical distribution, we can express this continuous expression as a discrete summation:\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2} &= \\int \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i) log(P(x|\\theta)) dx \\\\\n",
    "    &= \\sum_{i=0}^{N-1} \\frac{1}{N} log(P(x_i|\\theta)) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{i=0}^{N-1} log(P(x_i|\\theta))\n",
    "\\end{align}\n",
    "\n",
    "The last line amounts to the log-likelihood of the dataset. We can then conclude that maximizing the log-likelihood of a dataset, relatively to a set of distribution parameters $\\theta$ and a given dataset $x$ is equivalent to minimizing the Kullback Leibler divergence between the empirical distribution and the model distribution.\n",
    "\n",
    "### Practical KL Divergence between two data sets\n",
    "We are interested in computing the Kullback Leibler divergence between a model $X$ and a real dataset $Y$, indexed by $i$ where $i$ is the index of a specific event whose occurence probability is $X_i$ according to the model, and $Y_i$ in practice.\n",
    "\n",
    "As in the definition of the empirical distribution, our datasets must verify:\n",
    "$$\n",
    "    \\sum_{i=0}^{N-1} X_i = 1\n",
    "$$\n",
    "\n",
    "then the expression of the kl divergence is simply:\n",
    "$$\n",
    "    D_{KL}(Y\\|X) = \\sum_{i=0}^{N-1} log \\left( \\frac{Y_i}{X_i} \\right) Y_i\n",
    "$$\n",
    "\n",
    "Moreover there is an important condition that must be respected: \n",
    "$$\n",
    "    X_i = 0 \\implies Y_i = 0\n",
    "$$\n",
    "If so, if the event $k$ has no occurence, we have $X_k=Y_k=0$ and $log \\left( \\frac{y_k}{x_k} \\right) y_k$ has a limit in $0$ which is $0$. Otherwise, Kullback Leibler cannot be computed.\n",
    "\n",
    "The explanation is pretty straightforward: if an event, or a symbol from $Y$ has no occurence in $X$, it has no reason to be encoded using $X$ probability distribution, then the average number of bits needed to encode a symbol from $Y$ is a nonsense, as some symbols cannot be represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is variational inference ?\n",
    "\n",
    "### Variational distribution for the posterior\n",
    "\n",
    "We are still in the framework of bayes theorem, and we want to compute $p$ our posterior that stands for $p(\\theta|x)$.\n",
    "\n",
    "In variational inference,  you create a variational distribution $q$ over your model parameters $\\theta$, that we will call latent variables here. A variational distribution is parametrized by a variational parameter, we will call $\\nu$:\n",
    "\n",
    "$$\n",
    " q(\\theta|\\nu)\n",
    "$$\n",
    "\n",
    "The first idea of variational inference, is that we will try to find $\\nu$ such that $q(\\theta|\\nu)$ becomes close to our posterior distribution $p(\\theta|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-divergence for variational/posterior closeness\n",
    "\n",
    "The closeness of those distribution will be computed using Kullback-Liebler divergence:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(\\theta)}{p(\\theta|x)} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "As already mentionned earlier, it is good to remember that KL-divergence is not symmetric, in terms of semantic, one must keep in mind that the measure is relates to the expectation over $q$ distribution. So basically, rare event with respect to $q$ won't result in a large diverge, even if their occurence is high in $p$ (you will use few bits to represent rare events, that's not a big deal). However, if high probably events from $q$ are rare in the posterior, (hence coded with high number of bits) you will need to code this event (or symbol) many time, with many bits, and the divergence with respect to an optimal coding will be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's inequality and evidence lower bound (ELBO)\n",
    "\n",
    "Let's recall, that, for concave functions, over a domain $d$ we have\n",
    "\\begin{align*}\n",
    "  f\\left(\\mathbb{E}_d \\left[ x \\right] \\right) \\geq \\mathbb{E}_d \\left[f\\left( x \\right)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Let's now rewrite the log probability ouf our evidence, as latent variable marginalization, and introduce our variational distribution $q(\\theta)$ with a small trick:\n",
    "\\begin{align*}\n",
    "  $log(p(x)) &= log \\left(\\int_{\\theta} p(x,\\theta) \\right) \\\\\n",
    "  &= log \\left(\\int_{\\theta} p(x,\\theta) \\frac{q(\\theta)}{q(\\theta)} \\right) \\\\\n",
    "  &= log \\left(\\mathbb{E}_q \\left[ \\frac{p(x,\\theta)}{q(\\theta)} \\right] \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "As $log$ is a nice concave function, we can give a lower bound of the evidence in the previous expression\n",
    "\\begin{align*}\n",
    "  log(p(x)) &= log \\left(\\mathbb{E}_q \\left[ \\frac{p(x,\\theta)}{q(\\theta)} \\right] \\right) \\\\\n",
    "  log(p(x)) &\\geq \\mathbb{E}_q \\left[ log \\left(\\frac{p(x,\\theta)}{q(\\theta)} \\right) \\right] \\\\\n",
    "  log(p(x)) \\geq \\text{ELBO} &= \\mathbb{E}_q \\left[ log \\left(p(x,\\theta)\\right) -log\\left( q(\\theta) \\right) \\right] \\\\\n",
    "  \\text{ELBO} &= \\underbrace{ \\mathbb{E}_q \\left[log \\left(p(x,\\theta)\\right)\\right]}_{\\text{Expectation under} \\; q \\; \\text{of the joint evidence, latent variable probability}} - \n",
    "  \\underbrace{\\mathbb{E}_q \\left[log \\left( q(\\theta) \\right) \\right]}_{\\text{Entropy of our variational distribution }q} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Ok what is the actual interest of this lower bound expression ? We need to understand how this later expression is linked with the KL divergence between $p$ and $q$, that reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(\\theta)}{p(\\theta|x)} \\right) \\right] \\\\\n",
    "    &= \\mathbb{E}_q \\left[ log \\left( q(\\theta) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(\\theta|x) \\right) \\right] \\\\\n",
    "        &= \\mathbb{E}_q \\left[ log \\left( q(\\theta) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(\\theta,x) \\right) \\right] + \\mathbb{E}_q \\left[ log \\left( p(x)\\right) \\right] \\\\\n",
    "    &= -\\left( \\underbrace{\\mathbb{E}_q \\left[ log \\left( p(x,\\theta) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( q(\\theta) \\right) \\right]}_{\\text{= ELBO}}\\right) + log \\left( p(x)\\right)\\\\\n",
    "\\end{align}\n",
    "\n",
    "The expectation in the last term $log \\left( p(x)\\right)$ goes away, as there is no dependance to $q$, it has somehow already been marginalized away. This overall evidence probability is a constant, hence can be discarded for our purpose, which is, finding an optimal surrogate $q(\\theta)$ to approximate our posterior $p(\\theta|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean field variational inference\n",
    "\n",
    "There are plenty of families of distributions that have been studied in the framework of variational inference, exponential families, neural networks, Gaussian processes, ...\n",
    "\n",
    "Let's assume that the variational distribution factorizes:\n",
    "\n",
    "\\begin{align}\n",
    "    q(\\theta_0, \\theta_1, \\dots , \\theta_{m-1}) = \\prod_{j=0}^{m-1} q(\\theta_j) \\\\\n",
    "\\end{align}\n",
    "\n",
    "That would of course only work if all dimensions are independants. But that's one easy solution to start with a simple variational inference workflow:\n",
    "\n",
    "1. Choose a model distribution $q$ (normal for instance ?)\n",
    "2. Derive ELBO expression\n",
    "3. Perform coordinate ascent on each of the $\\theta_i$\n",
    "4. Eventually repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic introduction to generative paradigm\n",
    "\n",
    "### Generative interpretation of PCA\n",
    "\n",
    "To serve as a basic introductory example to generative approach in machine learning/signal processing, we will use the famous PCA example and see how it can be analyzed from a generative perspective.\n",
    "This example is heavily inspired by Mahdi Karami PhD thesis, chapter 2, later also refered in this document.\n",
    "\n",
    "Let $z \\in \\mathbb{R}^d$ be a random vector of observation, PCA can be defined as linear transformation of $z$ into a subspace $\\mathbb{R}^{d_0}$ as $r = U^T z$ where $U\\in \\mathbb{R}^{d\\times d_0}$ and $0<d_0\\leq d$ such that the columns from $U^T$ that stands for the subspaces selected for the projections capture a maximum of variance with respect to a given set of data.\n",
    "\n",
    "Let $\\left( \\mu, \\Sigma \\right)$ denote the mean and covariance matrices of $z$. As seen in the StatisticalTes notebook, we recall that the solution of PCA is a decorrelation matrix, and explain how it is obtained in next paragraph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall on PCA\n",
    "Let $X$ be the matrix containing the centered samples (1 sample per column) each one drawn from the same distribution.\n",
    "The covariance matrix $\\Sigma$ then writes $\\Sigma=XX^T$, then the decorrelation matrix $W_D$ should verify,\n",
    "\n",
    "for decorrelation:\n",
    "\n",
    "\\begin{align*}\n",
    "  W_D X (W_D X)^T &= W_D X X^T W_D^T\\\\\n",
    "  &= W_D \\Sigma W_D^T \\\\\n",
    "  &= D\n",
    "\\end{align*}\n",
    "\n",
    "where $W_D \\Sigma W_D^T = D$ is a diagonal matrix.\n",
    "One can use an eigenvalue decomposition of the form $\\Sigma = U D U^T$, or equivalently, thanks to positive definitiveness and spectral theorem: $D = U^T \\Sigma U$ and make the subsequent identification $W_D=U^T$\n",
    "\n",
    "By disentangling the linear dependancies in the centered dataset, $U^T$ allows to somehow simplify the resulting distribution. Moreover, if you only keep the $d_0$ eigen vectors associated with the largest eigenvalues, then you project onto a space that where the distribution approximation becomes even simpler (lower dimensional gaussian) while retaining most of the information (information is assumed to be measured in term of variance within this context).\n",
    "This reduced form can be written:\n",
    "\\begin{align*}\n",
    "  D_{d_0} = U_{d_0}^T \\Sigma U_{d_0}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic interpretation\n",
    "Assume a probabilistic model of the form:\n",
    "\\begin{align*}\n",
    "  \\phi \\mathtt{\\sim} \\mathcal{N} \\left( \\mu_0, D_{d_0}\\right), \\quad 0 < d_0 \\leq d \\\\\n",
    "  z|\\phi \\mathtt{\\sim} \\mathcal{N} \\left( W \\phi + \\mu_{\\epsilon}, \\sigma^2 D_{d_0}\\right), \\quad W\\in \\mathbb{R}^{d\\times d_0} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let's try to see how the solution for the maximum likelihood for this model would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming distributions with bijectors\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The fundamental equation we need here, is known for a very long time, and relates to change of variable, initially used to integrate quantities in physics.\n",
    "Given an invertible function:$f:\\underset{Z}{z} \\mapsto \\underset{X}{x}$ and a distribution $p_{\\theta}(z), \\; z \\in Z$, the change of variable forumla reads\n",
    "\n",
    "\\begin{align*}\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f^{-1}(x) \\right) |det \\left( J_{f^{-1}} \\right)| \\\\\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f^{-1}(x) \\right) \\frac{1}{|det \\left( J_{f}\\right)|} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $|det \\left( J_{f}\\right)|$ is the absolute value of the determinant of the Jacobian of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick recall, what is a Jacobian ?\n",
    "\n",
    "We first recall that, for a function $f:\\underset{\\mathbb{R}^n}{x} \\mapsto \\underset{\\mathbb{R}^n}{f(x)}$, we can derive the matrix of partial derivative, called the jacobian matrix:\n",
    "\n",
    "\\begin{align*}\n",
    "  J=\\begin{pmatrix}\n",
    "    \\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\dots & \\frac{\\partial f_0}{\\partial x_{n-1}}\\\\\n",
    "    \\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_{n-1}}\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  \\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} & \\dots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule for transformation composition\n",
    "\n",
    "As seen in the DifferentiatingPerceptron notebook, for complex functions, that are applied each after another, like in most deep learning models (excepted here we are only interested in bijective layers), the chain rule applies, and we have, for an initial distribution:\n",
    "\\begin{align*}\n",
    "  p_{\\theta}(z), \\; z \\in Z\n",
    "\\end{align*}\n",
    "And a chain of bijective transforms:\n",
    "\\begin{align*}\n",
    "  x = f_{\\theta}(z) = f_{K-1} \\circ f_{K-2} \\circ \\dots \\circ f_{1} \\circ f_{0}(z)\n",
    "\\end{align*}\n",
    "The following holds\n",
    "\\begin{align*}\n",
    "  p_{\\theta}'(x) &= p_{\\theta}(z) \\prod_{i=0}^{K-1}\\frac{1}{|det \\left( J_{f_i}\\right)|} \\\\\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f_{\\theta}^{-1}(x) \\right) \\frac{1}{|det \\left( J_{f_{\\theta}}\\right)|} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Throughout this notebook, we will mostly use $z$ as a placeholder for the latent variable, $f_{\\theta}$ as the bijective transformation, parametrized by $\\theta$ that can generated and observed variable $x$, the later will often have larger dimension, to express images or time series.\n",
    "\n",
    "Now, let's assume we would like to compute the (log) likelihood of a given sample $x$:\n",
    "\\begin{align*}\n",
    "  log (p_{\\theta}'(x)) &= log(p_{\\theta}(z)) - \\sum_{i=0}^{K-1} log \\left( |det \\left( J_{f_i}\\right)| \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Depending on the invertible set of transform you use, given you have acces to automatic differentiation, this expression of likelihood can be use within an optimization objective in order to learn useful representations encoded by $f_{\\theta}$, given a set of training examples $\\{x_0, x_1, \\dots x_{n-1} \\in X\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE vs GANs vs NF\n",
    "\n",
    "As we have seen previously, VAE use variational inference, which is optimizing for the ELBO, which is a surrogate for the posterior distribution of latent variables given a sample for the encoder. We don't get the exact posterior.\n",
    "\n",
    "In GAN, there is basically no encoder, hence it is impossible to compute likelihood of latent variable given a sample. We can only train the generator/discriminator in a min/max fashion, with the usual risks of mode collapse, or infinite oscillations.\n",
    "\n",
    "From a theoretical point of view, NF has very interesting properties, as it allows exact likelihood evaluation, and potentially optimization during training by $log(p_{\\theta}(z)) - \\sum_{i=0}^{K-1} log \\left( |det \\left( J_{f_i}\\right)| \\right)$\n",
    "Exact posterior inference for a given $x$ throught the invertible transform $z=f_{\\theta}^{-1}(x)$\n",
    "\n",
    "Unfortunately, all those theoretical advantages will come with complex numerical challenges, such as finding transformations with tractable and numerically stable Jacobian determinant computations. One often tries to find transformations which Jacobian is diagonal or triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows: example of bijective systems in DL\n",
    "\n",
    "### Non linear independent components estimation (NICE)\n",
    "\n",
    "#### The triangular trick\n",
    "In the paper Non Linear independent components estimation (see [here](https://arxiv.org/abs/1410.8516) or [here](https://paperswithcode.com/method/nice) ) the authors use the concept of coupling layer.\n",
    "\n",
    "Coupling layer subsets input, let say for instance into 2 subsets (by simply projecting onto two complementary and trivial supports) and then apply identity on one part of this subset, and apply more complex transformation $g$ that use both subsets for the second output, with an intermediate transformation $m$ over thr first subset:\n",
    "\n",
    "* inputs: $z_{0:d-1}$ and $z_{d:D-1}$\n",
    "* output 1 is identity: $x_{0:d-1} = z_{0:d-1}$\n",
    "* output 2: $x_{d:D-1} = g(z_{d:D-1},m(z_{0:d-1}))$\n",
    "\n",
    "If you look carefully, you can see that the Jacobian of said transformation will look like this:\n",
    "\n",
    "\\begin{align*}\n",
    "  J_f &= \\begin{pmatrix}\n",
    "    \\frac{\\partial Id_{0:d-1}}{\\partial z_{0:d-1}} & \\frac{\\partial Id_{0:d-1}}{z_{d:D-1}} \\\\\n",
    "    \\frac{\\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{0:d-1}} & \\frac{ \\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{d:D-1}} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix}\n",
    "    Id_{0:d-1} & 0\\\\\n",
    "    \\frac{\\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{0:d-1}} & \\frac{ \\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{d:D-1}} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The problem of having a triangular jacobian then amounts to having the lower right part of previous expression triangular. If we analyse the invertibility requirement, we can see that, Identity being trivially invertible, we should concentrate on $g$.\n",
    "Now $g$ mainly needs to be invertible with regard to its first argument, given the second, because second argument $m(z_{0:d-1})$ can always be fully inverted thanks to the identity in the other output. In the NICE paper, $g$ is set to be a simple addition, and all the complexity (DNN) is used in the $m$ function.\n",
    "\n",
    "#### Overcoming additive coupling with scaling matrix\n",
    "Additive coupling layers like $g$ given as an example, are very limited in the sense that their unit gaussian forbids local expansion or contractions, which drastically reduces the expressivity of such functions.\n",
    "\n",
    "To start with, given one of our target which is to keep latent space $Z$ distribution to be as simple as possible (gaussian ?) while having the power to be matched with complex multimodal distribution of sample space $X$, we need those complex transformation.\n",
    "Also in the framework of generative network, we would like high density regions from sample space $X$ to expand to large regions of the latent space $Z$ for expressivity.\n",
    "\n",
    "The authors show that, this problem can somehow be circumvented by using a diagonal feature scaling matrix $S$ before applying $f$ transformations when decoding from the latent space.\n",
    "The log likelihood for a given decoding then reads:\n",
    "\n",
    "\\begin{align*}\n",
    "  log (p_{\\theta}'(x)) &= \\sum_{i=0}^{D-1} log(p_{\\theta}(f^{-1}(x)_i) - log \\left( |S_{ii}| \\right) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond dimensionality constraints of bijectors: Real NVP\n",
    "\n",
    "The problem with bijectors, is that by design, input and output dimensionality should be equal. Although in general, latent space in generative models are expected to be smaller in dimension. This is the problem the authors of [Real NVP](https://arxiv.org/abs/1605.08803) see explanation with code [here](https://paperswithcode.com/method/realnvp) or [here](https://keras.io/examples/generative/real_nvp) tried to address.\n",
    "\n",
    "In this work, authors tried to use a multiscal approach for images, where each set of latent variable was operating on a different resolution from the sample space.\n",
    "\n",
    "This allows for instance to drop high resolution elements during inference (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing flows and VAE\n",
    "\n",
    "Up to now, we have mostly seen how to derive likelihood of samples from a distribution of latent variable (hoping to back propagate the error in likelihood to learn something useful).\n",
    "\n",
    "We might want to parametrize the approximate posterior $p(x|z)$ in a VAE with a flow to form richer distributions. This is very interesting, as usually in VAE you are restricted to a simple multidimensional gaussian distribution in the latent space, although you could leverage normalizing flow to turn this gaussian encoder output into a more complex distribution before decoding.\n",
    "\n",
    "![title](data/vae_normalizing_flow.png)\n",
    "![title](data/vae_normalizing_flow2.png)\n",
    "courtesy: Ari Seff from [here](https://www.youtube.com/watch?v=i7LjDvsLWCg)\n",
    "\n",
    "This is the idea of Rezende & all in [Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked autoencoder and masked autoregressive flow for density estimation\n",
    "\n",
    "Mostly refers to [MADE Masked Autoencoder for Distribution Estimation](https://arxiv.org/abs/1502.03509) see author interview [here](https://www.youtube.com/watch?v=315xKcYX-1w) and [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057)\n",
    "\n",
    "The idea here is to take the likelihood of an output sequence as a product of conditional probability of each element upon its predecessor:\n",
    "\n",
    "\\begin{align*}\n",
    "  p_{\\theta}(x_0, x_1, \\dots x_{n-1}) &= \\prod_{i=0}^{n-1} p_{\\theta}(x_i|x_{0:i})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For each element in the sequence, one can model its distribution as\n",
    "\\begin{align*}\n",
    "  x_i = \\mu_i + z_i \\times \\sigma_i\\\\\n",
    "\\end{align*}\n",
    "with\n",
    "\\begin{align*}\n",
    "  z_i \\mathtt{\\sim} \\mathcal{N}(0,1) \\quad \\text{and} \\quad\\mu_i = f_{\\mu}(x_{0:i}) \\quad \\text{and} \\quad \\sigma_i = f_{\\sigma}(x_{0:i})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "See also inverse regressive flows see [improved variational inference with inverse autoregressive flow](https://arxiv.org/abs/1606.04934) and [\n",
    "Parallel WaveNet: Fast High-Fidelity Speech Synthesis](https://arxiv.org/abs/1711.10433)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows: main references\n",
    "Here are a few very important references we might refere to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic introduction with a great youtube video:\n",
    "https://www.youtube.com/watch?v=i7LjDvsLWCg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the amazing work of Mahdi Karami. The document is a PhD thesis dedicated to the study of tools for variational inference, normalizing flows, analysis and design of operator with tractable inverse jacobian computation, with additional numerical and convex optimization tools. It also contains applications to latent space representations of complex distributions met in signal/image processing, and much more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets move on to a very interesting review paper: Normalizing Flows: An Introduction and Review of Current Methods by Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1908.09257.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce4a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1908.09257.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reference paper, that exposes a lot of important properties of Normalizing flows, and design of methods to leverage them in machine learning / representation learning: Normalizing Flows for Probabilistic Modeling and Inference by George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1912.02762.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1912.02762.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a paper more targeted to application: Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows by Marco Rudolph, Bastian Wandt and Bodo Rosenhahn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/2008.12577.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/2008.12577.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
