{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0382423f326a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#mpl.rc('text', usetex = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Generic imports\n",
    "import functools\n",
    "import warnings\n",
    "\n",
    "#Numerical tools\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "# Deep learning stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Probabilistic stuff\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows for Generative model in Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is somehow a follow up the InformationTheoryOptimization notebook. Some basic concepts exposed there will be used here. But we will give a short reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes theorem\n",
    "\n",
    "Although Bayesian statistics is not really a novel approach, the recent rise of probabilistic programming libraries, availability of a lot of data, and powerful computer to run markov chains, made the tedious task of running bayesian inference algorithms a lot easier.\n",
    "\n",
    "Let's recall some simple elements about the Bayes theorem that we have been in other notebooks:\n",
    "\n",
    "For two given random variables, $X$ and $\\theta$, we can write :\n",
    "\\begin{equation}\n",
    "    P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We could use a slightly more formal version, that includes a model $\\mathcal{M}$ that we believe is the underlying model for the random variable $x$, parametrized by $\\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\theta|x,\\mathcal{M}) = \\frac{P(x|\\theta, \\mathcal{M})P(\\theta|\\mathcal{M})}{P(x|\\mathcal{M})}\n",
    "\\end{equation}\n",
    "\n",
    "* Usually, $\\theta$ is the random variable that we would like to caracterize. It usually consist in a set of parameters for a given statistical model (mean and variance for a normal distribution for instance).\n",
    "\n",
    "* On the contrary $x$, called the evidence, is usually derived from a known dataset, or from sampling a real life process. We will see shortly how we can define an empirical distribution from a set of samples. \n",
    "\n",
    "* We call $P(x|\\theta)$ the likelihood of $x$, given the model distribution  $\\theta$. This is the likelihood (or its logarithm) we tried to maximize with various instances of expectation maximization algorithm in the InformationTheory notebook by finding the optimal model parameters $\\theta$.\n",
    "Maximum likelihood had some success in the past for some instance of problems where likelihood or its logarithm were concave and differentiable, and a gradient based methods allowed to find a global maximum.\n",
    "\n",
    "* We call $P(\\theta|x)$ the posterior distribution of $\\theta$, given a known (usually empirical distribution of data) $x$. As opposite to the prior, it gives an idea of the probability of the model AFTER some data ($x$) has been seen.\n",
    "\n",
    "* We call $P(\\theta)$ the a-priori distribution for the variable $\\theta$. This one can be derived if we have a-priori knowledge on the model parameters $\\theta$, it may consist in apriori knowledge of some surrogate parameters that we integrate as a marginal distribution and scale. A prior usually allows us to compute the probability of a given set of parameters $\\theta \\in \\mathbb{R}^n$ that feed the model $\\mathcal{M}$.\n",
    "\n",
    "A priori usually writes:\n",
    "  \\begin{align*}\n",
    "    y &\\rightarrow P(\\theta=y) \\\\\n",
    "    \\mathbb{R}^n &\\mapsto [0,1]\n",
    "  \\end{align*}\n",
    "  \n",
    "A concrete example is for instance, the use of the framework of random markov field (or Gibbs random field) in a n-dimensional space like an image, were we can consider the pixels as a set of vertices of a graph, and only neighbouring pixels are connected by edges. The probability of a given graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is given by $P(\\mathcal{G}) = \\alpha e^{-\\beta U(\\mathcal{G})}$ where $\\alpha$ and $\\beta$ are normalization factor, and $U(\\mathcal{G}) = \\sum_{c\\in C} V_c(\\mathcal(G))$ is a sum of clique potentials $V_c(\\mathcal(G))$ over all possible cliques $C$. A very common instance of Gibbs measure is the gaussian-like distribution where, we have, for each neighbouring pair of pixel $p_1-p_2$ (a clique): $V_{p_1-p_2}(\\mathcal{G}) = (\\mathcal{G}_{p_1}-\\mathcal{G}_{p_2})^2$\n",
    "When one has no apriori on $\\theta$, then the highest entropy hypothesis is implicitly used. More pragmatically, we write $P(\\theta=y) \\sim Uniform(\\mathrm{supp} \\theta)$ where $\\mathrm{supp} \\theta$ is the support of $\\theta$\n",
    "Notice that, given a discrete process, as long as we received new data, the prior distribution can be taken just as being the previous step posterior.\n",
    "\n",
    "* Usually $P(x)$ is the empirical distribution that correspond to a given set of outcomes (actual data). It represents the probability of the given outcome of the experiment regardless of the value of the underlying model $\\theta$. We can see $P(x)$ as a marginalization of $\\theta$ in the joint distribution $P(x,\\theta)=P(x|\\theta)P(\\theta)$ i.e. $P(x) = \\int_{\\theta} P(x,\\theta)$ such that it acts as a constant used to normalize likelihood  / prior product to make sure the result posterior is a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference and the sampling task\n",
    "\n",
    "As briefly seen in the InformationTheory notebook, in the general case, to compute the posterior over the whole support of $\\theta$ is an extremely complex problem, especially if $\\theta$ is a vector from a high dimension space, and the actual distribution of the posterior is very complex (not smooth, many local density maxima, etc...). MCMC types of methods often comes into play here, where one computes $P(\\theta=y_0|x)$ for a given $y_0$, then jumps to another point $y_1$ in space $\\theta$ , with a given probably, dependant on $P(\\theta=y|x)$.\n",
    "Little by little one can get a discrete approximation of the posterior, but it can be very slow.\n",
    "\n",
    "For instance, as part of tensorflow probability you get the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs one step of the Metropolis-Hastings algorithm.\n",
      "\n",
      "  The [Metropolis-Hastings algorithm](\n",
      "  https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) is a\n",
      "  Markov chain Monte Carlo (MCMC) technique which uses a proposal distribution\n",
      "  to eventually sample from a target distribution.\n",
      "\n",
      "  Note: `inner_kernel.one_step` must return `kernel_results` as a\n",
      "  `collections.namedtuple` which must:\n",
      "\n",
      "  - have a `target_log_prob` field,\n",
      "  - optionally have a `log_acceptance_correction` field, and,\n",
      "  - have only fields which are `Tensor`-valued.\n",
      "\n",
      "  The Metropolis-Hastings log acceptance-probability is computed as:\n",
      "\n",
      "  ```python\n",
      "  log_accept_ratio = (current_kernel_results.target_log_prob\n",
      "                      - previous_kernel_results.target_log_prob\n",
      "                      + current_kernel_results.log_acceptance_correction)\n",
      "  ```\n",
      "\n",
      "  If `current_kernel_results.log_acceptance_correction` does not exist, it is\n",
      "  presumed `0.` (i.e., that the proposal distribution is symmetric).\n",
      "\n",
      "  The most common use-case for `log_acceptance_correction` is in the\n",
      "  Metropolis-Hastings algorithm, i.e.,\n",
      "\n",
      "  ```none\n",
      "  accept_prob(x' | x) = p(x') / p(x) (g(x|x') / g(x'|x))\n",
      "\n",
      "  where,\n",
      "    p  represents the target distribution,\n",
      "    g  represents the proposal (conditional) distribution,\n",
      "    x' is the proposed state, and,\n",
      "    x  is current state\n",
      "  ```\n",
      "\n",
      "  The log of the parenthetical term is the `log_acceptance_correction`.\n",
      "\n",
      "  The `log_acceptance_correction` may not necessarily correspond to the ratio of\n",
      "  proposal distributions, e.g, `log_acceptance_correction` has a different\n",
      "  interpretation in Hamiltonian Monte Carlo.\n",
      "\n",
      "  #### Examples\n",
      "\n",
      "  ```python\n",
      "  import tensorflow_probability as tfp\n",
      "  hmc = tfp.mcmc.MetropolisHastings(\n",
      "      tfp.mcmc.UncalibratedHamiltonianMonteCarlo(\n",
      "          target_log_prob_fn=lambda x: -x - x**2,\n",
      "          step_size=0.1,\n",
      "          num_leapfrog_steps=3))\n",
      "  # ==> functionally equivalent to:\n",
      "  # hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
      "  #     target_log_prob_fn=lambda x: -x - x**2,\n",
      "  #     step_size=0.1,\n",
      "  #     num_leapfrog_steps=3)\n",
      "  ```\n",
      "\n",
      "  \n",
      "=================================\n",
      "Runs one step of the RWM algorithm with symmetric proposal.\n",
      "\n",
      "  Random Walk Metropolis is a gradient-free Markov chain Monte Carlo\n",
      "  (MCMC) algorithm. The algorithm involves a proposal generating step\n",
      "  `proposal_state = current_state + perturb` by a random\n",
      "  perturbation, followed by Metropolis-Hastings accept/reject step. For more\n",
      "  details see [Section 2.1 of Roberts and Rosenthal (2004)](\n",
      "  http://emis.ams.org/journals/PS/images/getdoc510c.pdf?id=35&article=15&mode=pdf).\n",
      "\n",
      "  Current class implements RWM for normal and uniform proposals. Alternatively,\n",
      "  the user can supply any custom proposal generating function.\n",
      "\n",
      "  The function `one_step` can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics\n",
      "  are governed by `target_log_prob_fn(*current_state)`. (The number of\n",
      "  independent chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "    num_results=1000,\n",
      "    current_state=dtype(1),\n",
      "    kernel=tfp.mcmc.RandomWalkMetropolis(target.log_prob),\n",
      "    num_burnin_steps=500,\n",
      "    trace_fn=None,\n",
      "    seed=42)\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.math.reduce_mean(\n",
      "          tf.math.squared_difference(samples, sample_mean),\n",
      "          axis=0))\n",
      "\n",
      "  print('Estimated mean: {}'.format(sample_mean))\n",
      "  print('Estimated standard deviation: {}'.format(sample_std))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from a 2-D Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "  true_mean = dtype([0, 0])\n",
      "  true_cov = dtype([[1, 0.5],\n",
      "                    [0.5, 1]])\n",
      "  num_results = 500\n",
      "  num_chains = 100\n",
      "\n",
      "  # Target distribution is defined through the Cholesky decomposition `L`:\n",
      "  L = tf.linalg.cholesky(true_cov)\n",
      "  target = tfd.MultivariateNormalTriL(loc=true_mean, scale_tril=L)\n",
      "\n",
      "  # Initial state of the chain\n",
      "  init_state = np.ones([num_chains, 2], dtype=dtype)\n",
      "\n",
      "  # Run Random Walk Metropolis with normal proposal for `num_results`\n",
      "  # iterations for `num_chains` independent chains:\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "      num_results=num_results,\n",
      "      current_state=init_state,\n",
      "      kernel=tfp.mcmc.RandomWalkMetropolis(target_log_prob_fn=target.log_prob),\n",
      "      num_burnin_steps=200,\n",
      "      num_steps_between_results=1,  # Thinning.\n",
      "      trace_fn=None,\n",
      "      seed=54)\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  x = tf.squeeze(samples - sample_mean)\n",
      "  sample_cov = tf.matmul(tf.transpose(x, [1, 2, 0]),\n",
      "                         tf.transpose(x, [1, 0, 2])) / num_results\n",
      "\n",
      "  mean_sample_mean = tf.math.reduce_mean(sample_mean)\n",
      "  mean_sample_cov = tf.math.reduce_mean(sample_cov, axis=0)\n",
      "  x = tf.reshape(sample_cov - mean_sample_cov, [num_chains, 2 * 2])\n",
      "  cov_sample_cov = tf.reshape(tf.matmul(x, x, transpose_a=True) / num_chains,\n",
      "                              shape=[2 * 2, 2 * 2])\n",
      "\n",
      "  print('Estimated mean: {}'.format(mean_sample_mean))\n",
      "  print('Estimated avg covariance: {}'.format(mean_sample_cov))\n",
      "  print('Estimated covariance of covariance: {}'.format(cov_sample_cov))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution using Cauchy proposal.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "  num_burnin_steps = 500\n",
      "  num_chain_results = 1000\n",
      "\n",
      "  def cauchy_new_state_fn(scale, dtype):\n",
      "    cauchy = tfd.Cauchy(loc=dtype(0), scale=dtype(scale))\n",
      "    def _fn(state_parts, seed):\n",
      "      next_state_parts = []\n",
      "      part_seeds = tfp.random.split_seed(\n",
      "          seed, n=len(state_parts), salt='rwmcauchy')\n",
      "      for sp, ps in zip(state_parts, part_seeds):\n",
      "        next_state_parts.append(sp + cauchy.sample(\n",
      "          sample_shape=sp.shape, seed=ps))\n",
      "      return next_state_parts\n",
      "    return _fn\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "      num_results=num_chain_results,\n",
      "      num_burnin_steps=num_burnin_steps,\n",
      "      current_state=dtype(1),\n",
      "      kernel=tfp.mcmc.RandomWalkMetropolis(\n",
      "          target.log_prob,\n",
      "          new_state_fn=cauchy_new_state_fn(scale=0.5, dtype=dtype)),\n",
      "      trace_fn=None,\n",
      "      seed=42)\n",
      "\n",
      "  sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.math.reduce_mean(\n",
      "          tf.math.squared_difference(samples, sample_mean),\n",
      "          axis=0))\n",
      "\n",
      "  print('Estimated mean: {}'.format(sample_mean))\n",
      "  print('Estimated standard deviation: {}'.format(sample_std))\n",
      "  ```\n",
      "\n",
      "  \n",
      "=================================\n",
      "Runs one step of Metropolis-adjusted Langevin algorithm.\n",
      "\n",
      "  Metropolis-adjusted Langevin algorithm (MALA) is a Markov chain Monte Carlo\n",
      "  (MCMC) algorithm that takes a step of a discretised Langevin diffusion as a\n",
      "  proposal. This class implements one step of MALA using Euler-Maruyama method\n",
      "  for a given `current_state` and diagonal preconditioning `volatility` matrix.\n",
      "  Mathematical details and derivations can be found in\n",
      "  [Roberts and Rosenthal (1998)][1] and [Xifara et al. (2013)][2].\n",
      "\n",
      "  See `UncalibratedLangevin` class description below for details on the proposal\n",
      "  generating step of the algorithm.\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should reduce log-probabilities across\n",
      "  all event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics are\n",
      "  governed by `target_log_prob_fn(*current_state)`. (The number of independent\n",
      "  chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Simple chain with warm-up.\n",
      "\n",
      "  In this example we sample from a standard univariate normal\n",
      "  distribution using MALA with `step_size` equal to 0.75.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "  import matplotlib.pyplot as plt\n",
      "\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "  dtype = np.float32\n",
      "\n",
      "  # Target distribution is Standard Univariate Normal\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  def target_log_prob(x):\n",
      "    return target.log_prob(x)\n",
      "\n",
      "  # Define MALA sampler with `step_size` equal to 0.75\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "      num_results=1000,\n",
      "      current_state=dtype(1),\n",
      "      kernel=tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
      "          target_log_prob_fn=target_log_prob,\n",
      "          step_size=0.75),\n",
      "      num_burnin_steps=500,\n",
      "      trace_fn=None,\n",
      "      seed=42)\n",
      "\n",
      "  sample_mean = tf.reduce_mean(samples, axis=0)\n",
      "  sample_std = tf.sqrt(\n",
      "      tf.reduce_mean(\n",
      "          tf.math.squared_difference(samples, sample_mean),\n",
      "          axis=0))\n",
      "\n",
      "  print('sample mean', sample_mean)\n",
      "  print('sample standard deviation', sample_std)\n",
      "\n",
      "  plt.title('Traceplot')\n",
      "  plt.plot(samples.numpy(), 'b')\n",
      "  plt.xlabel('Iteration')\n",
      "  plt.ylabel('Position')\n",
      "  plt.show()\n",
      "  ```\n",
      "\n",
      "  ##### Sample from a 3-D Multivariate Normal distribution.\n",
      "\n",
      "  In this example we also consider a non-constant volatility function.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow.compat.v2 as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "\n",
      "  tf.enable_v2_behavior()\n",
      "\n",
      "  dtype = np.float32\n",
      "  true_mean = dtype([0, 0, 0])\n",
      "  true_cov = dtype([[1, 0.25, 0.25], [0.25, 1, 0.25], [0.25, 0.25, 1]])\n",
      "  num_results = 500\n",
      "  num_chains = 500\n",
      "\n",
      "  # Target distribution is defined through the Cholesky decomposition\n",
      "  chol = tf.linalg.cholesky(true_cov)\n",
      "  target = tfd.MultivariateNormalTriL(loc=true_mean, scale_tril=chol)\n",
      "\n",
      "  # Here we define the volatility function to be non-constant\n",
      "  def volatility_fn(x):\n",
      "    # Stack the input tensors together\n",
      "    return 1. / (0.5 + 0.1 * tf.math.abs(x))\n",
      "\n",
      "  # Initial state of the chain\n",
      "  init_state = np.ones([num_chains, 3], dtype=dtype)\n",
      "\n",
      "  # Run MALA with normal proposal for `num_results` iterations for\n",
      "  # `num_chains` independent chains:\n",
      "  states = tfp.mcmc.sample_chain(\n",
      "      num_results=num_results,\n",
      "      current_state=init_state,\n",
      "      kernel=tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
      "          target_log_prob_fn=target.log_prob,\n",
      "          step_size=.1,\n",
      "          volatility_fn=volatility_fn),\n",
      "      num_burnin_steps=200,\n",
      "      num_steps_between_results=1,\n",
      "      trace_fn=None,\n",
      "      seed=42)\n",
      "\n",
      "  sample_mean = tf.reduce_mean(states, axis=[0, 1])\n",
      "  x = (states - sample_mean)[..., tf.newaxis]\n",
      "  sample_cov = tf.reduce_mean(\n",
      "      tf.matmul(x, tf.transpose(x, [0, 1, 3, 2])), [0, 1])\n",
      "\n",
      "  print('sample mean', sample_mean.numpy())\n",
      "  print('sample covariance matrix', sample_cov.numpy())\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Gareth Roberts and Jeffrey Rosenthal. Optimal Scaling of Discrete\n",
      "       Approximations to Langevin Diffusions. _Journal of the Royal Statistical\n",
      "       Society: Series B (Statistical Methodology)_, 60: 255-268, 1998.\n",
      "       https://doi.org/10.1111/1467-9868.00123\n",
      "\n",
      "  [2]: T. Xifara et al. Langevin diffusions and the Metropolis-adjusted\n",
      "       Langevin algorithm. _arXiv preprint arXiv:1309.2983_, 2013.\n",
      "       https://arxiv.org/abs/1309.2983\n",
      "  \n",
      "=================================\n",
      "Runs one step of the No U-Turn Sampler.\n",
      "\n",
      "  The No U-Turn Sampler (NUTS) is an adaptive variant of the Hamiltonian Monte\n",
      "  Carlo (HMC) method for MCMC. NUTS adapts the distance traveled in response to\n",
      "  the curvature of the target density. Conceptually, one proposal consists of\n",
      "  reversibly evolving a trajectory through the sample space, continuing until\n",
      "  that trajectory turns back on itself (hence the name, 'No U-Turn'). This class\n",
      "  implements one random NUTS step from a given `current_state`.\n",
      "  Mathematical details and derivations can be found in\n",
      "  [Hoffman, Gelman (2011)][1] and [Betancourt (2018)][2].\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that a prefix of leftmost dimensions of `current_state` index independent\n",
      "  chain states (and are therefore updated independently).  The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions.  Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0][0, ...]` could have a\n",
      "  different target distribution from `current_state[0][1, ...]`.  These\n",
      "  semantics are governed by `target_log_prob_fn(*current_state)`. (The number of\n",
      "  independent chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Matthew D. Hoffman, Andrew Gelman.  The No-U-Turn Sampler: Adaptively\n",
      "  Setting Path Lengths in Hamiltonian Monte Carlo.  2011.\n",
      "  https://arxiv.org/pdf/1111.4246.pdf.\n",
      "\n",
      "  [2]: Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo.\n",
      "  _arXiv preprint arXiv:1701.02434_, 2018. https://arxiv.org/abs/1701.02434\n",
      "  \n",
      "=================================\n",
      "Runs one step of the Replica Exchange Monte Carlo.\n",
      "\n",
      "  [Replica Exchange Monte Carlo](\n",
      "  https://en.wikipedia.org/wiki/Parallel_tempering) is a Markov chain\n",
      "  Monte Carlo (MCMC) algorithm that is also known as Parallel Tempering. This\n",
      "  algorithm takes multiple samples (from tempered distributions) in parallel,\n",
      "  then swaps these samples according to the Metropolis-Hastings criterion.\n",
      "  See also the review paper [1].\n",
      "\n",
      "  The `K` replicas are parameterized in terms of `inverse_temperature`'s,\n",
      "  `(beta[0], beta[1], ..., beta[K-1])`. If the user provides\n",
      "  `target_log_prob_fn`, then the `kth` replica samples from density `p_k(x)`,\n",
      "  with `log(p_k(x)) = beta_k * target_log_prob(x)`.\n",
      "  In this case, geometrically decaying `beta` often works well.  That is, with\n",
      "  `R < 1`, we recommend trying `beta[k] = R^k` so that\n",
      "  `1.0 = beta[0] > beta[1] > ... > 0`.  See [2].\n",
      "\n",
      "  The user can also provide two functions, `tempered_log_prob_fn` and\n",
      "  `untempered_log_prob_fn`. In this case, the `kth` replica samples from\n",
      "  density `p_k(x)` with\n",
      "  `log(p_k(x)) = beta_k * tempered_log_prob_fn(x) + untempered_log_prob_fn(x)`.\n",
      "  The this case, `beta` may be zero, and one often sets `beta[-1]` to zero.\n",
      "  This means the last replica samples using `untempered_log_prob_fn`.\n",
      "  In the Bayesian setup, `untempered_log_prob_fn` will often be the log prior,\n",
      "  and `tempered_log_prob_fn` the likelihood.\n",
      "\n",
      "  In all cases,\n",
      "\n",
      "  * `beta[0] == 1` ==> First replica samples from the target density.\n",
      "  * `beta[k] < 1`, for `k = 1, ..., K-1` ==> Other replicas sample from\n",
      "    \"tempered\" versions of target (peak is less high, valley less low).  These\n",
      "    distributions should allow easier exploration of separated modes.\n",
      "\n",
      "  By default, samples from adjacent replicas `i`, `i + 1` are used as proposals\n",
      "  for each other in a Metropolis step.  This allows the lower `beta` samples,\n",
      "  which explore less dense areas of `p`, to eventually swap state with the\n",
      "  `beta == 1` chain, allowing it to explore these new regions.\n",
      "\n",
      "  Samples from replica 0 are returned, and the others are discarded, unless\n",
      "  `state_includes_replicas`.\n",
      "\n",
      "  #### Examples\n",
      "\n",
      "  ##### Sampling from the Standard Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "\n",
      "  target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
      "\n",
      "  # Geometric decay is a good rule of thumb.\n",
      "  inverse_temperatures = 0.5**tf.range(4, dtype=dtype)\n",
      "\n",
      "  # If everything was Normal, step_size should be ~ sqrt(temperature).\n",
      "  step_size = 1.5 / tf.sqrt(inverse_temperatures)\n",
      "\n",
      "  def make_kernel_fn(target_log_prob_fn):\n",
      "    return tfp.mcmc.HamiltonianMonteCarlo(\n",
      "        target_log_prob_fn=target_log_prob_fn,\n",
      "        step_size=step_size, num_leapfrog_steps=3)\n",
      "\n",
      "  remc = tfp.mcmc.ReplicaExchangeMC(\n",
      "      target_log_prob_fn=target.log_prob,\n",
      "      inverse_temperatures=inverse_temperatures,\n",
      "      make_kernel_fn=make_kernel_fn)\n",
      "\n",
      "  def trace_swaps(unused_state, results):\n",
      "    return (results.is_swap_proposed_adjacent,\n",
      "            results.is_swap_accepted_adjacent)\n",
      "\n",
      "  samples, (is_swap_proposed_adjacent, is_swap_accepted_adjacent) = (\n",
      "      tfp.mcmc.sample_chain(\n",
      "          num_results=1000,\n",
      "          current_state=1.0,\n",
      "          kernel=remc,\n",
      "          num_burnin_steps=500,\n",
      "          trace_fn=trace_swaps)\n",
      "  )\n",
      "\n",
      "  # conditional_swap_prob[k] = P[ExchangeAccepted | ExchangeProposed],\n",
      "  # for the swap between replicas k and k+1.\n",
      "  conditional_swap_prob = (\n",
      "      tf.reduce_sum(tf.cast(is_swap_accepted_adjacent, tf.float32), axis=0)\n",
      "      /\n",
      "      tf.reduce_sum(tf.cast(is_swap_proposed_adjacent, tf.float32), axis=0))\n",
      "  ```\n",
      "\n",
      "  ##### Sampling from a 2-D Mixture Normal Distribution.\n",
      "\n",
      "  ```python\n",
      "  import numpy as np\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import matplotlib.pyplot as plt\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  dtype = np.float32\n",
      "\n",
      "  target = tfd.MixtureSameFamily(\n",
      "      mixture_distribution=tfd.Categorical(probs=[0.5, 0.5]),\n",
      "      components_distribution=tfd.MultivariateNormalDiag(\n",
      "          loc=[[-1., -1], [1., 1.]],\n",
      "          scale_identity_multiplier=[0.1, 0.1]))\n",
      "\n",
      "  inverse_temperatures = 0.2**tf.range(4, dtype=dtype)\n",
      "\n",
      "  # step_size must broadcast with all batch and event dimensions of target.\n",
      "  # Here, this means it must broadcast with:\n",
      "  #  [len(inverse_temperatures)] + target.event_shape\n",
      "  step_size = 0.075 / tf.reshape(tf.sqrt(inverse_temperatures), shape=(4, 1))\n",
      "\n",
      "  def make_kernel_fn(target_log_prob_fn):\n",
      "    return tfp.mcmc.HamiltonianMonteCarlo(\n",
      "        target_log_prob_fn=target_log_prob_fn,\n",
      "        step_size=step_size, num_leapfrog_steps=3)\n",
      "\n",
      "  remc = tfp.mcmc.ReplicaExchangeMC(\n",
      "      target_log_prob_fn=target.log_prob,\n",
      "      inverse_temperatures=inverse_temperatures,\n",
      "      make_kernel_fn=make_kernel_fn)\n",
      "\n",
      "  samples = tfp.mcmc.sample_chain(\n",
      "      num_results=1000,\n",
      "      # Start near the [1, 1] mode. Standard HMC would get stuck there.\n",
      "      current_state=tf.ones(2, dtype=dtype),\n",
      "      kernel=remc,\n",
      "      trace_fn=None,\n",
      "      num_burnin_steps=500)\n",
      "\n",
      "  plt.figure(figsize=(8, 8))\n",
      "  plt.xlim(-2, 2)\n",
      "  plt.ylim(-2, 2)\n",
      "  plt.plot(samples[:, 0], samples[:, 1], '.')\n",
      "  plt.show()\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: David J. Earl, Michael W. Deem\n",
      "       Parallel Tempering: Theory, Applications, and New Perspectives\n",
      "       https://arxiv.org/abs/physics/0508111\n",
      "  [2]: David A. Kofke\n",
      "       On the acceptance probability of replica-exchange Monte Carlo trials.\n",
      "       J. of Chem. Phys. Vol. 117 No. 5.\n",
      "  \n",
      "=================================\n",
      "Runs one step of Hamiltonian Monte Carlo.\n",
      "\n",
      "  Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm\n",
      "  that takes a series of gradient-informed steps to produce a Metropolis\n",
      "  proposal. This class implements one random HMC step from a given\n",
      "  `current_state`. Mathematical details and derivations can be found in\n",
      "  [Neal (2011)][1].\n",
      "\n",
      "  The `one_step` function can update multiple chains in parallel. It assumes\n",
      "  that all leftmost dimensions of `current_state` index independent chain states\n",
      "  (and are therefore updated independently). The output of\n",
      "  `target_log_prob_fn(*current_state)` should sum log-probabilities across all\n",
      "  event dimensions. Slices along the rightmost dimensions may have different\n",
      "  target distributions; for example, `current_state[0, :]` could have a\n",
      "  different target distribution from `current_state[1, :]`. These semantics are\n",
      "  governed by `target_log_prob_fn(*current_state)`. (The number of independent\n",
      "  chains is `tf.size(target_log_prob_fn(*current_state))`.)\n",
      "\n",
      "  #### Examples:\n",
      "\n",
      "  ##### Simple chain with warm-up.\n",
      "\n",
      "  In this example we sample from a standard univariate normal\n",
      "  distribution using HMC with adaptive step size.\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "\n",
      "  tf.enable_eager_execution()\n",
      "\n",
      "  # Target distribution is proportional to: `exp(-x (1 + x))`.\n",
      "  def unnormalized_log_prob(x):\n",
      "    return -x - x**2.\n",
      "\n",
      "  # Initialize the HMC transition kernel.\n",
      "  num_results = int(10e3)\n",
      "  num_burnin_steps = int(1e3)\n",
      "  adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
      "      tfp.mcmc.HamiltonianMonteCarlo(\n",
      "          target_log_prob_fn=unnormalized_log_prob,\n",
      "          num_leapfrog_steps=3,\n",
      "          step_size=1.),\n",
      "      num_adaptation_steps=int(num_burnin_steps * 0.8))\n",
      "\n",
      "  # Run the chain (with burn-in).\n",
      "  @tf.function\n",
      "  def run_chain():\n",
      "    # Run the chain (with burn-in).\n",
      "    samples, is_accepted = tfp.mcmc.sample_chain(\n",
      "        num_results=num_results,\n",
      "        num_burnin_steps=num_burnin_steps,\n",
      "        current_state=1.,\n",
      "        kernel=adaptive_hmc,\n",
      "        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)\n",
      "\n",
      "    sample_mean = tf.reduce_mean(samples)\n",
      "    sample_stddev = tf.math.reduce_std(samples)\n",
      "    is_accepted = tf.reduce_mean(tf.cast(is_accepted, dtype=tf.float32))\n",
      "    return sample_mean, sample_stddev, is_accepted\n",
      "\n",
      "  sample_mean, sample_stddev, is_accepted = run_chain()\n",
      "\n",
      "  print('mean:{:.4f}  stddev:{:.4f}  acceptance:{:.4f}'.format(\n",
      "      sample_mean.numpy(), sample_stddev.numpy(), is_accepted.numpy()))\n",
      "  ```\n",
      "\n",
      "  ##### Estimate parameters of a more complicated posterior.\n",
      "\n",
      "  In this example, we'll use Monte-Carlo EM to find best-fit parameters. See\n",
      "  [_Convergence of a stochastic approximation version of the EM algorithm_][2]\n",
      "  for more details.\n",
      "\n",
      "  More precisely, we use HMC to form a chain conditioned on parameter `sigma`\n",
      "  and training data `{ (x[i], y[i]) : i=1...n }`. Then we use one gradient step\n",
      "  of maximum-likelihood to improve the `sigma` estimate. Then repeat the process\n",
      "  until convergence. (This procedure is a [Robbins--Monro algorithm](\n",
      "  https://en.wikipedia.org/wiki/Stochastic_approximation).)\n",
      "\n",
      "  The generative assumptions are:\n",
      "\n",
      "  ```none\n",
      "    W ~ MVN(loc=0, scale=sigma * eye(dims))\n",
      "    for i=1...num_samples:\n",
      "        X[i] ~ MVN(loc=0, scale=eye(dims))\n",
      "      eps[i] ~ Normal(loc=0, scale=1)\n",
      "        Y[i] = X[i].T * W + eps[i]\n",
      "  ```\n",
      "\n",
      "  We now implement a stochastic approximation of Expectation Maximization (SAEM)\n",
      "  using `tensorflow_probability` intrinsics. [Bernard (1999)][2]\n",
      "\n",
      "  ```python\n",
      "  import tensorflow as tf\n",
      "  import tensorflow_probability as tfp\n",
      "  import numpy as np\n",
      "\n",
      "  tf.enable_eager_execution()\n",
      "\n",
      "  tfd = tfp.distributions\n",
      "\n",
      "  def make_training_data(num_samples, dims, sigma):\n",
      "    dt = np.asarray(sigma).dtype\n",
      "    x = np.random.randn(dims, num_samples).astype(dt)\n",
      "    w = sigma * np.random.randn(1, dims).astype(dt)\n",
      "    noise = np.random.randn(num_samples).astype(dt)\n",
      "    y = w.dot(x) + noise\n",
      "    return y[0], x, w[0]\n",
      "\n",
      "  def make_weights_prior(dims, log_sigma):\n",
      "    return tfd.MultivariateNormalDiag(\n",
      "        loc=tf.zeros([dims], dtype=log_sigma.dtype),\n",
      "        scale_identity_multiplier=tf.math.exp(log_sigma))\n",
      "\n",
      "  def make_response_likelihood(w, x):\n",
      "    if w.shape.ndims == 1:\n",
      "      y_bar = tf.matmul(w[tf.newaxis], x)[0]\n",
      "    else:\n",
      "      y_bar = tf.matmul(w, x)\n",
      "    return tfd.Normal(loc=y_bar, scale=tf.ones_like(y_bar))  # [n]\n",
      "\n",
      "  # Setup assumptions.\n",
      "  dtype = np.float32\n",
      "  num_samples = 500\n",
      "  dims = 10\n",
      "  tf.random.set_seed(10014)\n",
      "  np.random.seed(10014)\n",
      "\n",
      "  weights_prior_true_scale = np.array(0.3, dtype)\n",
      "  y, x, _ = make_training_data(\n",
      "      num_samples, dims, weights_prior_true_scale)\n",
      "\n",
      "  log_sigma = tf.Variable(0., dtype=dtype, name='log_sigma')\n",
      "\n",
      "  optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
      "\n",
      "  @tf.function\n",
      "  def mcem_iter(weights_chain_start, step_size):\n",
      "    with tf.GradientTape() as tape:\n",
      "      tape.watch(log_sigma)\n",
      "      prior = make_weights_prior(dims, log_sigma)\n",
      "\n",
      "      def unnormalized_posterior_log_prob(w):\n",
      "        likelihood = make_response_likelihood(w, x)\n",
      "        return (\n",
      "            prior.log_prob(w) +\n",
      "            tf.reduce_sum(likelihood.log_prob(y), axis=-1))  # [m]\n",
      "\n",
      "      def trace_fn(_, pkr):\n",
      "        return (\n",
      "            pkr.inner_results.log_accept_ratio,\n",
      "            pkr.inner_results.accepted_results.target_log_prob,\n",
      "            pkr.inner_results.accepted_results.step_size)\n",
      "\n",
      "      num_results = 2\n",
      "      weights, (\n",
      "          log_accept_ratio, target_log_prob, step_size) = tfp.mcmc.sample_chain(\n",
      "          num_results=num_results,\n",
      "          num_burnin_steps=0,\n",
      "          current_state=weights_chain_start,\n",
      "          kernel=tfp.mcmc.SimpleStepSizeAdaptation(\n",
      "              tfp.mcmc.HamiltonianMonteCarlo(\n",
      "                  target_log_prob_fn=unnormalized_posterior_log_prob,\n",
      "                  num_leapfrog_steps=2,\n",
      "                  step_size=step_size,\n",
      "                  state_gradients_are_stopped=True,\n",
      "              ),\n",
      "              # Adapt for the entirety of the trajectory.\n",
      "              num_adaptation_steps=2),\n",
      "          trace_fn=trace_fn,\n",
      "          seed=123)\n",
      "\n",
      "      # We do an optimization step to propagate `log_sigma` after two HMC\n",
      "      # steps to propagate `weights`.\n",
      "      loss = -tf.reduce_mean(target_log_prob)\n",
      "\n",
      "    avg_acceptance_ratio = tf.math.exp(\n",
      "        tfp.math.reduce_logmeanexp(tf.minimum(log_accept_ratio, 0.)))\n",
      "\n",
      "    optimizer.apply_gradients(\n",
      "        [[tape.gradient(loss, log_sigma), log_sigma]])\n",
      "\n",
      "    weights_prior_estimated_scale = tf.math.exp(log_sigma)\n",
      "    return (weights_prior_estimated_scale, weights[-1], loss,\n",
      "            step_size[-1], avg_acceptance_ratio)\n",
      "\n",
      "  num_iters = int(40)\n",
      "\n",
      "  weights_prior_estimated_scale_ = np.zeros(num_iters, dtype)\n",
      "  weights_ = np.zeros([num_iters + 1, dims], dtype)\n",
      "  loss_ = np.zeros([num_iters], dtype)\n",
      "  weights_[0] = np.random.randn(dims).astype(dtype)\n",
      "  step_size_ = 0.03\n",
      "\n",
      "  for iter_ in range(num_iters):\n",
      "    [\n",
      "        weights_prior_estimated_scale_[iter_],\n",
      "        weights_[iter_ + 1],\n",
      "        loss_[iter_],\n",
      "        step_size_,\n",
      "        avg_acceptance_ratio_,\n",
      "    ] = mcem_iter(weights_[iter_], step_size_)\n",
      "    tf.compat.v1.logging.vlog(\n",
      "        1, ('iter:{:>2}  loss:{: 9.3f}  scale:{:.3f}  '\n",
      "            'step_size:{:.4f}  avg_acceptance_ratio:{:.4f}').format(\n",
      "                iter_, loss_[iter_], weights_prior_estimated_scale_[iter_],\n",
      "                step_size_, avg_acceptance_ratio_))\n",
      "\n",
      "  # Should converge to ~0.22.\n",
      "  import matplotlib.pyplot as plt\n",
      "  plt.plot(weights_prior_estimated_scale_)\n",
      "  plt.ylabel('weights_prior_estimated_scale')\n",
      "  plt.xlabel('iteration')\n",
      "  ```\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Radford Neal. MCMC Using Hamiltonian Dynamics. _Handbook of Markov Chain\n",
      "       Monte Carlo_, 2011. https://arxiv.org/abs/1206.1901\n",
      "\n",
      "  [2]: Bernard Delyon, Marc Lavielle, Eric, Moulines. _Convergence of a\n",
      "       stochastic approximation version of the EM algorithm_, Ann. Statist. 27\n",
      "       (1999), no. 1, 94--128. https://projecteuclid.org/euclid.aos/1018031103\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(tfp.mcmc.MetropolisHastings.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.RandomWalkMetropolis.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.MetropolisAdjustedLangevinAlgorithm.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.NoUTurnSampler.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.ReplicaExchangeMC.__doc__)\n",
    "print(\"=================================\")\n",
    "print(tfp.mcmc.HamiltonianMonteCarlo.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic inference for latent variable models parametrized by $\\theta$\n",
    "\n",
    "I found this [blog post](https://mbernste.github.io/posts/elbo/) introduction very well written, as it sets bayesian framework in the context of a model where parameters $\\theta$ are distincts from latent states $z$. This type of models is very common, and the formulation of posterior changes quite a lot from what we have seen in the InformationTheory notebook, as the model parameters are not the one we want a distribution for anymore.\n",
    "\n",
    "Here is an exerpt of the above link:\n",
    "\n",
    "Before digging in, let’s review the probabilistic inference task for a latent variable model. In a latent variable model, we posit that our observed data $x$ is a realization from some random variable $X$. Moreoever, we posit the existence of another random variable $Z$ where $X$ and $Z$ are distributed according to a joint distribution $p(X,Z;\\theta)$ where $\\theta$ parameterizes the distribution.Unfortunately, our data is only a realization of $X$, not $Z$, and therefore $Z$ remains unobserved (i.e. latent).\n",
    "\n",
    "There are two predominant tasks that we may be interested in accomplishing:\n",
    "\n",
    "* Given some prior distribution over $Z$, $P(Z)$, compute the posterior distribution $p(Z∣X;\\theta)$\n",
    "* Compute the maximum likelihood estimate of $\\theta$:\n",
    "\\begin{align*}\n",
    "  \\underset{\\theta}{argmax} \\quad l(\\theta)\n",
    "\\end{align*}\n",
    "where $l(\\theta)$ is the log-likelihood function:\n",
    "\\begin{align*}\n",
    "  l(\\theta) = log(p(x;\\theta)) = log \\left( \\int p(x,z;\\theta) dz \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Variational inference is used for Task 1 and expectation-maximization is used for Task 2. Both of these algorithms rely on the ELBO, a concept we will explain hereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO for latent variable models\n",
    "\n",
    "To understand the evidence lower bound, we must first understand what we mean by “evidence”. The evidence, quite a simply, is just a name given to the likelihood function evaluated at a fixed $\\theta$:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{evidence} \\; = \\; log(p(x;\\theta))\n",
    "\\end{align*}\n",
    "\n",
    "Why is this quantity called the “evidence” ? Intuitively, if we have chosen the right model $p$ and $\\theta$, then we would expect that the marginal probability of our observed data $x$, would be high. Thus, a higher value of $log(p(x;\\theta))$ indicates, in some sense, that we may be on the right track with the model $p$ and parameters $\\theta$ that we have chosen. That is, this quantity is “evidence” that we have chosen the right model for the data.\n",
    "\n",
    "If we happen to also know (or posit) that $Z$ follows some distribution denoted by $q$ (and that $p(x,z;\\theta) = p(x|z;\\theta)q(z)$), then the evidence lower bound is, well, just a lower bound on the evidence that makes use of the known (or posited) $q$. Specifically,\n",
    "\n",
    "\\begin{align*}\n",
    "  log(p(x;\\theta)) \\geq \\mathbb{E}_q \\left[ \\frac{p(x,z;\\theta)}{q(z)}  \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where the ELBO is simply the right-hand side of the above equation. We will show how this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler divergence, also called the relative entropy or discrimination information, can be computed between two distributions, say $x$ and $y$ and writes $ D_{KL}(x\\|y) $.\n",
    "This metric represents the expectation of the difference of the number of bits needed to encode a symbol from the distribution x, weither the encoding is optimal for the distribution x or y.\n",
    "\n",
    "The general case reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\mathbb{E}_x \\left[ log \\left( \\frac{x}{y} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "In the discrete case it reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\sum_{i\\in I} p(x_i) log(p(x_i)) -\\sum_{i\\in I} p(x_i) log(p(y_i)) \\\\\n",
    "                 &= -H(x) + H_y(x)\n",
    "\\end{align}\n",
    "or equivalently\n",
    "$$\n",
    "    D_{KL}(x\\|y) = \\sum_{i\\in I} p(x_i) log\\left(\\frac{p(x_i)}{p(y_i)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### Reminder on KL Divergence and maximum likelihood\n",
    "This paragraph was inspired by [this article](http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/)\n",
    "by Liangjie Hong\n",
    "\n",
    "#### Definitions\n",
    "Let's consider $P(x_i|\\theta)$ a distribution used to generate (or sample) $N$ points $x_i, i=0,\\dots,N-1$\n",
    "\n",
    "We can define a general distribution model as:\n",
    "$$\n",
    "    P_\\theta(x) = P(x|\\theta) = P(x;\\theta)\n",
    "$$\n",
    "And, to handle data from real world, we will define the following empirical distribution (also called probability mass function):\n",
    "$$\n",
    "    P_D(x) = \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i)\n",
    "$$\n",
    "\n",
    "#### KL Divergence between model and data\n",
    "We can now express the Kullback Leibler divergence between the model distribution and the empirical one:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) &= \\int P_D(x) log(P_D(x)) dx -\\int P_D(x) log(P_\\theta(x)) dx \\\\\n",
    "    &= H(P_D(x)) - \\int P_D(x) log(P_\\theta(x)) dx\n",
    "\\end{align}\n",
    "\n",
    "As $H(P_D(x))$ is a constant term, relatd to the empirical distribution of real data, we will call it $\\epsilon$, and we may get rid of it later.\n",
    "Instead we will mainly consider, the $\\theta$, ie, the model related term:\n",
    "$$\n",
    "   D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) = \\epsilon -\\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2}\n",
    "$$\n",
    "\n",
    "Thanks to the definition of the empirical distribution, we can express this continuous expression as a discrete summation:\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2} &= \\int \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i) log(P(x|\\theta)) dx \\\\\n",
    "    &= \\sum_{i=0}^{N-1} \\frac{1}{N} log(P(x_i|\\theta)) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{i=0}^{N-1} log(P(x_i|\\theta))\n",
    "\\end{align}\n",
    "\n",
    "The last line amounts to the log-likelihood of the dataset. We can then conclude that maximizing the log-likelihood of a dataset, relatively to a set of distribution parameters $\\theta$ and a given dataset $x$ is equivalent to minimizing the Kullback Leibler divergence between the empirical distribution and the model distribution.\n",
    "\n",
    "### Practical KL Divergence between two data sets\n",
    "We are interested in computing the Kullback Leibler divergence between a model $X$ and a real dataset $Y$, indexed by $i$ where $i$ is the index of a specific event whose occurence probability is $X_i$ according to the model, and $Y_i$ in practice.\n",
    "\n",
    "As in the definition of the empirical distribution, our datasets must verify:\n",
    "$$\n",
    "    \\sum_{i=0}^{N-1} X_i = 1\n",
    "$$\n",
    "\n",
    "then the expression of the kl divergence is simply:\n",
    "$$\n",
    "    D_{KL}(Y\\|X) = \\sum_{i=0}^{N-1} log \\left( \\frac{Y_i}{X_i} \\right) Y_i\n",
    "$$\n",
    "\n",
    "Moreover there is an important condition that must be respected: \n",
    "$$\n",
    "    X_i = 0 \\implies Y_i = 0\n",
    "$$\n",
    "If so, if the event $k$ has no occurence, we have $X_k=Y_k=0$ and $log \\left( \\frac{y_k}{x_k} \\right) y_k$ has a limit in $0$ which is $0$. Otherwise, Kullback Leibler cannot be computed.\n",
    "\n",
    "The explanation is pretty straightforward: if an event, or a symbol from $Y$ has no occurence in $X$, it has no reason to be encoded using $X$ probability distribution, then the average number of bits needed to encode a symbol from $Y$ is a nonsense, as some symbols cannot be represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is variational inference ?\n",
    "\n",
    "### Variational distribution for the posterior\n",
    "\n",
    "We are still in the framework of bayes theorem for latent variable models, and we want to compute $p$ our posterior that stands for $p(z|x)$.\n",
    "\n",
    "As evoked earlier, in variational inference, you create a variational distribution $q$ over your model latent variables $z$, and eventually express the posterior as a function of the joint distribution $p(x,z)=p(x|z)q(z)$.\n",
    "\n",
    "In a genera context, a variational distribution is parametrized by a variational parameter, we will call $\\nu$:\n",
    "\n",
    "\\begin{align*}\n",
    "  q(z|\\nu) \\quad \\text{or} \\quad q(z;\\nu)\n",
    "\\end{align*}\n",
    "\n",
    "The first idea of variational inference, is that we will try to find $\\nu$ such that $q(z;\\nu)$ becomes close to our posterior distribution $p(z|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-divergence for variational/posterior closeness\n",
    "\n",
    "The closeness of those distribution will be computed using Kullback-Liebler divergence:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(z)}{p(z|x)} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "As already mentionned earlier, it is good to remember that KL-divergence is not symmetric, in terms of semantic, one must keep in mind that the measure is related to the expectation over $q$ distribution. So basically, rare event with respect to $q$ won't result in a large divergence, even if their occurence is high in $p$ (you will use few bits to represent rare events, that's not a big deal). However, if high probability events from $q$ are rare in the posterior, (hence coded with high number of bits) you will need to code this event (or symbol) many time, with many bits, and the divergence with respect to an optimal coding will be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving evidence lower bound (ELBO)\n",
    "\n",
    "Let's recall, that, for concave functions, over a domain $d$ we can use Jensen's inequality that writes:\n",
    "\\begin{align*}\n",
    "  f\\left(\\mathbb{E}_d \\left[ x \\right] \\right) \\geq \\mathbb{E}_d \\left[f\\left( x \\right)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Let's now rewrite the log probability ouf our evidence, as latent variable marginalization, and introduce our variational distribution $q(z)$ with a small trick:\n",
    "\\begin{align*}\n",
    "  log(p(x)) &= log \\left(\\int_{z} p(x,z) dz \\right) \\\\\n",
    "  &= log \\left(\\int_{z} p(x,z) \\frac{q(z)}{q(z)} dz \\right) \\\\\n",
    "  &= log \\left(\\mathbb{E}_q \\left[ \\frac{p(x,z)}{q(z)} \\right] \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "As $log$ is a nice concave function, we can give a lower bound of the evidence in the previous expression:\n",
    "\\begin{align*}\n",
    "  log(p(x)) &= log \\left(\\mathbb{E}_q \\left[ \\frac{p(x,z)}{q(z)} \\right] \\right) \\\\\n",
    "  log(p(x)) &\\geq \\mathbb{E}_q \\left[ log \\left(\\frac{p(x,z)}{q(z)} \\right) \\right] \\\\\n",
    "  log(p(x)) \\geq \\text{ELBO} &= \\mathbb{E}_q \\left[ log \\left(p(x,z)\\right) -log\\left( q(z) \\right) \\right] \\\\\n",
    "  \\text{ELBO} &= \\underbrace{ \\mathbb{E}_q \\left[log \\left(p(x,z)\\right)\\right]}_{\\text{Expectation under} \\; q \\; \\text{of the joint sample, latent variable probability}} - \n",
    "  \\underbrace{\\mathbb{E}_q \\left[log \\left( q(z) \\right) \\right]}_{\\text{Entropy of our variational distribution }q} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Ok what is the actual interest of this lower bound expression ? We need to understand how this later expression is linked with the KL divergence between $p$ and $q$, that reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(z)}{p(z|x)} \\right) \\right] \\\\\n",
    "    &= \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(z|x) \\right) \\right] \\\\\n",
    "        &= \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(z,x) \\right) \\right] + \\mathbb{E}_q \\left[ log \\left( p(x)\\right) \\right] \\\\\n",
    "    &= log \\left( p(x)\\right) - \\left( \\underbrace{\\mathbb{E}_q \\left[ log \\left( p(x,z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right]}_{\\text{= ELBO}}\\right)\\\\\n",
    "\\end{align}\n",
    "\n",
    "The expectation in the last term $log \\left( p(x)\\right)$ can be ignored in our context, as there is no dependance to $q$ or $\\nu$, it has somehow already been marginalized away.\n",
    "Another way to put it is that we can see this evidence probability as a constant, hence can be discarded for our purpose, which is, finding an optimal surrogate $q(z; \\nu)$ to approximate our posterior $p(z|x)$.\n",
    "\n",
    "#### ELBO and variational inference: conclusion\n",
    "\n",
    "* We have shown here that finding the variational distribution $q$ parametrized by $\\nu$ that maximizes the ELBO amounts to find a model for the latent variable that provides high probability for the actual samples $x$.\n",
    "* However, ELBO is still a surrogate, as its name indicates, maximizing ELBO does not provides any guarantee with respect to the actual posterior optimality, unless you can show that the KL divergence between your variational distribution $q(z)$ and the posterior $p(z|x)$ is equal to 0 (Hence the distributions are equal).\n",
    "* We have shown that the gap between posterior (log posterior actually) and ELBO is actually the KL divergence between the variational distribution $q(z)$ and the posterior $p(z|x)$.\n",
    "* Equivalent of last point can be formulated as: we have shown that maximizing the posterior with respect to $z$ is equivalent to minimizing the KL divergence between the variational distribution $q(z)$ and the posterior $p(z|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of variational inference: Mean field variational inference\n",
    "\n",
    "There are plenty of families of distributions that have been studied in the framework of variational inference, exponential families, neural networks, Gaussian processes, ...\n",
    "\n",
    "Let's assume that the variational distribution factorizes:\n",
    "\n",
    "\\begin{align}\n",
    "    q(z_0, z_1, \\dots , z_{m-1}) = \\prod_{j=0}^{m-1} q(z_j) \\\\\n",
    "\\end{align}\n",
    "\n",
    "That would of course only work if all dimensions are independants. But that's one easy solution to start with a simple variational inference workflow:\n",
    "\n",
    "1. Choose a model distribution $q(\\; \\cdot \\; ;\\nu)$ (normal for instance ?)\n",
    "2. Derive ELBO expression\n",
    "3. Perform coordinate ascent on each of the $z_i$\n",
    "4. Eventually repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link between latent variable models and generative paradigm\n",
    "\n",
    "### Probabilistic generative interpretation of PCA\n",
    "\n",
    "To serve as a basic introductory example to generative approach in machine learning/signal processing, we will use the famous PPCA example and see how it can be analyzed from a generative perspective.\n",
    "This example was first introduced by Tipping and Bishop in their 1999 article \"Probabilistic Principal Component Analysis\"\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be a random vector of observation, PCA can be defined as linear transformation of $x$ into a subspace $\\mathbb{R}^{d_0}$ as $z = U_{d_0}^T x$ where $U_{d_0}\\in \\mathbb{R}^{d\\times d_0}$ and $0<d_0\\leq d$ such that the columns from $U_{d_0}$ that stands for the subspaces selected for the projections capture a maximum of variance with respect to a given set of data.\n",
    "\n",
    "Let $\\left( \\mu, \\Sigma \\right)$ denote the mean and covariance matrices of $x$. As seen in the StatisticalTes notebook, we recall that the solution of PCA is a decorrelation matrix, and explain how it is obtained in next paragraph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall on PCA/ whitening transform\n",
    "Let $X$ be the matrix containing the centered samples (1 sample per column) in the simplified space.\n",
    "The covariance matrix $\\Sigma$ then writes $\\Sigma=XX^T$, then the withening matrix $W_W$ should verify,\n",
    "\n",
    "\\begin{align*}\n",
    "  W_W X (W_W X)^T &= W_W X X^T W_W^T \\\\\n",
    "  &= W_W \\Sigma W_W^T \\\\\n",
    "  &= I\n",
    "\\end{align*}\n",
    "\n",
    "where $W_ W \\Sigma W_W^T = I$ is the identity matrix.\n",
    "\n",
    "One can start by using the same approach as for PCA(=decorrelation), ie use use eigen decomposition of the form $\\Sigma = Q D Q^T$, where $D$ is a diagonal matrix.\n",
    "Or equivalently, thanks to positive definitiveness of $\\Sigma=XX^T$ and spectral theorem: $D = Q^T \\Sigma Q$. The next step simply consisting in composing with $D^{-\\frac{1}{2}}$ to obtain identity:\n",
    "\\begin{align*}\n",
    "  \\Sigma &= Q D Q^T \\\\\n",
    "  D &= Q^T \\Sigma Q \\qquad \\text{thanks to posisitve definitiveness and spectral theorem} \\\\\n",
    "  D^{-\\frac{T}{2}} D D^{-\\frac{1}{2}} &= D^{-\\frac{T}{2}} Q^T \\Sigma Q D^{-\\frac{1}{2}} \\\\\n",
    "  I &= D^{-\\frac{T}{2}} Q^T \\Sigma Q D^{-\\frac{1}{2}} \\\\\n",
    "  I &= U^T \\Sigma U\n",
    "\\end{align*}\n",
    "\n",
    "and simply identify $U = W_W^T = Q D^{-\\frac{1}{2}}$\n",
    "\n",
    "#### Choleski decomposition whitening\n",
    "Another method is to use a choleski decomposition of the form $L L^T = A$ where $L$ is a lower triangular matrix. The process of inverting a triangular matrix for a given vector is a very simple operation sometimes called forward substitution, as seen in the FixedPointIterationsMethods notebook in Gauss Seidel method.\n",
    "\n",
    "\\begin{align*}\n",
    "  L L^T &= C \\\\\n",
    "  L^T &= L^{-1}C \\\\\n",
    "  I &= L^{-1} C L^{-T}\\\\\n",
    "  I &= L^{-1} X X^T L^{-T} \\\\\n",
    "  I &= L^{-1} X (L^{-1} X)^{T} \\\\\n",
    "  W_W X (W_W X)^{T} &= I \\\\\n",
    "\\end{align*}\n",
    "\n",
    "And then simply identify $U^{T} = W_W = L^{-1}$\n",
    "\n",
    "#### Additional notes on whitening\n",
    "It might be noticed that if you find a $W_W$ such that the following holds:\n",
    "\\begin{align*}\n",
    "  W_W X (W_W X)^T &= W_W X X^T W_W^T \\\\\n",
    "  &= W_W C W_W^T \\\\\n",
    "  &= I\n",
    "\\end{align*}\n",
    "Then, the property will hold for any $W_W'$ such that $W_W' = A W_W$ with $A$ an arbitrary matrix with unitary property, ie $A^{*}A=I$\n",
    "There might then be multiple choice for the final whitening matrix, and one have some additional freedom to choose the whitenning transform such that it exhibit different sort of optimality, such as zca like transform for instance, that optimize for average cross-covariance or average cross-correlation between original and transformed data.\n",
    "\n",
    "More on this in [Optimal whitening and decorrelation](https://arxiv.org/abs/1512.00809)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent space interpretation of PCA/whitening transform\n",
    "By disentangling the relative linear dependancies (i.e correlation) in the centered dataset, $U^T$ allows to find a space where the best gaussian approximation of the transformed samples correspond to a simple multivariate gaussian with unit covariance. Moreover, if you only keep the $d_0$ eigen vectors associated with the largest eigenvalues, then you project onto a space in which the distribution approximation becomes even simpler (lower dimensional gaussian) while retaining most of the information (information is assumed to be measured in term of variance within this context).\n",
    "This reduced form can be written:\n",
    "\\begin{align*}\n",
    "  I_{d_0} = U_{d_0}^T \\Sigma U_{d_0}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic interpretation of linear mapping latent variable model\n",
    "Now, assume we define a probabilistic model based on the simple low dimensional, unit covariance multivariate gaussian, paired with a simple linear model that links latent space to observable space:\n",
    "\\begin{align*}\n",
    "  z \\mathtt{\\sim} \\mathcal{N} \\left( 0, I_{d_0}\\right), \\quad 0 < d_0 \\leq d \\\\\n",
    "  x = W z + \\mu_W + \\epsilon , \\quad W\\in \\mathbb{R}^{d\\times d_0}, \\mu_W \\in \\mathbb{R}^{d}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\epsilon$ is a placeholder for the noise/uncertainty realisation, which itself is modeled by\n",
    "\n",
    "\\begin{align*}\n",
    "  \\epsilon \\mathtt{\\sim} \\mathcal{N}(0, \\psi) \n",
    "\\end{align*}\n",
    "\n",
    "The distribution of observation would then reads\n",
    "\n",
    "\\begin{align*}\n",
    "  X \\mathtt{\\sim} \\mathcal{N} \\left(\\mu_W, WW^T+\\psi\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finding solutions $\\mu_W, W, \\psi$ based on maximum likelihood of samples $z$ might not be easy, as it can be difficult to decompose empirical estimation of data covariance into data model related term $WW^T$ and noise related term $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Principal Component Analysis (PPCA)\n",
    "\n",
    "In the PPCA paper, the author exposes a model where the noise model is constrained to be isotropic, ie this assumption translates into a model where residual variance $\\sigma^2=\\psi_i$ are constrained to be equal:\n",
    "\\begin{align*}\n",
    "  \\epsilon \\mathtt{\\sim} \\mathcal{N} \\left(0, \\sigma^2 I_d\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let's try to see how the likelihood for this model would be:\n",
    "\n",
    "\\begin{align*}\n",
    "  x|z \\mathtt{\\sim} \\mathcal{N} \\left(W z + \\mu_W, \\sigma^2 I_d \\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Of course, as mentionned earlier, access to realization of $z$ is impossible, as $z$ is a hidden variable, hence, we need to marginalize it out with its distribution assumption: $z \\mathtt{\\sim} \\mathcal{N} \\left( 0, I_{d_0}\\right)$\n",
    "\n",
    "That gives us:\n",
    "\n",
    "\\begin{align*}\n",
    "  x \\mathtt{\\sim} \\mathcal{N}\\left( \\mu_W, WW^T+ \\sigma^2 I_d \\right)\\\\\n",
    "  x \\mathtt{\\sim} \\mathcal{N}\\left( \\mu_W, C \\right)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of maximum likelhood estimate for the canonical multivariate gaussian model\n",
    "\n",
    "One can then express likelihood of a set of data given a multivariate gaussian model in dimension d, given the expression for one point (See EllipsoidEstimation notebook):\n",
    "\n",
    "\\begin{align*}\n",
    "    p(x|\\mu_W,C) = \\frac{1}{\\sqrt{(2\\pi)^d det(C)}} e^{-\\frac{1}{2} (x-\\mu_W)^{\\intercal}C^{-1}(x-\\mu_W)}\n",
    "\\end{align*}\n",
    "And then extend to multiple points: $x_0, x_1, x_{n-1}$:\n",
    "\\begin{align*}\n",
    "    p(x_0, x_1, x_{n-1}|\\mu_W,C) &= \\prod_{i=0}^{n-1}\n",
    "        \\frac{1}{\\sqrt{(2\\pi)^d det(C)}} e^{-\\frac{1}{2} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W)} \\\\\n",
    "    &= \\left(\\frac{1}{\\sqrt{(2\\pi)^d det(C)}}\\right)^n e^{-\\frac{1}{2}\n",
    "        \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W)} \\\\\n",
    "    &= \\left((2\\pi)^d det(C)\\right)^{-\\frac{n}{2}} e^{-\\frac{1}{2}\n",
    "        \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W)} \\\\\n",
    "\\end{align*}\n",
    "Whose log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "    log(p(x_0, x_1, x_{k-1}|\\mu_W,C)) &= -\\frac{nd}{2} log(2\\pi) -\\frac{n}{2} log(det(C)) -\\frac{1}{2}\n",
    "        \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The line vector, matrix, column vector product in the last term, can be rewritten in a much more compact form thanks to a clever trick:\n",
    "* The trace is invariant under cyclic permutations of matrix products: $\\mathrm{tr}\\left[ABC\\right] = \\mathrm{tr}\\left[CAB\\right] = \\mathrm{tr}\\left[BCA\\right]$\n",
    "* Since $x^TAx$ is scalar, we can take its trace and obtain the same value: $x^TAx = \\mathrm{tr}\\left[x^TAx\\right] = \\mathrm{tr}\\left[x^TxA\\right]$\n",
    "\n",
    "Hence $C^{-1}$ in the sum from the last term can be factored out:\n",
    "\n",
    "\\begin{align*}\n",
    "  & \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W) \\\\\n",
    "  =& \\sum_{i=0}^{n-1} tr\\left((x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W) \\right) \\\\\n",
    "  =& \\sum_{i=0}^{n-1} tr\\left((x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W)C^{-1} \\right)\\\\\n",
    "  =& \\sum_{i=0}^{n-1} tr\\left(C^{-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W) \\right)\\\\\n",
    "  =& tr\\left( \\sum_{i=0}^{n-1} C^{-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W) \\right)\\\\\n",
    "  =& tr\\left( C^{-1} \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W) \\right)\\\\\n",
    "  =& tr\\left(C^{-1} S'\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "With $S'=\\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W)$\n",
    "\n",
    "At the end we are given with the following expression for the log-likelihood:\n",
    "\\begin{align*}\n",
    "    log(p(x_0, x_1, x_{k-1}|\\mu_W,C)) &= -\\frac{n}{2}\\left( d log(2\\pi) + log(det(C)) + \\frac{1}{n} tr\\left(C^{-1}S\\right) \\right)  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "For the sake of readability, we are going to put the $\\frac{1}{n}$ inside of the $S'$ expression:\n",
    "With $S=\\frac{1}{n}\\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W)$\n",
    "we now get\n",
    "\\begin{align*}\n",
    "    log(p(x_0, x_1, x_{k-1}|\\mu_W,C)) &= -\\frac{n}{2}\\left( d log(2\\pi) - log(det(C^{-1})) + tr\\left(C^{-1}S\\right) \\right)  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "It is very interesting to notice that this problem appears to be very close to the expression of the convex conjugate of the minimum volume enclosing ellipsoid in the notebook EllipsoidEstimation (this is a pretty advanced convex optimization topic). Not sure exactly how the two relates however"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving maximum likelihood estimate for canonical multivariate gaussian parameters\n",
    "Now, in order to get maximum likelihood parameters $(\\hat{\\mu_W}, \\hat{C})$ in the generic case, we need to prove that the function is concave, our strategy to do so is to compute the terms of the gradients, and the Hessian. First show that Hessian is definite negative, which proves that log-likelihood is concave, and then find $(\\hat{\\mu_W}, \\hat{C})$ for which the gradient vanishes.\n",
    "\n",
    "##### Gradient in $\\mu_W$\n",
    "To take the derivative with respect to $\\mu_W$ and equate to zero we will make use of the following matrix calculus identity:\n",
    "\n",
    "> $\\mathbf{ \\frac{\\partial w^T A w}{\\partial w} = 2Aw}$ if $\\mathbf{w}$\n",
    "> does not depend on $\\mathbf{A}$ and $\\mathbf{A}$ is symmetric.\n",
    "\n",
    "We will make use of the former expression for $tr \\left( C^{-1} S \\right) = \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}C^{-1}(x_i-\\mu_W)$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\partial }{\\partial \\mu_W} log(p(x_i | \\mu_W, C)) & = -\\frac{n}{2} \\frac{1}{n} \\sum_{i=0}^{n-1} C^{-1} ( x_i - \\mu_W ) = 0 \\\\\n",
    "\t0 & = -\\frac{1}{2} \\sum_{i=0}^{n-1}2 C^{-1} ( x_i - \\mu_W )\\\\\n",
    "    0 & = C^{-1} \\sum_{i=0}^{n-1} (\\mu_W-x_i)\\\\\n",
    "\t0 & = \\sum_{i=0}^{n-1} (\\mu_W-x_i) \\; \\text{Since $C$ is positive definite, multiplying vector must be zero} \\\\\n",
    "\t0 & = n \\mu_W - \\sum_{i=0}^{n-1} x_i \\\\\n",
    "\t\\hat{\\mu_W} &= \\frac{1}{n} \\sum_{i=0}^{n-a} x_i = \\bar{x}\n",
    "\\end{align*}\n",
    "Which is often called the *sample mean* vector, or empirical mean, or in that case, the order-0 moment estimate.\n",
    "\n",
    "##### Gradient in $C$\n",
    "\n",
    "Deriving the MLE for the covariance matrix requires a bit more work and the use of the following linear algebra and calculus properties:\n",
    "\n",
    "> - $\\frac{\\partial}{\\partial A} \\mathrm{tr}\\left[AB\\right] = B^T$\n",
    "> - $\\frac{\\partial}{\\partial A} \\log det(A) = A^{-T}$\n",
    "\n",
    "We can now write\n",
    "\n",
    "\\begin{align*}\n",
    "   log(p(x_i | \\mu_W, C)) &= -\\frac{n}{2}\\left( d log(2\\pi) - log(det(C^{-1})) + tr\\left(C^{-1}S\\right) \\right) \\\\\n",
    "   \\frac{\\partial}{\\partial C^{-1}} log(p(x_i | \\mu_W, C)) & = \\frac{n}{2} \\frac{\\partial log(det(C^{-1}))}{\\partial C^{-1}} - \\frac{n}{2} \\frac{\\partial tr\\left(C^{-1}S\\right)}{\\partial C^{-1}} \\\\\n",
    "   &=  \\frac{n}{2} C^{T} - \\frac{n}{2} S^T\n",
    "\\end{align*}\n",
    "\n",
    "Equating to zero and solving for $C$ simply yields $C = S$ where $S=\\frac{1}{n}\\sum_{i=0}^{n-1} (x_i-\\mu_W)^{\\intercal}(x_i-\\mu_W)$ which is simply an alternative expression for the covariance matrix of the dataset, based on sum of outer products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form solution for the maximum likelihood in isotropic noise/affine model of PPCA\n",
    "\n",
    "Although we exposed a demonstration for the trivial case of MLE for multivariate gaussian, the author of PPCA give a closed form solution for the actual decomposition $C=WW^T+\\sigma^2 I$:\n",
    "\\begin{align*}\n",
    "  W_{ML} = U_{d_0} \\left( D_{d_0} - \\sigma^{2} I_{d_0} \\right)^{\\frac{1}{2}} R\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "* $U_{d_0} \\in \\mathbb{R}^{d}\\times\\mathbb{R}^{d_0}$ is the matrix of $d_0$ column vectors associated with the $d_0$ largest eigenvalues of $S$\n",
    "* $D_{d_0}$ is the diagonal matrix that contains the $d_0$ largest eigenvalues of $S$\n",
    "* $R$ is an arbitrary unitary matrix from $\\mathbb{R}^{d_0}\\times\\mathbb{R}^{d_0}$\n",
    "\n",
    "\\begin{align*}\n",
    "  {\\sigma_{ML}}^{2} = \\frac{1}{d-d_0} \\sum_{j=d_0}^{d-1} \\lambda_j\n",
    "\\end{align*}\n",
    "Which can be more or less understood as the average variance, lost in the subspace projection.\n",
    "\n",
    "In practice, the authors advise to first start to estimate ${\\sigma_{ML}}^{2}$, then update $W_{ML}$ with the provided definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "The author also show, that, there is an expression for what we call the posterior, ie:\n",
    "\\begin{align*}\n",
    "      z|x = \\mathcal{N}\\left( M^{-1} W^T (x-\\mu_W), \\sigma^2 M^{-1} \\right)\\\\\n",
    "\\end{align*}\n",
    "Where $M=WW^T+ \\sigma^2 I_d$\n",
    "\n",
    "Although the derivation is not completely trivial, it could anyway be obtained through numerical application with the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "      p(z|x) = \\frac{p(x|z)*p(z)}{p(x)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "* $p(z)=\\frac{1}{\\sqrt{(2\\pi)^{d_0}}}e^{-\\frac{1}{2}(z)^T(z)}$\n",
    "* $p(x|z)=\\frac{1}{\\sqrt{(2\\pi)^{d}\\sigma^{2d}}}e^{-\\frac{1}{2\\sigma^2}(x-Wz-\\mu)^T((x-Wz-\\mu))}$\n",
    "* $p(x)=\\frac{1}{\\sqrt{(2\\pi)^{d}det(M)}}e^{-\\frac{1}{2}(x-Wz-\\mu)^TC^{-1}((x-Wz-\\mu))}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPCA in practice with tfd\n",
    "def probabilistic_pca(data_dim, latent_dim, num_datapoints, stddv_datapoints):\n",
    "  # x=Wz+\\mu+/epsilon I guess this is just initialization then\n",
    "  w = yield tfd.Normal(loc=tf.zeros([data_dim, latent_dim]),\n",
    "                 scale=2.0 * tf.ones([data_dim, latent_dim]),\n",
    "                 name=\"w\")\n",
    "  # p(z)+N(0,1)\n",
    "  z = yield tfd.Normal(loc=tf.zeros([latent_dim, num_datapoints]),\n",
    "                 scale=tf.ones([latent_dim, num_datapoints]),\n",
    "                 name=\"z\")\n",
    "  # x=Wz+\\mu+/epsilon, with \\epsilon=N(0,\\sigma^2 Id)\n",
    "  x = yield tfd.Normal(loc=tf.matmul(w, z),\n",
    "                       scale=stddv_datapoints,\n",
    "                       name=\"x\")\n",
    "\n",
    "num_datapoints = 5000\n",
    "data_dim = 2\n",
    "latent_dim = 1\n",
    "stddv_datapoints = 0.5\n",
    "\n",
    "concrete_ppca_model = functools.partial(probabilistic_pca,\n",
    "    data_dim=data_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_datapoints=num_datapoints,\n",
    "    stddv_datapoints=stddv_datapoints)\n",
    "\n",
    "model = tfd.JointDistributionCoroutineAutoBatched(concrete_ppca_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example of variational inference with VAE\n",
    "\n",
    "The example here comes from the excellent book from Aurelien Geron \"Hands on Machine learning with scikit learn and tensorflow\", for which there is a nice set of python tutorials there: https://github.com/ageron/handson-ml2\n",
    "\n",
    "The book cites an (interesting tutorial by Carl Doersch)[https://arxiv.org/abs/1606.05908]\n",
    "\n",
    "Let's give a bit more informations about this example to replace it within the context of variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the rationale behind latent loss\n",
    "\n",
    "Lets consider our posterior $p(z|x)$ which is the probability of latent vector $z$ given a sample image $x$. The simple variational encoder designed here is designed to provide an expression for this posterior:\n",
    "\n",
    "\\begin{align*}\n",
    "  p(z|x) &= \\mathtt{\\sim} \\mathcal{N} (\\mu_x, \\sigma_{x}^{2}) \\\\\n",
    "  &= \\mathtt{\\sim} \\mathcal{N} (f_{\\mu}(x), f_{\\sigma^{2}}(x) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $f_{\\mu}(x)$ and $f_{\\sigma^{2}}$ are just few layers of dense nn with selu activation and a last affine layer (dense without activation).\n",
    "\n",
    "Ok, now, we must recall that $f_{\\mu}(x)$ and $f_{\\sigma^{2}}$ are useful if they can learn commonalities between many samples drawn from $X$. So that, for each $x$, they can generate a distribution, with the following properties:\n",
    "* 1: simple enough so that we can easily sample from it (this actually a built in constraint given by the choice of the normal distribution)\n",
    "* 2: have a high density for areas in z that correspond to vectors that can be used by the decoder to reconstruct real samples\n",
    "\n",
    "Lets quickly recall the expression of Kullback Leibler between posterior and desired variational distribution:\n",
    "\\begin{align*}\n",
    "    D_{KL}(q\\|p) &= \\mathbb{E}_q \\left[ log \\left( \\frac{q(z)}{p(z|x)} \\right) \\right] \\\\\n",
    "    &= \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(z|x) \\right) \\right] \\\\\n",
    "        &= \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( p(z,x) \\right) \\right] + \\mathbb{E}_q \\left[ log \\left( p(x)\\right) \\right] \\\\\n",
    "    &= -\\left( \\underbrace{\\mathbb{E}_q \\left[ log \\left( p(x,z) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( q(z) \\right) \\right]}_{\\text{= ELBO}}\\right) + log \\left( p(x)\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now let see how the ELBO translate for two univariate normal distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "  ELBO &= \\mathbb{E}_q \\left[ log \\left( p(x) \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( q(x) \\right) \\right] \\\\\n",
    "  &= \\mathbb{E}_q \\left[ log \\left( \\frac{1}{\\sigma_p\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_p}{\\sigma_p }\\right)^{2}} \\right) \\right] - \\mathbb{E}_q \\left[ log \\left( \\frac{1}{\\sigma_q\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_q}{\\sigma_q}\\right)^{2}} \\right) \\right]\\\\\n",
    "  &= \\mathbb{E}_q \\left[ -log(\\sigma_p) -log(\\sqrt{2\\pi}) -\\frac{1}{2} \\left(\\frac{x-\\mu_p}{\\sigma_p }\\right)^{2} \\right] - \\mathbb{E}_q \\left[-log(\\sigma_q) -log(\\sqrt{2\\pi}) -\\frac{1}{2} \\left(\\frac{x-\\mu_q}{\\sigma_q }\\right)^{2}\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Proper distributions like normal always integrates to one, so we can factor out constants:\n",
    "\n",
    "\\begin{align*}\n",
    "  ELBO &= -log(\\sigma_p)-log(\\sqrt{2\\pi}) + log(\\sigma_q) + log(\\sqrt{2\\pi})\n",
    "          -\\frac{1}{2} \\mathbb{E}_q\\left[ \\left(\\frac{x-\\mu_p}{\\sigma_p }\\right)^{2} \\right]\n",
    "          +\\frac{1}{2} \\mathbb{E}_q \\left[ \\left(\\frac{x-\\mu_q}{\\sigma_q }\\right)^{2}\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the last expression, it can be easily spotted that $\\mathbb{E}_q \\left[ \\left(\\frac{x-\\mu_q}{\\sigma_q }\\right)^{2}\\right] = 1$ as it is a well known property of normal distributions.\n",
    "\n",
    "Now we have\n",
    "\n",
    "\\begin{align*}\n",
    "  ELBO &= log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{1}{2} -\n",
    "          \\frac{1}{2\\sigma_p^2} \\mathbb{E}_q \\left[(x-\\mu_p)^{2} \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "  (x-\\mu_p)^{2} &= (x-\\mu_q+\\mu_q-\\mu_p)^{2} \\\\\n",
    "  &= (x-\\mu_q)^{2} +(\\mu_q-\\mu_p)^{2} + 2 (x-\\mu_q)(\\mu_q-\\mu_p) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can plug that expression back in the expectation over $q$ in the original ELBO derivation:\n",
    "\n",
    "\\begin{align*}\n",
    "ELBO &= log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{1}{2} -\n",
    "        \\frac{1}{2\\sigma_p^2} \\mathbb{E}_q \\left[(x-\\mu_p)^{2} \\right] \\\\\n",
    "     &= \\frac{\\sigma_q}{\\sigma_p} + \\frac{1}{2} -\n",
    "        \\frac{1}{2\\sigma_p^2} \\mathbb{E}_q \\left[(x-\\mu_q)^{2}\n",
    "                                                 +(\\mu_q-\\mu_p)^{2}\n",
    "                                                 + 2 (x-\\mu_q)(\\mu_q-\\mu_p) \\right] \\\\\n",
    "     &= log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{1}{2} -\n",
    "        \\frac{1}{2\\sigma_p^2} \\left( \\mathbb{E}_q \\left[(x-\\mu_q)^{2} \\right]+\n",
    "                                     \\mathbb{E}_q \\left[(\\mu_q-\\mu_p)^{2} \\right]+\n",
    "                                     2(\\mu_q-\\mu_p) \\mathbb{E}_q \\left[ (x-\\mu_q)\\right] \\right) \\\\\n",
    "     &= log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{1}{2} -\n",
    "        \\frac{1}{2\\sigma_p^2} \\left( \\sigma_q^2 +\n",
    "                                     (\\mu_q-\\mu_p)^{2} \\right) \\\\\n",
    "     &= log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{1}{2} -\n",
    "        \\frac{\\sigma_q^2+(\\mu_q-\\mu_p)^{2}}{2\\sigma_p^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The term $2(\\mu_q-\\mu_p) \\mathbb{E}_q \\left[(x-\\mu_q)\\right]$ simply vanishes because of symmetry of the normal distribution around $\\mu_q$\n",
    "And we used the normal property to compute $\\mathbb{E}_q \\left[(x-\\mu_q)^{2}\\right]=\\sigma_q^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned earlier $log \\left( p(x)\\right)$ is constant across the learning process, so there is no need to take it into account for the upcoming optimization steps. \n",
    "\n",
    "Basically, what we want is to minimize the KL divergence between  between\n",
    "\\begin{align*}\n",
    "  -\\text{ELBO} &= -\\frac{1}{2} \\left(\n",
    "                    1\n",
    "                    + log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right)\n",
    "                    - \\frac{\\sigma_q^2+(\\mu_q-\\mu_p)^{2}}{\\sigma_p^2} \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "As, by design, this VAE choose to \n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{L} &= - \\frac{1}{2} \\sum_{i=0}^{K-1} 1 + log(\\sigma_{i}^{2}) - \\sigma_{i}^{2} - \\mu_{i}^{2} \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few utility functions\n",
    "K = keras.backend\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean\n",
    "    \n",
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "\n",
    "# We will be working with fashion mnist dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 5s 9ms/step - loss: 0.4437 - rounded_accuracy: 0.8155 - val_loss: 0.3526 - val_rounded_accuracy: 0.8906\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3462 - rounded_accuracy: 0.8953 - val_loss: 0.3412 - val_rounded_accuracy: 0.9036\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3343 - rounded_accuracy: 0.9039 - val_loss: 0.3335 - val_rounded_accuracy: 0.9052\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3291 - rounded_accuracy: 0.9082 - val_loss: 0.3318 - val_rounded_accuracy: 0.9101\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3252 - rounded_accuracy: 0.9116 - val_loss: 0.3254 - val_rounded_accuracy: 0.9132\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3241 - rounded_accuracy: 0.9131 - val_loss: 0.3250 - val_rounded_accuracy: 0.9100\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3217 - rounded_accuracy: 0.9146 - val_loss: 0.3217 - val_rounded_accuracy: 0.9144\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3194 - rounded_accuracy: 0.9161 - val_loss: 0.3240 - val_rounded_accuracy: 0.9157\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3195 - rounded_accuracy: 0.9165 - val_loss: 0.3232 - val_rounded_accuracy: 0.9103\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3179 - rounded_accuracy: 0.9175 - val_loss: 0.3199 - val_rounded_accuracy: 0.9162\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3178 - rounded_accuracy: 0.9177 - val_loss: 0.3179 - val_rounded_accuracy: 0.9204\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3164 - rounded_accuracy: 0.9190 - val_loss: 0.3177 - val_rounded_accuracy: 0.9208\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3157 - rounded_accuracy: 0.9191 - val_loss: 0.3176 - val_rounded_accuracy: 0.9202\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3148 - rounded_accuracy: 0.9200 - val_loss: 0.3160 - val_rounded_accuracy: 0.9187\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3147 - rounded_accuracy: 0.9198 - val_loss: 0.3206 - val_rounded_accuracy: 0.9164\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3152 - rounded_accuracy: 0.9198 - val_loss: 0.3150 - val_rounded_accuracy: 0.9210\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3145 - rounded_accuracy: 0.9205 - val_loss: 0.3180 - val_rounded_accuracy: 0.9199\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3137 - rounded_accuracy: 0.9208 - val_loss: 0.3182 - val_rounded_accuracy: 0.9157\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3136 - rounded_accuracy: 0.9211 - val_loss: 0.3153 - val_rounded_accuracy: 0.9219\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3127 - rounded_accuracy: 0.9212 - val_loss: 0.3147 - val_rounded_accuracy: 0.9220\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3133 - rounded_accuracy: 0.9214 - val_loss: 0.3163 - val_rounded_accuracy: 0.9219\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3126 - rounded_accuracy: 0.9217 - val_loss: 0.3137 - val_rounded_accuracy: 0.9210\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3122 - rounded_accuracy: 0.9213 - val_loss: 0.3167 - val_rounded_accuracy: 0.9208\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3127 - rounded_accuracy: 0.9220 - val_loss: 0.3134 - val_rounded_accuracy: 0.9232\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.3114 - rounded_accuracy: 0.9221 - val_loss: 0.3156 - val_rounded_accuracy: 0.9216\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dimensionality of the latent space\n",
    "codings_size = 10\n",
    "\n",
    "inputs = keras.layers.Input(shape=[28, 28])\n",
    "z = keras.layers.Flatten()(inputs)\n",
    "z = keras.layers.Dense(150, activation=\"selu\")(z)\n",
    "z = keras.layers.Dense(100, activation=\"selu\")(z)\n",
    "\n",
    "# Last layer to compute f_{\\mu}, and f_{log(\\sigma^2)}\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "\n",
    "# Sampling operator: N(0,1) * exp(log(\\sigma^2)/2) + \\mu give the actual output of the encoder\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = keras.models.Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "# Now define the decoder, this one is not \"variational\". There is no concept of randomness here\n",
    "# Some implementations of GANs, like StyleGans\n",
    "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "x = keras.layers.Dense(100, activation=\"selu\")(decoder_inputs)\n",
    "x = keras.layers.Dense(150, activation=\"selu\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "\n",
    "# Define placeholders for overall model\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# Latent loss is where the variational aspect takes place.\n",
    "latent_loss = -0.5 * K.sum(\n",
    "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "    axis=-1)\n",
    "variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
    "\n",
    "# Main loss is usual cross entropy (image values are between 0 and 1)\n",
    "# we could have used mse for reconstruction to test\n",
    "variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[rounded_accuracy])\n",
    "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
    "                             validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "\n",
    "show_reconstructions(variational_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE and dimensionality reduction\n",
    "\n",
    "Lets take a look at how to perform dimensionality reduction with VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE()\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n",
    "X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())\n",
    "\n",
    "# adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap)\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(X_valid_2D):\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from VAEs\n",
    "\n",
    "Lets take a look at how to perform sampling with VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to show grid of images\n",
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a few random codings, decode them and plot the resulting images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "codings = tf.random.normal(shape=[12, codings_size])\n",
    "images = variational_decoder(codings).numpy()\n",
    "plot_multiple_images(images, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming distributions with bijectors\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The fundamental equation we need here, is known for a very long time, and relates to change of variable, initially used to integrate quantities in physics.\n",
    "Given an invertible function:$f:\\underset{Z}{z} \\mapsto \\underset{X}{x}$ and a distribution $p_{\\theta}(z), \\; z \\in Z$, the change of variable forumla reads\n",
    "\n",
    "\\begin{align*}\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f^{-1}(x) \\right) |det \\left( J_{f^{-1}} \\right)| \\\\\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f^{-1}(x) \\right) \\frac{1}{|det \\left( J_{f}\\right)|} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $|det \\left( J_{f}\\right)|$ is the absolute value of the determinant of the Jacobian of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick recall, what is a Jacobian ?\n",
    "\n",
    "We first recall that, for a function $f:\\underset{\\mathbb{R}^n}{x} \\mapsto \\underset{\\mathbb{R}^n}{f(x)}$, we can derive the matrix of partial derivative, called the jacobian matrix:\n",
    "\n",
    "\\begin{align*}\n",
    "  J=\\begin{pmatrix}\n",
    "    \\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\dots & \\frac{\\partial f_0}{\\partial x_{n-1}}\\\\\n",
    "    \\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_{n-1}}\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  \\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} & \\dots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule for transformation composition\n",
    "\n",
    "As seen in the DifferentiatingPerceptron notebook, for complex functions, that are applied each after another, like in most deep learning models (excepted here we are only interested in bijective layers), the chain rule applies, and we have, for an initial distribution:\n",
    "\\begin{align*}\n",
    "  p_{\\theta}(z), \\; z \\in Z\n",
    "\\end{align*}\n",
    "And a chain of bijective transforms:\n",
    "\\begin{align*}\n",
    "  x = f_{\\theta}(z) = f_{K-1} \\circ f_{K-2} \\circ \\dots \\circ f_{1} \\circ f_{0}(z)\n",
    "\\end{align*}\n",
    "The following holds\n",
    "\\begin{align*}\n",
    "  p_{\\theta}'(x) &= p_{\\theta}(z) \\prod_{i=0}^{K-1}\\frac{1}{|det \\left( J_{f_i}\\right)|} \\\\\n",
    "  p_{\\theta}'(x) &= p_{\\theta}\\left( f_{\\theta}^{-1}(x) \\right) \\frac{1}{|det \\left( J_{f_{\\theta}}\\right)|} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Throughout this notebook, we will mostly use $z$ as a placeholder for the latent variable, $f_{\\theta}$ as the bijective transformation, parametrized by $\\theta$ that can generated and observed variable $x$, the later will often have larger dimension, to express images or time series.\n",
    "\n",
    "Now, let's assume we would like to compute the (log) likelihood of a given sample $x$:\n",
    "\\begin{align*}\n",
    "  log (p_{\\theta}'(x)) &= log(p_{\\theta}(z)) - \\sum_{i=0}^{K-1} log \\left( |det \\left( J_{f_i}\\right)| \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Depending on the invertible set of transform you use, given you have acces to automatic differentiation, this expression of likelihood can be use within an optimization objective in order to learn useful representations encoded by $f_{\\theta}$, given a set of training examples $\\{x_0, x_1, \\dots x_{n-1} \\in X\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE vs GANs vs NF\n",
    "\n",
    "As we have seen previously, VAE use variational inference, which is optimizing for the ELBO, which is a surrogate for the posterior distribution of latent variables given a sample for the encoder. We don't get the exact posterior.\n",
    "\n",
    "In GAN, there is basically no encoder, hence it is impossible to compute likelihood of latent variable given a sample. We can only train the generator/discriminator in a min/max fashion, with the usual risks of mode collapse, or infinite oscillations.\n",
    "\n",
    "From a theoretical point of view, NF has very interesting properties, as it allows exact likelihood evaluation, and potentially optimization during training by $log(p_{\\theta}(z)) - \\sum_{i=0}^{K-1} log \\left( |det \\left( J_{f_i}\\right)| \\right)$\n",
    "Exact posterior inference for a given $x$ throught the invertible transform $z=f_{\\theta}^{-1}(x)$\n",
    "\n",
    "Unfortunately, all those theoretical advantages will come with complex numerical challenges, such as finding transformations with tractable and numerically stable Jacobian determinant computations. One often tries to find transformations which Jacobian is diagonal or triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows: example of bijective systems in DL\n",
    "\n",
    "### Non linear independent components estimation (NICE)\n",
    "\n",
    "#### The triangular trick\n",
    "In the paper Non Linear independent components estimation (see [here](https://arxiv.org/abs/1410.8516) or [here](https://paperswithcode.com/method/nice) ) the authors use the concept of coupling layer.\n",
    "\n",
    "Coupling layer subsets input, let say for instance into 2 subsets (by simply projecting onto two complementary and trivial supports) and then apply identity on one part of this subset, and apply more complex transformation $g$ that use both subsets for the second output, with an intermediate transformation $m$ over thr first subset:\n",
    "\n",
    "* inputs: $z_{0:d-1}$ and $z_{d:D-1}$\n",
    "* output 1 is identity: $x_{0:d-1} = z_{0:d-1}$\n",
    "* output 2: $x_{d:D-1} = g(z_{d:D-1},m(z_{0:d-1}))$\n",
    "\n",
    "If you look carefully, you can see that the Jacobian of said transformation will look like this:\n",
    "\n",
    "\\begin{align*}\n",
    "  J_f &= \\begin{pmatrix}\n",
    "    \\frac{\\partial Id_{0:d-1}}{\\partial z_{0:d-1}} & \\frac{\\partial Id_{0:d-1}}{z_{d:D-1}} \\\\\n",
    "    \\frac{\\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{0:d-1}} & \\frac{ \\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{d:D-1}} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix}\n",
    "    Id_{0:d-1} & 0\\\\\n",
    "    \\frac{\\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{0:d-1}} & \\frac{ \\partial g\\left(z_{d:D-1},m(z_{0:d-1}\\right)}{\\partial z_{d:D-1}} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The problem of having a triangular jacobian then amounts to having the lower right part of previous expression triangular. If we analyse the invertibility requirement, we can see that, Identity being trivially invertible, we should concentrate on $g$.\n",
    "Now $g$ mainly needs to be invertible with regard to its first argument, given the second, because second argument $m(z_{0:d-1})$ can always be fully inverted thanks to the identity in the other output. In the NICE paper, $g$ is set to be a simple addition, and all the complexity (DNN) is used in the $m$ function.\n",
    "\n",
    "#### Overcoming additive coupling with scaling matrix\n",
    "Additive coupling layers like $g$ given as an example, are very limited in the sense that their unit gaussian forbids local expansion or contractions, which drastically reduces the expressivity of such functions.\n",
    "\n",
    "To start with, given one of our target which is to keep latent space $Z$ distribution to be as simple as possible (gaussian ?) while having the power to be matched with complex multimodal distribution of sample space $X$, we need those complex transformation.\n",
    "Also in the framework of generative network, we would like high density regions from sample space $X$ to expand to large regions of the latent space $Z$ for expressivity.\n",
    "\n",
    "The authors show that, this problem can somehow be circumvented by using a diagonal feature scaling matrix $S$ before applying $f$ transformations when decoding from the latent space.\n",
    "The log likelihood for a given decoding then reads:\n",
    "\n",
    "\\begin{align*}\n",
    "  log (p_{\\theta}'(x)) &= \\sum_{i=0}^{D-1} log(p_{\\theta}(f^{-1}(x)_i) - log \\left( |S_{ii}| \\right) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond dimensionality constraints of bijectors: Real NVP\n",
    "\n",
    "The problem with bijectors, is that by design, input and output dimensionality should be equal. Although in general, latent space in generative models are expected to be smaller in dimension. This is the problem the authors of [Real NVP](https://arxiv.org/abs/1605.08803) see explanation with code [here](https://paperswithcode.com/method/realnvp) or [here](https://keras.io/examples/generative/real_nvp) tried to address.\n",
    "\n",
    "In this work, authors tried to use a multiscal approach for images, where each set of latent variable was operating on a different resolution from the sample space.\n",
    "\n",
    "This allows for instance to drop high resolution elements during inference (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing flows and VAE\n",
    "\n",
    "Up to now, we have mostly seen how to derive likelihood of samples from a distribution of latent variable (hoping to back propagate the error in likelihood to learn something useful).\n",
    "\n",
    "We might want to parametrize the approximate posterior $p(x|z)$ in a VAE with a flow to form richer distributions. This is very interesting, as usually in VAE you are restricted to a simple multidimensional gaussian distribution in the latent space, although you could leverage normalizing flow to turn this gaussian encoder output into a more complex distribution before decoding.\n",
    "\n",
    "![title](data/vae_normalizing_flow.png)\n",
    "![title](data/vae_normalizing_flow2.png)\n",
    "courtesy: Ari Seff from [here](https://www.youtube.com/watch?v=i7LjDvsLWCg)\n",
    "\n",
    "This is the idea of Rezende & all in [Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked autoencoder and masked autoregressive flow for density estimation\n",
    "\n",
    "Mostly refers to [MADE Masked Autoencoder for Distribution Estimation](https://arxiv.org/abs/1502.03509) see author interview [here](https://www.youtube.com/watch?v=315xKcYX-1w) and [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057)\n",
    "\n",
    "The idea here is to take the likelihood of an output sequence as a product of conditional probability of each element upon its predecessor:\n",
    "\n",
    "\\begin{align*}\n",
    "  p_{\\theta}(x_0, x_1, \\dots x_{n-1}) &= \\prod_{i=0}^{n-1} p_{\\theta}(x_i|x_{0:i})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For each element in the sequence, one can model its distribution as\n",
    "\\begin{align*}\n",
    "  x_i = \\mu_i + z_i \\times \\sigma_i\\\\\n",
    "\\end{align*}\n",
    "with\n",
    "\\begin{align*}\n",
    "  z_i \\mathtt{\\sim} \\mathcal{N}(0,1) \\quad \\text{and} \\quad\\mu_i = f_{\\mu}(x_{0:i}) \\quad \\text{and} \\quad \\sigma_i = f_{\\sigma}(x_{0:i})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "See also inverse regressive flows see [improved variational inference with inverse autoregressive flow](https://arxiv.org/abs/1606.04934) and [\n",
    "Parallel WaveNet: Fast High-Fidelity Speech Synthesis](https://arxiv.org/abs/1711.10433)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows: main references\n",
    "Here are a few very important references we might refere to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic introduction with a great youtube video from Ari Seff:\n",
    "https://www.youtube.com/watch?v=i7LjDvsLWCg\n",
    "\n",
    "Video recording of ECCV2020 Introduction session to Normalizing Flows by Marcus A. Brubaker\n",
    "https://www.youtube.com/watch?v=u3vVyFVU_lI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the amazing work of Mahdi Karami. The document is a PhD thesis dedicated to the study of tools for variational inference, normalizing flows, analysis and design of operator with tractable inverse jacobian computation, with additional numerical and convex optimization tools. It also contains applications to latent space representations of complex distributions met in signal/image processing, and much more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/Karami_Mahdi_202008_PhD.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets move on to a very interesting review paper: Normalizing Flows: An Introduction and Review of Current Methods by Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1908.09257.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce4a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1908.09257.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reference paper, that exposes a lot of important properties of Normalizing flows, and design of methods to leverage them in machine learning / representation learning: Normalizing Flows for Probabilistic Modeling and Inference by George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/1912.02762.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/1912.02762.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a paper more targeted to application: Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows by Marco Rudolph, Bastian Wandt and Bodo Rosenhahn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"doc/NormalizingFlows/2008.12577.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27e97ce780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"doc/NormalizingFlows/2008.12577.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
