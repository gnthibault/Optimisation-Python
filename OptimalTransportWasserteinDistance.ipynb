{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal transport and Wasserstein distance\n",
    "\n",
    "This note book has been inspired by the following sources:\n",
    "* [this wikipedia article](https://en.wikipedia.org/wiki/Wasserstein_metric)\n",
    "* A very nice serie of blog articles:\n",
    "    * [introduction](http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT1.htm)\n",
    "    * [Wassertein GAN](http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [The original wasserstein GAN paper](http://www.stat.cmu.edu/~larry/=sml/Opt.pdf)\n",
    "https://arxiv.org/abs/1701.07875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(\"doc/OptimalTransportWasserteinDistance/wassersteinGAN.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An elementary introduction to entropic regularization and proximal methods for numerical optimal transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(\"doc/OptimalTransportWasserteinDistance/CourseOT.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A PhD thesis on \"Entropic Optimal Transport in Machine Learning: applications to Distributional Regression, Barycentric Estimation and Probability Matching\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(\"doc/OptimalTransportWasserteinDistance/PhD_thesis__LUISEGIULIA.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [This short course from Carnegie Mellon University](http://www.stat.cmu.edu/~larry/=sml/Opt.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(\"doc/OptimalTransportWasserteinDistance/optimal_transport.pdf\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction from Mindcodec\n",
    "\n",
    "### Basic introduction\n",
    "\n",
    "This content is a reproduction from [mindcodec website](http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT1.htm) all credit for this part pertains to the author.\n",
    "\n",
    "Optimal transport problems can be formulated in a very intuitive way. Consider the following example: an online retailer has $N$ warehouses and there are $K$ customers who ordered e-book readers. The n-th storage area $x_n$\n",
    "contains $m_n$ readers while the k-th customer $y_k$ ordered $h_k$ readers. The transport cost $c(x,y)$ is the distance between the storage area $x$ and the address of customer $y$.\n",
    "The optimal transport problem consists of finding the least expensive way of moving all the readers stored in the storage areas to the customers who ordered them.\n",
    "A transportation map $\\Gamma$ is a matrix whose $\\Gamma_{nk}$ entry represents the number of e-book readers sent from the n-th storage area to the k-th customer. For consistency, the sum of all the readers leaving the n-th storage areas has to be equal to the total number of readers stored in that area while the sum of all the readers arriving to a customer’s house has to be equal to the number of e-book readers she ordered. These are the hard constraints of the transport problem and can be written in formulas as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "  \\sum_{k} \\Gamma_{nk} = m_n\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "  \\sum_{n} \\Gamma_{nk} = h_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final constraint is that the entries of the matrix have to be positive-valued (for obvious reasons). The optimal solution is the transportation matrix that minimizes the total cost while respecting the constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "  \\hat{T} = \\underset{\\Gamma \\in \\mathbb{R}^{N\\times K+}}{argmin} \\sum \\Gamma_{nk} c(x_n, y_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this expression we are assuming that transporting L e-readers from $x_n$ to $y_k$ is $L$ times more expensive than transporting one reader. Note that this assumption is not realistic in most real world transportation problems since the transportation cost usually does not scale linearly with the number of transported units. Nevertheless, this simplified problem gives rise to a very elegant and useful mathematical theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic formulation\n",
    "In machine learning and statistics it is often useful to reformulate the optimal transport problem in probabilistic terms. Consider two finite probability spaces $(X, P)$ and $(Y, Q)$ where $X$ and $Y$ are finite sets and $P$ and $Q$ are probability functions assigning a probability to each element of their set. The optimal transport between $P$ and $Q$ is the conditional probability function $\\Gamma(y|x)$ that minimizes the following cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "  \\underset{\\Gamma}{argmin} \\sum \\Gamma(y_n|x_k) P(x_k) c(x_n, y_k)\n",
    "\\end{align*}\n",
    "subject to the following marginalization constraint:\n",
    "\\begin{align*}\n",
    "  \\sum \\Gamma(y_n|x_k) P(x_k) = Q(y_n)\n",
    "\\end{align*}\n",
    "This simply means that the marginal distribution of the joint probability $\\Gamma(y_n|x_k) P(x_k)$ is $Q(y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $\\Gamma(y_n|x_k)$ is transporting the distribution $P(x)$ into the distribution $Q(y)$.\n",
    "This transportation can be interpreted as a stochastic function that takes $x$ as input and outputs a $y$\n",
    " with probability $γ\\Gamma(y|x)$. The problem thus consists of finding a stochastic transport that maps the probability distribution $P$ into the probability distribution $Q$ while minimizing the expected transportation cost. It is easy to see that this problem is formally identical to the deterministic problem that I introduced in the previous section. The transportation matrix $\\Gamma_{nk}$ is given by $\\Gamma(y_n|x_k) P(x_k)$.\n",
    "This ensures that the first constraint (number of ebook per warehouse) is automatically fulfilled while the second constraint (number of ebook per customer) still needs to be enforced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous formulation\n",
    "It is straightforward to extend the definition of probabilistic optimal transport to continuous probability distributions. This can be done by replacing the probabilities $P(x)$ and $Q(x)$ with the probability densities \n",
    "$p(x)$ and $q(x)$ and the summation with an integration:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\underset{\\gamma}{argmin} \\int \\gamma(y|x) p(x) c(x, y) dxdy\n",
    "\\end{align*}\n",
    "\n",
    "Analogously, the marginalization constraint becomes:\n",
    "\\begin{align*}\n",
    "  \\int \\gamma(y|x) p(x) dx = q(y)\n",
    "\\end{align*}\n",
    "\n",
    "This continuous optimal transport problem is usually introduced in a slightly different (and in my opinion less intuitive) form. I will denote the joint density $\\gamma(y|x) p(x)$ as $\\gamma(x,y)$.\n",
    "It is easy to see that the problem can be reformulated as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\underset{\\gamma}{argmin} \\int \\gamma(x,y) c(x, y) dxdy\n",
    "\\end{align*}\n",
    "\n",
    "with the two marginalization constraints:\n",
    "\\begin{align*}\n",
    "  \\int \\gamma(x,y)dx = q(y)\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "  \\int \\gamma(x,y)dy = p(x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal transport divergences\n",
    "In many situations the primary interest is not to obtain the optimal transportation map. Instead, we are often interested in using the optimal transportation cost as a statistical divergence between two probability distributions. A statistical divergence is a function that takes two probability distributions as input and outputs a non-negative number that is zero if and only if the two distributions are identical. Statistical divergences such as the $KL$ divergence are massively used in statistics and machine learning as a way of measuring dissimilarity between two probability distributions. Statistical divergences have a central role in several of the most active areas of statistical machine learning, such as generative modeling and variational Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal transport divergences and the Wasserstein distance\n",
    "An optimal transport divergence is defined as the optimal transportation cost between two probability distributions:\n",
    "\n",
    "\\begin{align*} \n",
    "  OT_c[p,q] = \\underset{\\gamma}{inf} \\int \\gamma(x,y) c(x,y) dxdy\n",
    "\\end{align*}\n",
    "\n",
    "where the optimization is subject to the usual marginalization constraints. This expression provides a valid divergence as far as the cost is always non-negative and $c(x,x)$ vanishes for all values of $x$. Clearly, the properties of an optimal transport divergence depend on its cost function. A common choice is the squared Euclidean distance:\n",
    "\n",
    "\\begin{align*}\n",
    "  c(x,y) = \\|x-y\\|_2^2\n",
    "\\end{align*}\n",
    "\n",
    "Using the Euclidean distance as a cost function, we obtain the famous (squared) 2-Wasserstein distance:\n",
    "\n",
    "\\begin{align*} \n",
    "  W_2[p,q]^2 = \\underset{\\gamma}{inf} \\int \\gamma(x,y) \\|x-y\\|_2^2 dxdy\n",
    "\\end{align*}\n",
    "\n",
    "The squared root of $W_2[p,q]^2$ is a proper metric function between probability distributions as it respects the triangle inequality. Using a proper metric such as the Wasserstein distance instead of other kinds of optimal transport divergences is not crucial for most machine learning applications, but it often simplifies the mathematical treatment. Finally, given an integer $k$, the k-Wasserstein distance is defined as follows:\n",
    "\n",
    "\\begin{align*} \n",
    "  W_k[p,q]^k = \\underset{\\gamma}{inf} \\int \\gamma(x,y) \\|x-y\\|_k^k dxdy\n",
    "\\end{align*}\n",
    " \n",
    "where $\\|\\cdot\\|_k^k$ denotes the $L_k$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem and the Wasserstein GAN\n",
    "Optimal transport problems are a special case of linear programming problems since both the function to be optimized and the constraints are linear functions of the transportation map. The theory behind linear programming dates back to the beginning of the last century and is one of the cornerstones of mathematical optimization. One of the most fundamental results of linear programming is that any linear problem has a dual problem whose solution provides an upper bound to the solution of the original (primal) problem. Fortunately, it turns out that in the case of optimal transport the solution of the dual problem does not simply provide a bound but is indeed identical to the solution of the primal problem. Furthermore, the dual formulation of the optimal transport problem is the starting point for adversarial algorithms and the Wasserstein GAN. The dual formulation of an optimal transport divergence is given by the following formula:\n",
    "\n",
    "\n",
    "\\begin{align*} \n",
    "  OT_c[p,q] = \\underset{f \\in L_c}{sup} \\left[\\int_x f(x)p(x)dx - \\int_y f(y)q(y)dy \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where $L_c$ is the set of functions whose growth is bounded by $c$:\n",
    "\n",
    "\\begin{align*} \n",
    "  L_c = \\{ f: \\mathbb{R} \\rightarrow \\mathbb{R} | f(x)-f(y)\\leq c(x,y) \\}\n",
    "\\end{align*} \n",
    "\n",
    "It is far from obvious why this expression is equivalent to the primal expression that I gave in the previous sections and I will spend the rest of the post proving this result. However, the formula in itself has a rather intuitive interpretation. Clearly, if $p$ is equal to $q$, the difference between their expected values of any function $f$ will be zero and consequently the divergence will vanish. Now assume that $p$ and $q$ differ in some region of their domain. In this case the divergence is obtained by finding the function $f$ that maximizes this difference in terms of its expected value. In other words, $f$ acts like a feature detector that extract the features that maximally differentiate $p$ from $q$.\n",
    "\n",
    "For example, imagine that $p$ is a distribution over landscape images without traces of human activity while $q$\n",
    "is a distribution over landscape images with a plane in the sky. In this case, the optimal $f$ will be a plane detector. From this example you can see how $f$ plays the role of a discriminator in the Wasserstein GAN. Note that without any constraints on $f$ any small difference in the distributions can be magnified arbitrarily and the divergence would be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proving the duality\n",
    "\n",
    "In order to prove the duality, we need to reformulate the constrained optimization in the primal problem as an unconstrained one. Consider the following optimization:\n",
    "\n",
    "\\begin{align*} \n",
    "  \\underset{f}{sup} \\left[\\int_{x'} f(x')p(x')dx' - \\int_{xy} f(x)\\gamma(x,y)dxdy \\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $f$ can be any function. The term on the left-hand side is the expectation of $f$ under $p$ while the term on the right-hand side is the expectation of $f$ under the marginal distribution $\\int \\gamma(x,y)f(y)q(y)dy$.\n",
    "This expression is clearly zero for all possible $f$ if the marginal constraint over $p$ is met since the two terms will be identical. However, if the constraint is not met the values of $f$ can be chosen to be arbitrarily large for the values of $x$ where the two marginals are different and the result of the optimization will be infinity. Therefore, adding two terms of this form to the loss function of our optimization problem will not change the problem when the constraints are met but it will exclude all the possible solutions that do not satisfy the constraints.\n",
    "\n",
    "Also note that the term on the left $\\int_{x'} f(x')p(x')dx'$ can be moved inside the expectation integral on the right since the expectation integral of a constant is the constant itself:\n",
    "\n",
    "\\begin{align*} \n",
    "  & \\underset{f}{sup} \\left[\\int_{x'} f(x')p(x')dx' - \\int_{xy} f(x)\\gamma(x,y)dxdy \\right] \\\\\n",
    "  &= \\underset{f}{sup} \\left[\\int_{x'} f(x')p(x')dx' - f(x)\\right] \\gamma(x,y)dxdy\n",
    "\\end{align*}\n",
    " \n",
    "We can now write a modified loss function that incorporates the constraints:\n",
    "\n",
    "\\begin{align*} \n",
    "  OT_c[p,q] = \\underset{\\gamma}{inf} \\left[ \\int \\gamma(x,y)c(x,y)dxdy + \\underset{f}{sup} \\mathcal{L}[f,\\gamma] \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}[f,\\gamma] = \\left[ \\left( \\int f(x')p(x')dx' - f(x)\\right) - \\left( \\int f(y')p(y')dy' - f(y)\\right) \\right] \\gamma(x,y) dxdy\n",
    "\\end{align*}\n",
    "\n",
    "The next thing to do is to exchange the order of the infinum and the supremum. This can be done by using Sion’s minimax theorem since the loss function is linear in both $f$ and $\\gamma$:\n",
    "\n",
    "\\begin{align*} \n",
    "  OT_c[p,q] &= \\underset{f}{sup} \\underset{\\gamma}{inf} \\left[ \\int \\gamma(x,y)c(x,y)dxdy + \\mathcal{L}[f,\\gamma] \\right] \\\\\n",
    "  &= \\underset{f}{sup} \\left[ \\int f(x')p(x')dx' - \\int f(y')p(y')dy' + \\underset{\\gamma}{inf} \\int l(x,y)\\gamma(x,y)dxdy \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*} \n",
    "  l(x,y) = c(x,y) - (f(x)-f(y))\n",
    "\\end{align*}\n",
    "\n",
    "We are almost there! The optimization over $f$ on the right-hand side of this expression can be converted into a constraint. In fact, if $l(x,y)\\geq 0$ for all $x$ and $y$ then the infimum is zero and is reached by assigning the whole probability density on the $x=y$ subspace.\n",
    "Conversely, if there is a region where $l(x,y) < 0$ the cost can become arbitrarily large by assigning an arbitrarily large amount of density to that region. By converting this term into a constraint we arrive at the dual formulation of the optimal transport problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why to use a Wasserstein divergence\n",
    "The original wGAN paper opens with a lengthy explanation of the advantages of the Wasserstein metric over other commonly used statistical divergences. While the discussion was rather technical, the take home message is simple: the Wasserstein metric can be used for comparing probability distributions that are radically different. What do I mean by different? The most common example is when two distributions have different support, meaning that they assign zero probability to different families of sets. For example, assume that $P(x)$ is a usual probability distribution on a two dimensional space defined by a probability density. All sets of zero volume (such as individual points and curves) in this space have zero probability under $p$. Conversely, $Q(x)$ is a weirder distribution that concentrates all its probability mass on a curve $\\alpha$.\n",
    "\n",
    "All sets that do not contain the curve have zero probability under $Q$ while some sets with zero volume have non-zero probability as far as they “walk along” the curve.\n",
    "\n",
    "Now, these two distributions are very different from each other and they are pretty difficult to compare. For example, in order to compute their KL divergence we would need to calculate:\n",
    "\n",
    "\\begin{align*}\n",
    "  D_{KL}(p||q) = \\int_x p(x) log \\left( \\frac{p(x)}{q(x)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where the density ratio $p(x)/q(x)$ needs to be evaluated for all points, but $Q$ does not even have a density with respect to the ambient space! However, we can still transport one distribution into the other using the optimal transport formalism introduced earlier ! The Wasserstein distance between the two distributions is given by:\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
