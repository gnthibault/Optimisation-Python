{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The recursive least square algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notation\n",
    "\n",
    "By default, we will consider the framework of one dimensional time series, but our purpose can be later extended to multidimensional data.\n",
    "\n",
    "### Input\n",
    "\n",
    "Let's begin with the set of input samples:\n",
    "$$\n",
    "    \\{ u(0), u(1), \\dots u(N) \\}\n",
    "$$\n",
    "Our time serie which will be an input of the RLS algorithm. In the real world, time series are often corrupted with noise, and we are interested in obtaining, for each time point $n$, an estimator $y(n)$ of the original perfect signal $d(n)$\n",
    "\n",
    "For the convenience of our demonstration, we will consider that all previous signals are available, ie:\n",
    "$$\n",
    "    u(k) = 0 \\forall k < 0\n",
    "$$\n",
    "\n",
    "We also call\n",
    "$$\n",
    "    \\{ d(0), d(1), \\dots d(N) \\}\n",
    "$$\n",
    "the desired response, that we wish to recover from $u$\n",
    "\n",
    "### Output\n",
    "\n",
    "Let's call\n",
    "$$\n",
    "    y(n) = \\sum_{k=0}^{M-1} w_k u(n-k)\n",
    "$$\n",
    "an estimator of the perfect time serie that can be considered as the result of the application of a linear system or filter over the $M$ previous elements of the time serie.\n",
    "\n",
    "The $w_k$ are the coefficient of the filter, and we would like to find the one that are the most suited for our estimator.\n",
    "\n",
    "\n",
    "## The (recursive) least square estimator\n",
    "\n",
    "The least square estimator is very commonly used in many fields of science and engineering. Part of its success comes from the fact that it can be derived from a simple bayesian reasonning when using a gaussian noise model, in our case it would read:\n",
    "\n",
    "$$\n",
    "    \\tilde{\\vec{y(n)}} = \\underset{y}{argmin} \\| y(n) - d(n) \\|_{2}^{2} \\\\\n",
    "    \\tilde{\\vec{y(n)}} = \\langle \\vec{w}, \\vec{u_n} \\rangle\n",
    "    \\text{ When } \\vec{w} = \\underset{\\vec{w}}{argmin} \\| \\langle \\vec{w}, \\vec{u_n} \\rangle - d(n) \\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "This approach only take into account $y(n)$ and $d(n)$ at a single time point to derive the filter $\\vec{w}$, and it is probably not the best option in case we have large deviation in a sample.\n",
    "We can instead, look for a $\\vec{w}$ that is optimal for the N last few samples, including the current one, and we can even use a forgetting factor $\\beta$ such that the quadratic fidelity take more into account the most recent samples:\n",
    "\n",
    "$$\n",
    "    0 < \\beta(n,i) \\leq 1, i=n-(N-1),n-(N-1)+1,\\dots,n\n",
    "$$\n",
    "\n",
    "An exponentially decreasy $\\beta$ may for instance be a good choice:\n",
    "\n",
    "$$\n",
    "   \\beta(n,i) = \\lambda^{n-i}\n",
    "$$\n",
    "\n",
    "We can then define our **_recursive_** least square estimator by using a few linear operators:\n",
    "\n",
    "\n",
    "With:\n",
    "\n",
    "$$ R^{n} =\n",
    "    \\begin{pmatrix}\n",
    "        u(n-(M-1)-(N-1)) & u(n-(M-1)-(N-2))& \\dots & u(n-(M-1))   \\\\\n",
    "        u(n-(M-2)-(N-1)) & u(n-(M-2)-(N-2))& \\dots & u(n-(M-2)) \\\\\n",
    "        \\vdots           & \\vdots  & \\dots & \\vdots   \\\\\n",
    "        u(n-(N-1))       & u(n-(N-2))  & \\dots & u(n)     \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Describing the content of this matrix is pretty straightforward:\n",
    "* The line T contains all inputs from $u(T)$ to $u(T-(N-1))$, ie, the $N$ previous entry, that should be used to predict the output $d(T)$\n",
    "* We have $M$ lines, in order to compute the predictor on the $M$ samples under consideration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we also want to include the forgetting function $\\beta$ in our model then we can use $B$, a nice diagonal matrix of size $N$:\n",
    "$$ B =\n",
    "    \\begin{pmatrix}\n",
    "        \\beta(0,N-1) & 0 & \\dots & 0 \\\\\n",
    "        0 & \\beta(0,N-2) & \\dots & 0 \\\\\n",
    "        \\vdots           & \\vdots  & \\ddots & \\vdots   \\\\\n",
    "        0 & 0 & \\vdots & \\beta(0,0) \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So at each time step index $n$, we would like to solve:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\vec{\\vec{w_n}} = &\\underset{\\vec{w}}{argmin} \\| B^{\\frac{1}{2}}(R^{n} \\vec{w} - \\vec{d_n}) \\|_{2}^{2} \\\\\n",
    "    \\iff &\\underset{\\vec{w_n}}{argmin} \\| (B^{\\frac{1}{2}}R^{n} \\vec{w_n} - B^{\\frac{1}{2}}\\vec{d_n}) \\|_{2}^{2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Where $\\vec{w_n}$ is the current estimate for $\\vec{w}$ at step n\n",
    "$$\\vec{w} = \\begin{pmatrix}w_{N-1} \\\\ w_{N-2} \\\\ \\vdots \\\\ w_{0}\\end{pmatrix}$$\n",
    "$$\\vec{d_n} = \\begin{pmatrix}d_{n-(N-1)} \\\\ d_{n-(N-2)} \\\\ \\vdots \\\\ d_{n}\\end{pmatrix}$$\n",
    "\n",
    "This is a simple linear least square that can be solved potentially with Moore Penrose Pseudo inverse.\n",
    "We recall that its expression for the functional : $\\frac{1}{2}||Ax-b||_2^2$ is $(A^t A)^{-1}A^t b$, which in this case gives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{w_n} = &((B^{\\frac{1}{2}}R^{n})^t B^{\\frac{1}{2}}R^{n})^{-1} (B^{\\frac{1}{2}}R^{n})^t B^{\\frac{1}{2}}\\vec{d_n} \\\\\n",
    " = & A_n^{-1} b_n\n",
    "\\end{align*}\n",
    "$$\n",
    "With\n",
    "* $A_n = (B^{\\frac{1}{2}}R^{n})^t B^{\\frac{1}{2}}R^{n})$\n",
    "* $b_n = (B^{\\frac{1}{2}}R^{n})^t B^{\\frac{1}{2}}\\vec{d_n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the recursivity kicks in\n",
    "\n",
    "We are now going to take a look at the links between $w_n = A_n^{-1} b_n$ and $w_{n-1} = A_{n-1}^{-1} b_{n-1}$\n",
    "\n",
    "Let's take first a closer look at \n",
    "$$ R^{n} =\n",
    "    \\begin{pmatrix}\n",
    "        u(n-(M-1)-(N-1)) & u(n-(M-1)-(N-2))& \\dots & u(n-(M-1))   \\\\\n",
    "        u(n-(M-2)-(N-1)) & u(n-(M-2)-(N-2))& \\dots & u(n-(M-2)) \\\\\n",
    "        \\vdots           & \\vdots  & \\dots & \\vdots   \\\\\n",
    "        u(n-(N-1))       & u(n-(N-2))  & \\dots & u(n)     \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ R^{n-1} =\n",
    "    \\begin{pmatrix}\n",
    "        u(n-(M-1)-N) & u(n-(M-1)-(N-1))& \\dots & u(n-(M-1)-1) \\\\\n",
    "        u(n-(M-2)-N) & u(n-(M-2)-(N-1))& \\dots & u(n-(M-2)-1) \\\\\n",
    "        \\vdots           & \\vdots  & \\dots & \\vdots   \\\\\n",
    "        u(n-N))       & u(n-(N-1))  & \\dots & u(n-1)     \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Matrix $R^{n-1}$ is like $R^{n}$ that would have been shifted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
