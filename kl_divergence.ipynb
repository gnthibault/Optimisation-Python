{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#math and linear algebra stuff\n",
    "import numpy as np\n",
    "\n",
    "#Math and linear algebra stuff\n",
    "import scipy.stats as scs\n",
    "\n",
    "#plots\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "#mpl.rc('text', usetex = True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Kullback Liebler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notation\n",
    "Let's talk about a set of N sample vectors $x_i, i=0,1,\\dots N-1$ where $x_i \\in \\mathbb{R}^P$, such that each $x_i$ is an output of a random process, that can be modeled using a probability density function: \n",
    "$$\n",
    "    P_{\\theta}(x) = P(x|\\theta)\n",
    "$$\n",
    "$\\theta \\in \\mathbb{R}^K$ being the pdf parameter vector.\n",
    "\n",
    "We can also define the empirical data distribution, that sums to one, using:\n",
    "$$\n",
    "    P_{D}(x) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\delta(x-x_i)\n",
    "$$\n",
    "\n",
    "Where $\\delta(x)$ stands for the Dirac delta function. \n",
    "\n",
    "## Quantity of information\n",
    "\n",
    "### Information theory framework\n",
    "In the framework of information theory, a signal of interest can be modeled as a message produced by a transmitter, enventually modified by the channel, that should be retrieved by the receiver.\n",
    "The intrinsic complexity of the message, or the quantity of information it contains can be modeled using shannon entropy, defined itself as the expectation of the self information for symbols coming from a given class of message.\n",
    "\n",
    "### Self information\n",
    "Self information $I$ of a random event $\\omega$ with probability $P(\\omega)$ is defined as\n",
    "\n",
    "$$\n",
    "    I(\\omega) = log\\left(\\frac{1}{P(\\omega)}\\right) = -log(P(\\omega))\n",
    "$$\n",
    "\n",
    "We generally use the logarithm in base 2 so that self information can be defined in bits, and so do the entropy.\n",
    "Let's list some interesting properties of this metric:\n",
    "  - as $I$ is a monotically increasing function of $\\frac{1}{P(\\omega)}$, the realisation of the event that have a low probability gives us a large quantity of information.\n",
    "  - An event that is not random, ie for wich $P(\\omega)=1$ gives no informations.\n",
    "  - The simultanneous occurence of two independent events $\\omega_a$ and $\\omega_b$ with respective probability $P(\\omega_a)$ and $P(\\omega_b)$ bring us a total quantity of infomation equal to the sum of the information of the two events, due to the use of the logarithm.\n",
    "\n",
    "\n",
    "### Entropy\n",
    "Shannon entropy $H(x)$, as said earlier is defined as $H(x)=E[I(X)] = E[-log(P(x))]$ which reads in the discrete case:\n",
    "$$\n",
    "    H(x) = - \\sum_{i=0}^{N-1} P(x_i)log(P(x_i))\n",
    "$$\n",
    "\n",
    "It is interesting to notice that $H(x)$ can be interpreted as the entopy rate of a data source or the average number of bits per symbol needed to encode a message whith a known symbol probability density function.\n",
    "\n",
    "This concept can obviously be extended to blocks or structured blocks of data instead of symbols.\n",
    "\n",
    "#### A small numerical illustration\n",
    "Let's say that we have a random bit generator, that can be tuned to generate the value $1$ with a probability $P_1$ varying from $0$ to $1$, the probability that the random bit generator output $0$ is given by $P_0 = 1-P_1$. This is a classic Bernoulli scheme.\n",
    "The entropy of this binary source is then\n",
    "\n",
    "\\begin{align}\n",
    "    & -P_1 log_2(P_1) - P_0 log_2(P_0) \\\\\n",
    "    =& -P_1 log_2(P_1) -  (1-P_1) log_2(1-P_1)\n",
    "\\end{align}\n",
    "\n",
    "Let see what does the entropy of this random bit generator, along the value of $P_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f8432f498d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8lXP6//HXVTEopeR8yLFpKkooDNoOQzklhylGiIjC\njBmDGeYn42zma5wlch6HSMRIMtqORc6lo1CpMCoiSu2u3x+fe9eyW3vv1d7rXvc6vJ+Px3rstdZ9\nr/u+9r3XXtf6nM3dERERqapB0gGIiEh+UoIQEZG0lCBERCQtJQgREUlLCUJERNJSghARkbSUICRv\nmFlrM3vPzL41s3OSjmdtmNlYMzstB+e5zMwerGH7JDPbP+44pDQoQRQAM/vMzH4ws8Vm9l308+YM\nX5uTD64suRB4yd2bufut9TmQmZ1iZiuia/VNlHgOz1KcSat28JK7t3f3V2BVMnkgd2HFJ/p7vpp0\nHKVGCaIwOHC4uzd19w2jn+dl48Bm1jAbx8mSVsBHWTzeG9G12gi4A3jUzJpm8fiSBRm+B40aEmOW\nziFVKEEUDkv7ZPTNysz+YWYLzWymmR0abbsS2A+4NbXUYWYrzWyAmU0HpkfP7WNmb5nZIjN708z2\nTjnHWDO7Onr+WzMbYWYbRdueNbOBVWL6wMx6VBPvUVE1yEIze8nMfhk9/1/gAOC2KNad0rz2VDOb\nHG3/2MzOXIvr9yDQGNg55XjDzGx+9DuXm1nblG33mtmt0e+32MzGmdn2Kdt/Y2ZTotfeQsrfx4JL\no5LfF2Z2X2ViMrNW0fU/1cxmm9kCM+tvZntE121hdLyarG9mj0ZxvW1mu6ac+1MzOzB6D/wV6BWV\nOt9LdyAz62Rm70Z/12HRcf+esv2IqPS1yMxeM7NdqpzrT1Hci8zsETNbdy1ee6GZfQB8b2YNzOyi\n6O+6OHqPHB3t24aQ4PeOfpeF0fNNzewBM/sqOt4lKcc/JTrnDWb2NXBZLddU0nF33fL8BnwKHFjN\ntlOAZcBphA+ps4C5KdvHAqdVec1KYDTQDPgF0BxYCJxI+NLQO3rcPOUYc4BfAesDTwAPRNuOB8an\nHLsD8D+gUZpYWwPfAwcCDYE/AzMq900Xa5XXdwe2i+7vBywBOtZwXV6J7jcEBgJLgZYp+5wKbACs\nA9wAvJey7d7o99g9uiYPAQ9H2zYGFgM9o2P/AVheGXv0t5hOKBFtAAxPuV6tout/O7AucDDwI/Bk\ndNwtgS+B/ar5vS6L/t6V5/4T8AnQsOp7Jdr3gRqu5zrAZ8A50bF6Rsf+e7R9tyiWPaL3Vp/o+Ouk\nnGs8sBmwETAZOHMtXvtu9Pv+InruWGCzlPfV9ymPV/09U+J/ABgRXeNWwDSgb8r+y4EB0d/vF0n/\nHxfiLfEAdMvgjxT+mRYTPrQXRT9Pj7adAkxP2Xf96ANo0+hxdQmia8rjk0j5kI+eewM4OeUYV6ds\n+1X0QWKEBLMA2DHa9g/g1mp+j0uBR1MeG/A5sH91sdZyXUYA51azrfIDYiHwEyGZHFfDsTaKrsuG\n0eN7gSEp27sDk6P7fQjVV6mvn8PqBPEicFbKttZRDA2iD7IKYPOU7V8Dx6c8fgI4r5o4L0s9d3QN\n5wG/TnmvZJog9gPmVHnuVVYniNuBy6tsn0qUvKJznZCy7Trg9rV47Sm1/H3fA45M+Xu+krKtQfQe\n/GXKc2cS2rAq9/8s7v/NYr+piqlw9HD3Fu7ePPo5NGXbF5V33P3H6G6TWo73ecr9LYFZVbbPArZK\neTynyrZ1CN/GlwHDgJPMzIATCNU56fzsPB7+k+dUOU+1zKx7VNWzwMwWET60W9bwknHu3oLw4T8S\nWNW7J6rSuDaq0viG8IHlVY73Rcr9H1h9Tbfk59eDKo+rXs9ZQCPCN+1KX6Xc/5HwbTv1cU1/v1Xn\niq7h59E519aWwNzqjk1IZn+Kqr0WRtd86yrnSo079Rpl8trU9yBmdnJKldQioB3V/31bEq7p7JTn\nanrPSh0oQRSOtG0QGaiuYS/1+XnAdlW2b8vPPzy2SbnfivCN+Ovo8f2EUshBwBJ3f7Oac86LXptq\nG6p8UKQT1W0/AVwPbOLuzYFRZHBd3P0HQlVDHzPrED19InAk4dv2RoTf3zI5HjCfcH2q/h6Vqv6e\nrQilmS/JjlXnipLy1qz5QQ+1N+rOZ83knPp7zAGuir6QVH45aeLuj2UQYyavXRWfmW0LDAEGRPs2\nJ3RYsKr7Rr4mXNOq1zn1OtS5UVsCJYji9yWwQy37PAfsbGa9zayhmfUiVCM9m7LPSWbWxsw2AC4H\nHo++veLu4wnVM/9H9aUHCCWNw83sADNrZGYXENoFxmXwe6wb3b5295Vm1h04JIPXEcW4CLib1Y2V\nGxKqKBaZWWPgGjL/QPkP0NbMjo6u1++BzVO2PwKcb2bbmVkT4CpC1drKaHtdk32l3SvPDZxPuIbp\nkvKXwHZREklnHFBhZgOj36MH0Dll+13AWWbWGcDMGpvZYdH1qs3avrYx4T30dVS66wu0r/K7bG1m\n6wBE13IYcJWZNTGzVoRrUdP7T9aSEkTheCbq3VF5G17DvqkfdDcBx0fVMjem2Y67LwSOAC4gfDO7\ngNCtdmHKbg8SSgrzCB/Uv69yzgcI/9APVRuU+3RCSeNWQgPw4YQ65hXp4qry2u+B84DHo14svYGn\nq9u/GjcC3c2sfRTvbMI3zkmENpeMuPsCQiPqdYTrtSPwWsou9xCu1yvATELVS2q35Kq/Z22Pq3oa\n6EVoj/odcIy7V6R57eOEZLTAzN5O83ssB44B+kXHOhF4hpA4cfd3gDMIveAWEhreT8kkzrV9rbtP\nIXzBGE+o2mvHz6/pS4QSxRdmVlk9dx7h2n5CuNYPufu91cUka8+iL4HxHNxsKOGD50t337WafW4m\n1CUvAU519/djC0jqxMzGAg+6+z017NMHOMPdNYq3gJnZeOAOd78/6VgkeXGXIO4FDq1uY1RNsKO7\n7wz0BwbHHI/EIKp2GgDcmXQssnbMbH8z2yyqYjoF2AV4Pum4JD/EmiDc/TVC0bU6PQhFfaKGzWZm\ntlkN+0syqi1mmtkhhB458wl171JYfgl8QPg/PR841t2z1ZguBa5Rwuffip93RZsbPac3aB5x9wNr\n2PYCtXeplTzl7ncRGpRF1qBGahERSSvpEsRcft7vurr+3JiZ+jSLiNSBu9epa3UuShA1DT4aCZwM\nYGZ7Ad/UVP+Z9LDzfLlddtlliceQL7e1uRbz5zuPPuoMGOB06uRssEH42a+fc+ONzosvOvPmOStX\nxh/3ggXO6687d97pnHOOs+++TpMmTps2zsknO3fd5Xz88drFoveFrkW6W33EWoIws4eBMmBjM5tN\nGKS0LmGGgCHu/lw0eOZjQjfXvnHGI6Vl6VJ45RV47jl44QWYPx/23x+6doWTToKOHWH99ZOJrUUL\n2GefcKu0YgVMngzjx8PYsfD//h80bAgHHwyHHQaHHALNmiUTr5SmWBOEu5+YwT4FtXKY5LfFi+GZ\nZ+CJJ+Cll6B9+/Dh+uCDISE0zONVARo1gl13DbczzwR3mDEjJLd77oHTToM99oBjjw23LbZIOmIp\ndkm3QUgdlJWVJR1C3igrK2Pp0pAUHnoIystDKeHYY+Huu2HjjZOOsO7MoHXrcDvnHPjhBxgzBoYP\nD6WL9u3hxBOhVy9o3lzvi1S6FtkR60jqbDIzL5RYJTcmTIChQ+Hxx2G33eDkk6FHj9Kohlm2LJQs\nHnwQRo8O1U+nnRZ+5nMpSXLPzPA6NlIrQUhBWbIEHn0U7rgDFiyAfv2gTx/YturcqiVk0SIYNgzu\nuitck/79Q7LYdNOkI5N8oAQhRW/+fLjlFhgyBPbeG84+Gw49VN+Wq5owISTPESPgmGPgj3+Edu2S\njkqSVJ8EoYFyktemTYO+faFt29AAPX58aG847DAlh3T23DM0aM+YAdttBwcdFK7Vq68mHZkUIiUI\nyUsffQQnnAD77Qc77AAffwy33go77ZR0ZIWhZUv429/gs8+gZ0849VQ44IDQfVYFccmUEoTklY8/\nDonhwANDt9SZM8MHXSH3RkrSeuvBGWesLon17x/GgbyR8eoXUsqUICQvzJ8f2hX22it035w5Ey66\nCDbcMOnIikOjRqGX1+TJoQH7hBNCj69Jk5KOTPKZEoQk6scf4aqrQlJo3Dh8073kEmii+WFj0ahR\nqG6aNg3KykIbxVlnwf/+l3Rkko+UICQR7mH8Qtu28O67offNP/+pqqRcWW89OP98mDo13G/bFm64\nAX76KenIJJ+om6vk3IwZMGAAfPkl3Hxz+CYryZo6NSSM2bNDN9n9tXBs0VA3VykIS5fCoEFhHEP3\n7qHkoOSQH9q0CZMa/v3v8LvfhQZtVTuJEoTkxBtvhF5JH34I770XBnA10kxgecUszGE1eTJstBHs\nsksYta6Ce+lSFZPE6ocf4NJL4ZFHwkjo445LOiLJ1Jtvhh5PrVuHaqfNN086IqkLVTFJXnrrrVBq\n+OILmDhRyaHQdOkSqgHbtYMOHUKnAiktKkFI1q1YAddcE0Y+33abEkMxmDAhtE3ss0/oWNC0adIR\nSaZUgpC88dlnoQfMK6+Eb59KDsVhzz1D29F664XShEZilwYlCMmaESOgc+fQ0Dl6NGy1VdIRSTY1\nbgyDB8NNN4X5nf7xD1i5MumoJE6qYpJ6W7YMLrwQRo4MvV66dEk6Ionb7NnQu3dYye7++8PkgJKf\nVMUkifn881ClNHt2qFJScigN224LL78cGrB33z20UUjxUYKQOnv55VA33bMnPPlk+DYppWOddeD6\n6+HGG8OaE/fck3REkm2qYpK15h56slx9dVgT+ZBDko5IkjZlSviiUFYW3hvrrpt0RFJJS45Kzvz0\nEwwcGAZRPf00bL990hFJvli8GE46KfwcPlwTL+YLtUFITixYENaB/vJLeP11JQf5uaZNV/dk69Il\nTAAohU0JQjIyfXpYzGfPPcOHgBbykXQaNgztEpdcEjovvPhi0hFJfShBSK3GjQv/7BddFP75GzZM\nOiLJd337hqk5fve70E4lhUltEFKjp54Kaxo/8ECYoltkbUyeHHo49e8PF18cZoyV3FIjtcRi8OCw\nPsDIkbDHHklHI4Vq3ryQJPbZJ8zoqxJobilBSFa5h8n2hg6FF16AHXdMOiIpdIsXQ48esNlmoTSq\nbrC5owQhWeMeps14/vmQHLbYIumIpFj8+CP06hVm+33iCdhgg6QjKg3q5ipZUVEBZ54Jr74aRkkr\nOUg2rb/+6vER3bqFUoXkNyUIAUJy6NsXZs4MXRNbtEg6IilG66wTJvdr3z6MwP/mm6QjkpooQQgr\nVoQRsF98Ac8+C02aJB2RFLMGDcJCUl26wG9+AwsXJh2RVEcJosQtXw4nnhi+yY0cqXphyQ2zMMlf\n165w0EFhlL7kHyWIErZiRRjI9MMPYbzDeuslHZGUErOw6NChh4aSxKJFSUckVakXU4mqqIBTT4X/\n/S9MuveLXyQdkZQqd7jgAnjtNRgzRutdZ1te92Iys25mNtXMppvZRWm2NzWzkWb2vplNNLNT446p\n1K1cGUa2zp0b1nFQcpAkmcE//xkGYx52GHz/fdIRSaVYSxBm1gCYDhwEzAMmAL3dfWrKPn8Bmrr7\nX8ysJTAN2MzdV1Q5lkoQWeAO550XFqB//nk1SEv+WLkydLP+5BN47jlVeWZLPpcgOgMz3H2Wuy8H\nHgV6VNnHgcq5QTcEFlRNDpI9gwaFqbr/8x8lB8kvDRrAnXfCppuG9a5X6FMgcXEniK2AOSmPP4+e\nS3Ur0NbM5gEfAL+POaaSdfPN8MgjoeTQrFnS0YisqWHDMBXH0qVhkkhVGiSrUdIBAIcC77n7gWa2\nIzDGzHZ19zVqIgcNGrTqfllZGWVlZTkLstA99FDoMfLaa+Ebmki+WnfdMOL64IPhz38O71vNApu5\n8vJyysvLs3KsuNsg9gIGuXu36PHFgLv7dSn7PAtc4+6vR4//C1zk7m9XOZbaIOrohRegTx946SVo\n1y7paEQys3BhWIekb1/405+SjqZw5XMbxARgJzNrZWbrAr2BkVX2mQUcDGBmmwGtgU9ijqtkvP9+\nGCU9fLiSgxSWFi1g1KgwoO6xx5KOpjTFWsXk7hVmdg7wAiEZDXX3KWbWP2z2IcCVwH1m9mH0sgvd\nXYPvs2D2bDjiCLj9dth336SjEVl722wTOlQcfHCYPHL//ZOOqLRooFyRWrQoJIUzzoA//CHpaETq\n58UXw6j/sWOhbdukoyksWg9Cfmb58rA8aPv2oXguUgzuvz+scDh+PGyySdLRFA4lCFnFHc4+G+bM\nCZPvaXlHKSZ//WtYr+TFFzUDQKaUIGSVm26Cu+8Og+E0p40Um5Ur4fjjwyDP++5T99dM5HMvJsmh\nUaPg2mvhmWeUHKQ4NWgQBtJNmgTXXVf7/lI/KkEUiWnTYL/9YMQI+PWvk45GJF6ffx4WHBoyBA4/\nPOlo8ptKECVu8WI4+mi48kolBykNW28Njz8eBtFNn550NMVLJYgCt3IlHHMMbL45DB6cdDQiuTVk\nSOipN368qlWro0bqEnb55WEqjbFjwxw2IqWmf3/48suwtkkD1YmsQVVMJWrUKLjrLnjiCSUHKV23\n3AJffaVG6zioBFGgZs2Czp1Dcthvv6SjEUnW55/DnnvCww/DAQckHU1+UQmixCxbBr/9LVx4oZKD\nCIRG6wcfDNNxzJuXdDTFQyWIAnTuuWE96eHDNVBIJNUVV4Q2uZdegnXWSTqa/KASRAkZNiysCHfv\nvUoOIlVdcglsuCFcemnSkRQHlSAKyCefwF57hQTRqVPS0Yjkp6+/ht12C1POHHpo0tEkTyWIEvDT\nT3DCCeEbkpKDSPVatgztEX37whdfJB1NYVMJokBceCFMmRJmaFXVkkjtLrsM3ngDRo8u7fERKkEU\nueefh0ceUbuDyNr4299Cj7/rr086ksKlEkSe++or6Ngx9O8uK0s6GpHCMmcO7LEHPPtsGCdRilSC\nKFLuYcnQk09WchCpi222CSOtTzoJlixJOprCoxJEHrv7brjtNnjzTU2lIVIfffqE7q+33550JLmn\nyfqK0IwZsM8+8PLLWqRdpL6+/RY6dAhfuEpt/QgliCKzYgXsu2+YNuDcc5OORqQ4vPIK9O4N778P\nm26adDS5ozaIIvOPf4Ti8MCBSUciUjz23z+0RQwYENr3pHYqQeSZSZPCbJRvvw2tWiUdjUhxWbo0\njLIeNAh69Uo6mtxQFVORWL4c9t4bzjoL+vVLOhqR4vTWW3DUUfDBB7DZZklHEz9VMRWJ668P0wSc\nfnrSkYgUr86dwzQcZ5+tqqbaqASRJyZOhAMPhHffDX23RSQ+y5bB7rvDX/8KJ56YdDTxUhVTgauo\nCFVLZ56pqiWRXJkwAY48Mnw522STpKOJT6xVTGZ2vZk1NbN1zOy/ZvY/MzupLieT9G6+GZo0UdWS\nSC7tuWfoSv7HPyYdSf6qtQRhZu+7e0cz6wkcAfwReMXdO+QiwJQ4irIE8emn4Y06bhzsvHPS0YiU\nliVLoH17uOMO6NYt6WjiEXcjdeXCfYcDj7v7t3U5kazJPfRY+vOflRxEktC4Mdx5Z/g//P77pKPJ\nP5kkiGfMbCqwO/BfM9sEWBpvWKXhwQfDbK0q4ook55BDoGvXsBiX/FwmVUy/ABoD37p7hZk1Bpq4\n+5e5CDAljqKqYlqwANq1C9MQ77FH0tGIlLbK/8f//Cf0biomcVcxjXP3he5eAeDuS4BRdTmZrHbx\nxfDb3yo5iOSDjTeGa68NVU0VFUlHkz+qTRBmtrmZ7Q6sb2a7mVmn6FYGbJCzCIvQG2/Ac8/BFVck\nHYmIVDrlFFh//dAmIUG1VUxmdgpwKrAH8HbKpu+A+9z9yYxOYNYNuJGQjIa6+3Vp9ikD/kVoEP+f\nux+QZp+iqGJasSIUYf/ylzCzpIjkj48+CotzTZwIm2+edDTZEetAOTM71t2H1zGwBsB04CBgHjAB\n6O3uU1P2aQa8ARzi7nPNrKW7f53mWEWRIG64AUaNghde0PrSIvnoootg7lx46KGkI8mOWBKEmZ3k\n7g+Z2Z+ANXZy9xsyCGwv4DJ37x49vji8dHUpwszOBrZw9/9Xy7EKPkHMnRsWLXnjDWjdOuloRCSd\nJUvCIl333RdmVi50cTVSN45+NgE2THPLxFbAnJTHn0fPpWoNtDCzsWY2wcz6ZHjsgnPRRdC/v5KD\nSD5r3DiU9M87L1QJl7JG1W1w9zujn5fnIIZOwIGEpDTOzMa5+8cxnzenXn89LB86ZUrSkYhIbY45\nJqxfPXgwnHNO0tEkp9oEUcnMdgBuAvYiVDWNA853908yOP5cYNuUx1tHz6X6HPja3ZcCS83sFaAD\nsEaCGDRo0Kr7ZWVllJWVZRBC8ioqwtKh118f5lwSkfxmFuZIO+CA0JmkZcukI8pceXk55eXlWTlW\nJo3U44HbgEeip3oD57p7l1oPbtYQmEZopJ4PvAWc4O5TUvZpA9wCdAN+AbwJ9HL3yVWOVbBtEEOG\nhFHTr7yihmmRQvKHP4RV6AYPTjqSuou7F9OH7r5rlec+yHSyvqib602s7uZ6rZn1JzRWD4n2uQDo\nC1QAd7n7LWmOU5AJYtEiaNMGRo+Gjh2TjkZE1sY334T/31GjwlKlhSiuXkwtorsXAYuARwlVTL2A\n5u7+l7qcsK4KNUEUwzcQkVJ2112hBuDllwuzBiCuBPEpISGkO7C7+w51OWFdFWKCmD4d9tkHJk+G\nTTdNOhoRqYuKilB6uPxy6Nkz6WjWnlaUy1M9e0KXLmHeJREpXGPGwIABYaT1uusmHc3aiXuyPqmD\nl1+G994LVUwiUth+85uwZsvttycdSW6pBBGDlSuhc+ewzkOxL4guUio++ih0e506FVq0qH3/fKES\nRJ55+GFo2FCT8YkUk3bt4Nhj4cork44kdzIqQZjZVkArUgbWufsrMcaVLoaCKEEsXQq//CX8+9+w\n775JRyMi2fTVV2Geprfegh1y2k2n7uIeB3EdoWvrZMI4BQi9mI6qywnrqlASxA03QHk5jByZdCQi\nEofLL4cZMwpntte4E8Q0YFd3X1aXE2RLISSIb78NDVkvvQTt2ycdjYjE4bvvwv/56NFhduZ8F3cb\nxCeEhXykFv/8Jxx2mJKDSDHbcEO45BL461+TjiR+mZQghhMmz/svsKoU4e7nxRvaGnHkdQniiy9C\nI9a770KrVklHIyJxWrYsTMFx//2w//5JR1OzuKuYTkn3vLvfX5cT1lW+J4iBA8MAmn/9K+lIRCQX\nHnoIbrstLACWz1NwaCR1wj75BPbcE6ZNK6xpgUWk7iqn4LjiCujRI+loqhfXXEzD3P23ZjaR9EuO\n7prmZbHJ5wTRty9su23o3SAipePpp+Gyy0LVcoM8HVUWV4LYwt3nm1naGnV3n1WXE9ZVviaI6dPh\n178O3d422ijpaEQkl9xD7cHFF8NxxyUdTXqqYkrQSSeFxqpLL006EhFJwnPPwZ//DB9+GGZQyDea\naiMhU6bACy/A73+fdCQikpTu3aFpUxg2LOlIsk8liHro1Qs6dYKLLko6EhFJ0pgxcM45YUK/Ro1q\n3z+XYi1BmNmRZqaSRhUTJ4YpvQcOTDoSEUnawQeHRcH+/e+kI8muTD74ewEzzOx6M2sTd0CF4oor\n4IILoEmTpCMRkaSZwd//HmZ6raioff9CUWuCcPeTgN2AmcB9ZjbOzM40sw1jjy5PTZkSSg9nn510\nJCKSL8rKYLPNiqstIqOqI3dfDDwBPApsAfQE3jWzc2OMLW9dfXVomG7cOOlIRCRfmIXejFddFRYN\nKwaZtEEcZWYjgHLCpH2d3b07YX6mP8UbXv6ZORNGjVLbg4is6dBDYb314Kmnko4kOzKZi+l+YGi6\nBYLM7CB3/29cwVU5V170YurXD7bcMtQ3iohU9dRT4fPhnXfyY46m2AfKmdnmQGfClBsT3P2Lupys\nPvIhQcyeDR07hlHTG2+caCgikqdWrgzrRFx3XZj+P2lxd3M9HXgLOAY4DhhvZqfV5WSF7vrr4Ywz\nlBxEpHoNGoS2iCuuCFNxFLJMV5Tbx90XRI83Bt5w91/mIL7UOBItQXz5ZZhSY8oU2HzzxMIQkQJQ\nUQG/+hXcdRd07ZpsLHFPtbEA+C7l8XfRcyXl1lvDyGklBxGpTcOGYZzU9dcnHUn9ZFKCeADYBXia\n0AbRA/gwuuHuN8QcY2UciZUgvv8ett8+LAyy886JhCAiBWbpUthuuzANxy67JBdH3CWImcBTrF4T\n4mngU2DD6Fb0hg4NxUQlBxHJ1HrrwXnnhbXqC1XGk/WZWRMAd/8+1oiqP38iJYjly2GnncLoyC5d\ncn56ESlgixbBjjvCBx/ANtskE0PcvZjam9l7wEfAR2b2jpm1q8vJCtGwYaF6SclBRNZW8+Zw6qlw\n441JR1I3mbRBvAFc4u5jo8dlwNXuvk/84f0sjpyXINzDuIdrrsmP/swiUngqx0/NnBkSRq7F3QbR\nuDI5ALh7OVASsxC9+GLorta9e9KRiEih2nZbOOIIuPPOpCNZe5kkiE/M7G9mtl10uxT4JO7A8sGN\nN8L55+fHcHkRKVx/+APcdlto0ywkmSSI04BNgCeB4UDL6LmiNm0aTJgAJ56YdCQiUug6dYIddoAn\nn0w6krVTYxuEmTUErnP3C3IXUrWx5LQNYuBAaNEiDJcXEamvESPCwLlx43J73tjaINy9Ati3TlFF\nzKybmU01s+lmVu3qzWa2p5ktN7Nj6nO+bFi0CB5+WAsCiUj2HHUUfPEFjB+fdCSZy2R57ffMbCTw\nOLCk8kl3r7WwFK1lfStwEDAPmGBmT7v71DT7XQuMXovYY3P33aFRacstk45ERIpFw4Zh4NxNN8Fe\neyUdTWYy6eZ6b5qn3d1rbYcws72Ay6IFhjCzi6PXXldlv98DPwF7As+mSz65qmJasSLUFY4YAbvv\nHvvpRKSEfPttGFf14Yew9da5OWd9qpgyKUHc7e6vVznhrzM8/lbAnJTHnxPWlUg91pbA0e5+gJn9\nbFsSRoyIaoLQAAAPrklEQVSAVq2UHEQk+5o1g5NPDpN/Xntt0tHULpNeTLdk+Fxd3Qiktk0k2qn0\n1lvh3JJcaVtEcuGcc+Cee8Jkfvmu2hKEme0N7ANsYmZ/TNnUFGiY4fHnAtumPN46ei7VHsCjZmaE\nLrTdzWy5u4+serBBgwatul9WVkZZWVmGYWRm0qSwWlzPnlk9rIjIKjvtFLq9Pv449OmT/eOXl5dT\nXl6elWNV2wZhZl2BMuAsYHDKpu+AZ9x9Rq0HD91kpxEaqecTVqY7wd2nVLP/vdGxE2mDGDgQWraE\nyy+P9TQiUuJGjgxT+OSiy2usa1KbWSt3n1WnyMLruwE3Eaqzhrr7tWbWn9BYPaTKvveQUCP1d9+F\ntodcNh6JSGmqqAidYZ56CnbbLd5zxZ0gWgMXANuRUiXl7gfW5YR1FXeCGDw4LOwxfHhspxARWeXq\nq+Gzz2DIkFp3rZe4E8QHhCqmd4CKyufd/Z26nLCu4kwQ7tChA/zrX3DQQbGcQkTkZyrXuf/0U9ho\no/jOE/dsrivc/Q53f8vd36m81eVk+er112HZMjgwp2UiESllm20G3brBAw8kHUn1MkkQz5jZADPb\nwsxaVN5ijyyHbr89TKuhWVtFJJcGDAifPwkslpmRTKqYPk3ztLv7DvGEVG0csVQxff116Hb26afJ\nLOYhIqXLHXbZJUwF3rVrPOeIdSS1u29flwMXiocegiOPVHIQkdwzg379YOjQ+BJEfVRbxWRmF6bc\nP77KtqvjDCpX3MPEfP36JR2JiJSqk04K4yK++SbpSNZUUxtE75T7f6myrVsMseTcm2+Gxun99086\nEhEpVS1bwqGHhiUG8k1NCcKquZ/ucUGqLD2ocVpEktSvX/g8yjc1tUF4NffTPS44330XBsVNSTvp\nh4hI7hx0ECxcCO++G+Zpyhc1JYgOZraYUFpYP7pP9Hi92COL2bBhUFYGm2+edCQiUuoaNIDTTw+N\n1fmUIGrt5povst3Nde+94dJL4fDDs3ZIEZE6mzMHOnYMPzfYIHvHjXskddH56COYPTs0DImI5INt\ntoEuXfJrPriSTBAPPBDmYW+UyXp6IiI5cuqp+TX1RslVMVVUhGm9R4+Gdu2yEJiISJb8+CNstRVM\nnBh+ZoOqmNbC2LFhkiwlBxHJN+uvD8ceC//+d9KRBCWXIB58MCwaLiKSj04+OVQz5UPlTklVMS1Z\nEopt06fDpptmKTARkSxauRJ23BGefDI7q82piilDI0bAvvsqOYhI/mrQIHSiefDBpCMpsQRR2XtJ\nRCSf9ekT5mZasSLZOEomQcydC2+/DUcdlXQkIiI123ln2H57GDMm2ThKJkE8/DAcc0zoJSAiku/6\n9El+TETJNFJ36gT/939wwAFZDEpEJCZffx0aq+fNg8aN634cNVLXYsYMmD9f6z6ISOFo2TLMGffs\ns8nFUBIJ4rHH4LjjoGHDpCMREclcr17h8yspJVHFtMsucMcdoYuriEihWLQIttsuzPDatGndjqEq\nphpMnhzWet1nn6QjERFZO82bh6rxkSOTOX/RJ4jHHoPjjw+DT0RECk2S1UxFXcXkDr/6Fdx/f5hn\nXUSk0CxeHNaK+OyzUKJYW6piqsaHH8KyZdC5c9KRiIjUTdOmYc3qp57K/bmLOkE89hj89rdgdcqd\nIiL5IalqpqKtYnIPw9WHDcuvRcBFRNZW5UzUM2fCxhuv3WtVxZTGpElh2txsTJcrIpKkxo3h4INz\nP2iuaBPEiBFw9NGqXhKR4nD00eFzLZeKNkE89VS4oCIixeDww+Gll+CHH3J3zqJMELNmhZGHGhwn\nIsWiefPQI/OFF3J3zqJMEE8/DUceCY0aJR2JiEj2HH10bru7xp4gzKybmU01s+lmdlGa7Sea2QfR\n7TUz26W+51T1kogUox49QkN1rlaaizVBmFkD4FbgUKAdcIKZtamy2yfA/u7eAbgSuKs+51ywAN55\nB37zm/ocRUQk/2yzTZi877XXcnO+uEsQnYEZ7j7L3ZcDjwI9Undw9/Hu/m30cDywVX1O+OyzoTuY\nVo4TkWLUs2fuejPFnSC2AuakPP6cmhNAP2BUfU6o6iURKWaV7RC5GOOcN824ZnYA0BeodtWGQYMG\nrbpfVlZGWVnZz7b/8EPoBjZ0aDwxiogkrW1bWHddeP/99AOBy8vLKS8vz8q5Yp1qw8z2Aga5e7fo\n8cWAu/t1VfbbFRgOdHP3mdUcq9apNp55Bv71r5AkRESK1QUXQJMmkPKduVr5PNXGBGAnM2tlZusC\nvYGfLX1hZtsSkkOf6pJDpkaNgsMOq88RRETy32GHwfPPx3+e2CfrM7NuwE2EZDTU3a81s/6EksQQ\nM7sLOAaYBRiw3N3XmKC7thKEO+ywQyhFtG8fy68iIpIXli2DTTcNk/e1bFnzvvUpQRTNbK7TpoXe\nS7Nna/4lESl+PXpA795wwgk175fPVUw5M2oUdOum5CAipaFbt/C5F6eiSRDPPw/duycdhYhIbnTr\nBqNHh2UN4lIUCeKHH+D118OyfCIipWD77cMEfu+9F985iiJBvPxyWDWuWbOkIxERyZ3u3eOtZiqK\nBFHZ/iAiUkq6dYu3u2tR9GJq3TqsPd2xY46DEhFJ0NKlobvrrFmhuimdku7FNHMmfPcddOiQdCQi\nIrm13nqw774wZkw8xy/4BPH88+reKiKlq3v3+KqZCj5BjBkDhxySdBQiIsk45JDwORhHa0FBJ4iV\nK+HVV6HKpK4iIiWjdeuwwtynn2b/2AWdICZODPOQbLFF0pGIiCTDLHxJztIM3z9T0Ani5Zeha9ek\noxARSVbXruHzMNuUIERECpwSRBUrVypBiIgAtGkDP/4In32W3eMWbIKYPBk22gi23jrpSEREkmUW\nTymiYBNEebl6L4mIVCorU4JYRdVLIiKrqQQRcVeCEBFJ1bYtLF4Mc+Zk75gFmSCmTIEmTWDbbZOO\nREQkP8TRDlGQCUKlBxGRNSlBoAQhIpJOyScItT+IiKTXvj0sWADz5mXneAWXIObPDxNTbbdd0pGI\niOSXBg1gjz3g3XezdLzsHCZ3PvggrByn9R9ERNbUsWP4nMyGgksQ77+v1eNERKrToUP4nMyGgksQ\nlSUIERFZk0oQKkGIiKTVujXMnQvff1//YxVUgliyBGbPDjMXiojImho1CqOqJ06s/7EKKkFMmhSS\nwzrrJB2JiEj+ylY7REElCLU/iIjULlvtEAWVINT+ICJSO5UgREQkrV13DVXyFRX1O05BJYgPPwy/\nuIiIVK9ZM9h0U/j44/odp6ASxMYbQ/PmSUchIpL/stEOUVAJQu0PIiKZyUY7ROwJwsy6mdlUM5tu\nZhdVs8/NZjbDzN43s2pbGdT+ICKSmbwvQZhZA+BW4FCgHXCCmbWpsk93YEd33xnoDwyu7ngqQQTl\n5eVJh5A3dC1W07VYTdeiMEoQnYEZ7j7L3ZcDjwI9quzTA3gAwN3fBJqZ2WbpDqYSRKA3/2q6Fqvp\nWqymawGtWoXZJ+oj7gSxFZC6hPbn0XM17TM3zT6A1oAQEcmUWf1rXQqqkbpBQUUrIpKs+ta6mLtn\nJ5J0BzfbCxjk7t2ixxcD7u7XpewzGBjr7o9Fj6cCXd39yyrHii9QEZEi5u51WmKtUbYDqWICsJOZ\ntQLmA72BE6rsMxIYCDwWJZRvqiYHqPsvKCIidRNrgnD3CjM7B3iBUJ011N2nmFn/sNmHuPtzZnaY\nmX0MLAH6xhmTiIhkJtYqJhERKVx51+ybzYF1ha62a2FmJ5rZB9HtNTPbJYk4cyGT90W0355mttzM\njsllfLmU4f9ImZm9Z2aTzGxsrmPMlQz+R5qa2cjos2KimZ2aQJixM7OhZvalmX1Ywz5r/7np7nlz\nIySsj4FWwDrA+0CbKvt0B/4T3e8CjE867gSvxV5As+h+t1K+Fin7/Rd4Fjgm6bgTfF80Az4Ctooe\nt0w67gSvxV+AayqvA7AAaJR07DFci32BjsCH1Wyv0+dmvpUgsjqwrsDVei3cfby7fxs9HE8140eK\nQCbvC4BzgSeAr3IZXI5lci1OBIa7+1wAd/86xzHmSibXwoENo/sbAgvcfUUOY8wJd38NWFTDLnX6\n3My3BJHVgXUFLpNrkaofMCrWiJJT67Uwsy2Bo939DqCYe7xl8r5oDbQws7FmNsHM+uQsutzK5Frc\nCrQ1s3nAB8DvcxRbvqnT52bc3VwlB8zsAELvr32TjiVBNwKpddDFnCRq0wjoBBwINAbGmdk4d6/n\n6gAF6VDgPXc/0Mx2BMaY2a7u/n3SgRWCfEsQc4FtUx5vHT1XdZ9tatmnGGRyLTCzXYEhQDd3r6mI\nWcgyuRZ7AI+amRHqmrub2XJ3H5mjGHMlk2vxOfC1uy8FlprZK0AHQn19McnkWvQFrgFw95lm9inQ\nBng7JxHmjzp9buZbFdOqgXVmti5hYF3Vf/CRwMmwaqR22oF1RaDWa2Fm2wLDgT7uPjOBGHOl1mvh\n7jtEt+0J7RADijA5QGb/I08D+5pZQzPbgNAoOSXHceZCJtdiFnAwQFTn3hr4JKdR5o5Rfcm5Tp+b\neVWCcA2sWyWTawH8DWgB3B59c17u7p2TizoeGV6Ln70k50HmSIb/I1PNbDTwIVABDHH3yQmGHYsM\n3xdXAveldP+80N0XJhRybMzsYaAM2NjMZgOXAetSz89NDZQTEZG08q2KSURE8oQShIiIpKUEISIi\naSlBiIhIWkoQIiKSlhKEiIikpQQhJcfMKszs3Wj658fMbL3o+VqnTK7DuS4zsz9m63giuaQEIaVo\nibt3cvddgOXAWdHz9xLm7hERlCBEXgV2gtqnTI4Wn/ks5fEGZjY7mtKin5m9FS3S83hlqaTK68ea\nWafo/sbRvECYWQMzu97M3owWczkjy7+jSJ0oQUgpMgAza0RYSGViJi9y98XAe2bWNXrqCOB5d68g\nrL/Q2d13A6YCp2dyyOjn6YS5cboQ1jg408xaZfzbiMQkr+ZiEsmR9c3s3ej+q8DQtXjtMKAX8DJh\ncrjboud3NbMrgI0IU2yPXotjHgLsYmbHR4+bAjsTJpoTSYwShJSiH9y9Ux1fOxK4ysyaA7sDL0XP\n3wsc5e6TzOwUoGua165gdak9tQrKgHPdfUwdYxKJhaqYpBTVtJhQTVMm4+5LCGsJ3AQ846tnu2wC\nfGFm6wC/q+blnxHWrQA4PuX50cCAqMoLM9vZzNav7ZcQiZsShJSitFMYR1MmvwG0jhqfq5sS+TFC\nEng05bm/AW8RqqyqW3vhn8DZZvYOYZr2SncDk4F3zWwiMBiV7iUPaLpvERFJSyUIERFJSwlCRETS\nUoIQEZG0lCBERCQtJQgREUlLCUJERNJSghARkbSUIEREJK3/D+QDF394WgrPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8433030c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#P1 values:\n",
    "P1=np.linspace(0,1,101)\n",
    "\n",
    "#Handle the infinite value of log in 0, so that it returns 0\n",
    "def SafeLog2( v ):\n",
    "    r=v.copy()\n",
    "    r[v!=0]=np.log2(v[v!=0])\n",
    "    return r\n",
    "\n",
    "#Entropy of a Bernouilli distribution with probability p\n",
    "def BernoulliEntropy( p ):\n",
    "    return -( p*SafeLog2(p)+(1-p)*SafeLog2(1-p) )\n",
    "\n",
    "#plot entropy functions of P1\n",
    "plt.plot(P1,BernoulliEntropy(P1))\n",
    "\n",
    "#Fancy stuff\n",
    "plt.title('Entropy of a Random bit generator')\n",
    "plt.xlabel('P1 value')\n",
    "plt.ylabel('Entropy in bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In some cases later, we will need to compute the entropy of a dataset\n",
    "def GetEntropyFromQuantizedData( v, num_bins=100 ):\n",
    "    \"\"\"\n",
    "        We assume that v is a vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #Data is divided in num_bins equally spaced bins, then the histogram is returned\n",
    "    hist,edges = np.histogram(v,bins=num_bins)\n",
    "    \n",
    "    #get the empirical distribution, use floating point division\n",
    "    p=np.double(hist)/np.sum(hist)\n",
    "\n",
    "    #scipy entropy simply compute -sum(p * log(p))\n",
    "    return p, -np.dot(p,SafeLog2(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative quantity of information\n",
    "\n",
    "### Introduction\n",
    "It is very common in engineering to try to find dependency between data, so that we can exploit some knowledges or intuitions over relationships between them. In some cases, we can measure the validity of this model by measuring the relative quantity of informations given by a random variable when the outcome of some other random process in known, using the following tools:\n",
    "\n",
    "### Entropy of random Markov process\n",
    "A common way to give probabilistic interpretation of signals, or messages, is to account for the probability of occurence of a symbol given the 0, 1, 2, ..., p previous symbols in the signal.\n",
    "This model is known as the order-p Markov source, let's fist see how entropy is computed for a order-1 markov process:\n",
    "$$\n",
    "    H(S) = -\\sum_i p_i \\sum_j p(j|i)log(p(j|i))\n",
    "$$\n",
    "Where $j$ is the current symbol, and $i$ is the preceding symbol.\n",
    "\n",
    "For a second order Markov process, we have:\n",
    "$$\n",
    "    H(S) = -\\sum_i p_i \\sum_j p(j|i) \\sum_k p(k|i,j) log(p(k|i,j))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code compute the entropy of a markov chain whose matrix is given in parameter\n",
    "def GetEntropyFromMarkovRandomProcess( markov, prob ):\n",
    "    \"\"\"\n",
    "        We assume that markov is a 2D square matrix that\n",
    "        stands for a random markov chain.\n",
    "        For the record, the jth column of the matrix should sum to 1\n",
    "        and M(i,j) is the probability that an element in the state\n",
    "        j, will move to the state i, ie p(i|j)\n",
    "        \n",
    "        We assume that prob is the probability of each state in the\n",
    "        same order as in the markov matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    entropy = 0\n",
    "    #for each state\n",
    "    for i in range(markov.shape[0]):\n",
    "        Pji = markov[i,:]/np.sum(markov[i,:])\n",
    "        Sent = np.dot(Pji,SafeLog2(Pji))\n",
    "        entropy += prob[i]*Sent\n",
    "\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small numerical illustration\n",
    "\n",
    "Let's say that we have a 1 dimensional signal from $\\mathbb{Z}^N$, and we want to model the occurence of every new symbol in this signal as a first order random markov process.\n",
    "This model allows us to study the entropy of the signal in another framework, let's see how entropy values compares in the basic and markov models.\n",
    "\n",
    "A very interesting basic introduction to markov chain can be found here: http://www.ams.org/samplings/feature-column/fcarc-pagerank\n",
    "It should help to understand the link between graphs, probability and random linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Symbol wise model entropy is ', 1.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'amplitude' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-267f0cab692a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#Now use the first order markov model, by first computing the markov chain matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mhist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamplitude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#plot the histogram of values in the signal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'amplitude' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOr/A3iJ/tTrOOYtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84356b2190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First: generate a binary sinusoid like signal, of amplitude -1 or 1\n",
    "nbPeriod = 10\n",
    "s=np.round(np.cos(np.linspace(0,nbPeriod*2*np.pi,nbPeriod*2+1)))\n",
    "#drop the last element for pefectly flat histogram (no repetition)\n",
    "s=s[0:s.size-1]\n",
    "\n",
    "#Plot signal of interest\n",
    "plt.figure()\n",
    "plt.plot(s)\n",
    "plt.title('sinusoid')\n",
    "\n",
    "#compute histogram and symbol-wise entropy of this signal\n",
    "p,entropy=GetEntropyFromQuantizedData(s,2)\n",
    "print( \"Symbol wise model entropy is \",entropy)    \n",
    "\n",
    "#Now use the first order markov model, by first computing the markov chain matrix\n",
    "hist,edges = np.histogram(s,bins=amplitude)\n",
    "\n",
    "#plot the histogram of values in the signal\n",
    "plt.figure()\n",
    "plt.bar(edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(edges), max(edges))\n",
    "plt.title('histogram, symbol-wise entropy is '+str(entropy))\n",
    "print \"Sum of empirical distribution is \",np.sum(p)\n",
    "\n",
    "#Define markov matrix\n",
    "markov=np.zeros((hist.size,hist.size))\n",
    "\n",
    "#loop over current elements\n",
    "for i in range(hist.size):\n",
    "    #index in s where you can find previous element\n",
    "    if( i < hist.size-1 ):\n",
    "        curMask = (s>=edges[i]) * (s<edges[i+1])\n",
    "    else: #last range has both sides included, see numpy doc\n",
    "        curMask = (s>=edges[i]) * (s<=edges[i+1])\n",
    "    \n",
    "    #Now we shift the idx mask toward the right in order\n",
    "    #to get the mask of the current element\n",
    "    nextMask = np.roll(curMask, 1)\n",
    "    n = s[nextMask]\n",
    "    \n",
    "    #Check if the current state is present\n",
    "    if nextMask.any():\n",
    "        #Get the histogram of the next elements\n",
    "        for j in range(hist.size):\n",
    "            #index in s where you can find previous element\n",
    "            if( j < hist.size-1 ):\n",
    "                jMask = (n>=edges[j]) * (n<edges[j+1])\n",
    "            else: #last range has both sides included, see numpy doc\n",
    "                jMask = (n>=edges[j]) * (n<=edges[j+1])\n",
    "\n",
    "            #Now fill the ith column, jth line of the markov matrix\n",
    "            markov[j,i] = np.count_nonzero(jMask)\n",
    "\n",
    "        #When the ith column of markow matrix is full, we can normalize\n",
    "        #the weight so that they sum to 1\n",
    "        sum_weight = np.sum(markov[:,i])\n",
    "        if sum_weight > 0 :\n",
    "            markov[:,i] /= sum_weight\n",
    "        else:\n",
    "            raise ValueError('Definition of state based on floating range is wrong')\n",
    "    else:\n",
    "        #if current state is not present, we assume\n",
    "        #equiprobable next state, to avoid dangling nodes\n",
    "        markov[:,i] =  1/hist.size\n",
    "\n",
    "#Now compute the first order markov model entropy\n",
    "Mentropy = GetEntropyFromMarkovRandomProcess(markov,p)\n",
    "print( \"First order markov model entropy is \",Mentropy)    \n",
    "\n",
    "#Print markov matrix for fun\n",
    "plt.figure()\n",
    "plt.imshow(markov)\n",
    "plt.axis('off')\n",
    "plt.title('Markov matrix, first order markov model entropy is '+str(Mentropy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous numerical case was interesting because, our bit generator is perfectly balanced: there is as much -1 as 1's but it is not random, it is perfectly causal.\n",
    "The symbol wise entropy definition gives us an entropy of 1 bit, but the first order markov based model give us an entropy of 0, because the same input always generate the same next output, there is no information in this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conditional Entropy\n",
    "The concept of entropy also extends to conditional probability of events, in this case, $H(x|y)$ can be interpreted as the amount of information needed to describe the outcome of a random process x, given the value of the random process y and reads:\n",
    "\n",
    "\\begin{align}\n",
    "    H(x|y)  &= \\sum_{j=1}^{K} p(y_j) H(x|y=y_j) \\\\\n",
    "            &= \\sum_{j=1}^{K} p(y_j) \\sum_{i=1}^{N} -p(x_i|y_j) log(p(x_i|y_j)) \\\\\n",
    "            &= -\\sum_{j=1}^{K} \\sum_{i=1}^{N} p(x_i,y_j) log(p(x_i|y_j)) \\\\\n",
    "            &= -\\sum_{j\\in J,i\\in I} p(x_i,y_j) log(p(x_i|y_j)) \\\\\n",
    "            &= -\\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left(\\frac{p(x_i,y_j)}{p(y_j)}\\right) \\\\\n",
    "            &=  \\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left(\\frac{p(y_j)}{p(x_i,y_j)}\\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "The concept of cross entropy is usefull to characterize the difference between two distributions. \n",
    "It gives the numbe of bits needed to encode one symbole from the distribution x, using a code that is optimal for the distribution y.\n",
    "\n",
    "It reads:\n",
    "$$\n",
    "    H(x,y) = \\sum_{i \\in I} p(x_i) log( p(y_i) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "If a random process output depends itself on two random variables $x$ and $y$, its entropy can be considered as the joint entropy $H(x,y)$, which in the binary case, is the minimum number of bits needed to describe one output.\n",
    "If we know in advance the value of $y$, we have gained $H(y)$ bits of information, so we now only need $H(x,y)-H(y)$ to describe the output, that is somehow more deterministic.\n",
    "\n",
    "This is called the chaine rule and reads:\n",
    "$$\n",
    "    H(x|y) = H(x,y)-H(y)\n",
    "$$\n",
    "\n",
    "The proof is quite straightforward:\n",
    "\n",
    "\\begin{align}\n",
    "    H(x|y)  &= \\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left(\\frac{p(y_j)}{p(x_i,y_j)}\\right) \\\\\n",
    "            &= -\\sum_{j\\in J,i\\in I} p(x_i,y_j) log(p(x_i,y_j)) + \\sum_{j\\in J,i\\in I} p(x_i,y_j) log(p(y_j)) \\\\\n",
    "            &= H(x,y) + \\sum_{j\\in J,i\\in I} p(x_i,y_j) log(p(y_j)) \\text{ which is a marginal distribution}\\\\\n",
    "            &= H(x,y) + \\sum_{j\\in J} p(y_j) log(p(y_j)) \\\\\n",
    "            &= H(x,y)-H(y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bayes' rule\n",
    "The famous Bayes theorem links conditional probability of two random variables, for instance $x$ and $y$:\n",
    "$$\n",
    "    p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\n",
    "$$\n",
    "Following the chain rule, and the fact that $H(x,y)=H(y,x)$ we can easly extends the Bayes rule for conditional entropy:\n",
    "\\begin{align}\n",
    "    H(x|y)  &= H(x,y)-H(y) \\\\\n",
    "            &= H(y,x)-H(y) \\\\\n",
    "            &= H(y|x)+H(x)-H(y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mutual information\n",
    "This quantity gives an indication about the dependence between two random variables, it is more general than correlation coefficient, and not limited to real valued data. [Wikipedia](http://en.wikipedia.org/wiki/Mutual_infomation) gives the following interesting definition:\n",
    "Mutual information measures the information that X and Y share: it measures how much knowing one of these variables reduces uncertainty about the other. For instance, the mutual information of two independent variables is zero. At the opposite, if X is a deterministic function of Y and vice versa, the mutual information is equal to the entropy of X (or Y) which should be equal.\n",
    "\n",
    "The mutual information between two random variables $x$ and $y$ reads\n",
    "$$\n",
    "    I(x,y) = \\sum_{i=1}^{N} \\sum_{j=1}^{K} p(x_i,y_j) log\\left( \\frac{p(x_i,y_j)}{p(x_i)p(y_j)} \\right)\n",
    "$$\n",
    "\n",
    "We can link this expression to conditional entropy:\n",
    "\n",
    "\\begin{align}\n",
    "    I(x,y)  &= \\sum_{i=1}^{N} \\sum_{j=1}^{K} p(x_i,y_j) log\\left( \\frac{p(x_i,y_j)}{p(x_i)p(y_j)} \\right) \\\\\n",
    "            &= \\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left( \\frac{p(x_i,y_j)}{p(x_i)p(y_j)} \\right) \\\\\n",
    "            &= \\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left( \\frac{p(x_i,y_j)}{p(x_i)} \\right) - \\sum_{j\\in J,i\\in I} p(x_i,y_j) log(p(y_j))\n",
    "\\end{align}\n",
    "Using the fact that\n",
    "$$\n",
    "    p(x_i|y_j)p(y_j) = p(y_j|x_i)p(x_i) = p(x_i,y_j)\n",
    "$$\n",
    "we can rewrite the left side as\n",
    "\\begin{align}\n",
    "    & \\sum_{j\\in J,i\\in I} p(x_i,y_j) log\\left( \\frac{p(x_i,y_j)}{p(x_i)} \\right) \\\\\n",
    "    &= \\sum_{j\\in J,i\\in I} p(x_i)p(y_j|x_i) log(p(y_j|x_i)) \\\\\n",
    "    &= \\sum_{i\\in I} p(x_i) \\sum_{j\\in J} p(y_j|x_i) log(p(y_j|x_i)) \\\\\n",
    "    &= -\\sum_{i\\in I} p(x_i) H(y|x=x_i) \\\\\n",
    "    &= -H(y|x)\n",
    "\\end{align}\n",
    "and we can recognize a marginal probability in the right side, which can be rewritten as:\n",
    "\\begin{align}\n",
    "    & \\sum_{j\\in J} log(p(y_j)) \\sum_{i\\in I}p(x_i,y_j) \\\\\n",
    "    &= \\sum_{j\\in J} log(p(y_j))p(y_j) \\\\\n",
    "    &= -H(y) \\\\\n",
    "\\end{align}\n",
    "The two parts together give:\n",
    "$$\n",
    "    I(x,y) = -H(y|x) + H(y)\n",
    "$$\n",
    "Doing the same reasoning linearizing $p(x_i)$ out of the log in the denominator of the first expression of $ I(x,y)$ instead of $p(y_j)$ gives:\n",
    "$$\n",
    "    I(x,y) = -H(x|y) + H(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler divergence, also called he relative entropy or discrimination information, can be computed between two distributions, say $x$ and $y$ and writes $ D_{KL}(x\\|y) $.\n",
    "This metric represents the expectation of the difference of the number of bits needed to encode a symbol from the distribution x, weither the encoding is optimal for the distribution x or y.\n",
    "\n",
    "In the discrete case it reads:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL}(x\\|y) &= \\sum_{i\\in I} p(x_i) log(p(x_i)) -\\sum_{i\\in I} p(x_i) log(p(y_i)) \\\\\n",
    "                 &= -H(x) - H(x,y)\n",
    "\\end{align}\n",
    "or equivalently\n",
    "$$\n",
    "    D_{KL}(x\\|y) = \\sum_{i\\in I} p(x_i) log\\left(\\frac{p(x_i)}{p(y_i)}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KL Divergence and Expectation Maximisation algorithm\n",
    "This chapter was inspired by [this article](http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/)\n",
    "by Liangjie Hong\n",
    "\n",
    "## Definitions\n",
    "Let's consider $P(x_i|\\theta)$ a distribution used to generate (or sample) $N$ points $x_i, i=0,\\dots,N-1$\n",
    "\n",
    "We can define a general distribution model as:\n",
    "$$\n",
    "    P_\\theta(x) = P(x|\\theta)\n",
    "$$\n",
    "And, to handle data from real world, we will define the following empirical distribution:\n",
    "$$\n",
    "    P_D(x) = \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i)\n",
    "$$\n",
    "\n",
    "## KL Divergence between model and data\n",
    "We can now express the Kullback Leibler divergence between the model distribution and the empirical one:\n",
    "\n",
    "\\begin{align}\n",
    "    D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) &= \\int P_D(x) log(P_D(x)) dx -\\int P_D(x) log(P_\\theta(x)) dx \\\\\n",
    "    &= H(P_D(x)) - \\int P_D(x) log(P_\\theta(x)) dx\n",
    "\\end{align}\n",
    "\n",
    "As $H(P_D(x))$ is a constant term, relatd to the empirical distribution of real data, we will call it $\\epsilon$, and we may get rid of it later.\n",
    "Instead we will mainly consider, the $\\theta$, ie, the model related term:\n",
    "$$\n",
    "   D_{KL} \\left( P_D(x)||P_\\theta(x) \\right) = \\epsilon -\\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2}\n",
    "$$\n",
    "\n",
    "Thanks to the definition of the empirical distribution, we can express this continuous expression as a discrete summation:\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle P_D(x) , log(P_\\theta(x)) \\rangle_{L^2} &= \\int \\sum_{i=0}^{N-1} \\frac{1}{N} \\delta(x-x_i) log(P(x|\\theta)) dx \\\\\n",
    "    &= \\sum_{i=0}^{N-1} \\frac{1}{N} log(P(x_i|\\theta)) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{i=0}^{N-1} log(P(x_i|\\theta))\n",
    "\\end{align}\n",
    "\n",
    "The last line amounts to the log-likelihood of the dataset. We can then conclude that maximizing the log-likelihood of a dataset, relatively to a set of distribution parameters $\\theta$ and a given dataset $x$ is equivalent to minimizing the Kullback Leibler divergence between the empirical distribution and the model distribution.\n",
    "\n",
    "## Practical KL Divergence between two data sets\n",
    "We are interested in computing the Kullback Leibler divergence between two real datasets $X$ and $Y$, indexed by $i$ where $i$ is the index of a specific event whose occurence probability is $X_i$.\n",
    "\n",
    "As in the definition of the empirical distribution, our datasets must verify:\n",
    "$$\n",
    "    \\sum_{i=0}^{N-1} X_i = 1\n",
    "$$\n",
    "\n",
    "then the expression of the kl divergence is simply:\n",
    "$$\n",
    "    D_{KL}(Y\\|X) = \\sum_{i=0}^{N-1} log \\left( \\frac{Y_i}{X_i} \\right) Y_i dx \n",
    "$$\n",
    "\n",
    "Moreover there is an important condition that must be respected: \n",
    "$$\n",
    "    X_i = 0 \\implies Y_i = 0\n",
    "$$\n",
    "If so, if the event $k$ has not occurence, we have $X_k=Y_k=0$ and $log \\left( \\frac{y_k}{x_k} \\right) y_k$ has a limit in $0$ which is $0$. Otherwise, Kullback Leibler cannot be computed.\n",
    "\n",
    "The explanation is pretty straightforward: if an event, or a symbol from $X$ has no occurence in $Y$, it has no reason to be encoded using $Y$ probability distribution, then the average number of bits needed to encode a symbol from $X$ is a nonsense, as some symbols cannot be represented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Minimizing the log-likelihood using the expectation maximization algorithm.\n",
    "\n",
    "### Presenting a \"hidden variable\" kind of problem\n",
    "\n",
    "We are now going to take a simple example, where we aim to fit a multidimensional noisy dataset, to the sum of independant random poisson processes.\n",
    "\n",
    "Here is how we formalize the problem:<br>\n",
    "\n",
    "- Let $\\vec{\\lambda} = [\\lambda_0, \\lambda_1,\\dots \\lambda_{P-1}]$ be the set of parameters of the P hidden Poisson random processes<br>\n",
    "- The $P$ hidden Poisson random process can be indirectly linked to $N$ observable values using a larger set of random variables $N_{nk}, n<N, k<P$ such that $N_{nk} \\sim Poisson\\{a_{nk}\\lambda_k\\}$<br>\n",
    "- Let $Y_{n} = \\sum_{k=0}^{P-1} N_{nk}$ be an observable state vector, defined using a sum of the P hidden poisson process, hence itself being a Poisson process defined as: $Y_{n} \\sim Poisson\\{ \\sum_{k=0}^{P-1}a_{nk}\\lambda_k \\}$<br>\n",
    "- Let $\\vec{y_n}$ be a realization of  $Y_{n}$, hence also of dimension N<br>\n",
    "- Let $A$ be the matrix of size $N\\times K$, that contains the $a_{nk}$, known in advance<br>\n",
    "\n",
    "Althoug this problem with hidden variables may seem weird at first glance, it has some simple instances:\n",
    "\n",
    "- $\\lambda_k$ can be the mean number of cigarets the person $k$ smokes in one hour, among P different persons.<br>\n",
    "- $Y_{n}$ can be the total number of cigarets smoked in 24 hour in the room $n$, among N rooms.<br>\n",
    "- $a_{nk}$ is the amount of time in hour the person $k$ has spent in the room $n$ during the measure.<br>\n",
    "\n",
    "Another well known instance of this problem is the Emission Tomography Reconstruction Problem.\n",
    "\n",
    "Of course, in every case, our aim will be to retrieve all $\\lambda_k$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solving the problem\n",
    "\n",
    "Given a realization $y_n$ of $Y_{n}$, we can write the likelihood of this experience as:\n",
    "$$\n",
    "L(y_n|Y_n) = \\prod_{n=0}^{N} \\frac{\\lambda_{Y_n}^{y_n}e^{-\\lambda_{Y_n}}}{y_n!}\n",
    "$$\n",
    "with \n",
    "- $y_n$ is known\n",
    "- $\\lambda_{Y_n} = \\sum_{k=0}^{P-1}a_{nk}\\lambda_k$\n",
    "\n",
    "The log-likelihood comes easily as:\n",
    "$$\n",
    "LL(y_n|Y_n) = \\sum_{n=0}^{N} {y_n}log(\\lambda_{Y_n}) - \\lambda_{Y_n} - \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon = y_n!$ is a known constant\n",
    "\n",
    "As $\\lambda_{Y_n}$ is actually a linear function of $\\lambda_k$, we automatically think about computing the derivative, in order to test some gradient ascent in order to get the maximum log likelihood value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
